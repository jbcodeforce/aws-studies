{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Amazon Web Service Studies \u00b6 Info Updated 12/26/2022 Created in 2002, and launched as AWS in 2004 with S3, SQS as first services offering. Why cloud \u00b6 There are 5 main advantages to AWS: Cost Savings : Only pay for what you use, leveraging economy of scale: EC2 instance with different pricing model. Usage from hundreds of thousands of customers is aggregated in the cloud. Moving from capex to variable expense (OPEX). Agility : Teams can experiment and innovate quickly and frequently at minimum cost. Define infrastructure in minutes, as code, not weeks or even months. Elacticity : Scale up and down so no need to guess resource capacity. Innovation : Focus on business apps, not IT infrastructure and data centers. Global Footprint : Extensible, reliable, and secure on global cloud infrastructure. Reach in a minutes. Notes The cloud transition has happened much faster because it yields great value and has fewer blockers, and bigger customer gains drive higher volume reinvestments into the platform. In 2014, every day, AWS adds enough new server capacity to support Amazon's global infrastructure when it was at $7B annual revenu. Scalability is the ability of an application to accommodate growth without changing design. Scalability ensures that systems remain highly available into the future as the business expands. Elasticity is the power to instantly scale computing resources up or down easily. Elastic Load Balancing and Auto Scaling can automatically scale your AWS cloud-based resources up to meet unexpected demand. Use cases \u00b6 Enable to build scalable apps, adaptable to business demand Extend Enterprise IT Support flexible big data analytics Deep dive \u00b6 Six strategies to move to the cloud Cloud Value Frameworks \u00b6 Four business value pilars: Cost savings : Total Cost Ownership. -50% is classical Staff productivity : 62% improvement Operational resilience : -32% downtime Business Agility : 47% improvement ( IDC numbers ) Cost saving \u00b6 Understand the true cost of existing IT capabilities ROI = Cost saving / (sunk cost + migration cost) For sunk cost : assess the hardware depreciation and the potential recovery value by reselling data center or hardware. Migration costs : more difficult to assess, but we can use the break even migration cost per server by defining a target ROI. Now only one unknown in the previous equation: migration cost = Cost Savings / ROI - sunk cost. OPEX (Operational Expenses) For actual cost, we need to consider: Server cost with HW and SW license Storage cost with HW and SW license Network cost with HW and SW license Facilities cost for each of those machines: power, cooling, space SRE cost Extras: like project management, training, legal, advisors, contractors, cost of capital Think about standard depreciation of 3 or 5 years. Match to 3 year reserved instances Use Reserved Instance volume to assess discount Use realistic metrics and ratios like VM density, servers, racks...) Explore current CPU and memory usage Apply cost saving by using automation and configuration as code. Cost assessment can take from 3 weeks to multi months. Migration Evaluator to do on-premise server analysis to optimize cloud migration planning. Cloud readiness \u00b6 Human skills and experience required to transition to the cloud Application readiness to migrate: dependencies, integrations, translation Each stakeholders (devOps, operations, CFO, procurement) have their point of view Additional impacts \u00b6 Cost of delays - risk premium Competition - competitve ability Governance and compliance Operational resilience \u00b6 It really means security and up time. Impact for downtime is a direct cost on business revenue, but also cost get back up: which include 3nd party fee, equipment replacement, recovery activities, investigation cost.... Customer churns, company's reputation... Business agility \u00b6 Look at responding faster, experimenting more, and delivering results in the same or less amount of time. Business agility is about delivering more,respond faster to customer\u2019s requests or problems, develop new product, add features more quickly, expend to new market. Business agility allows customers to innovate by increasing \"failfast\" while reducing risks and costs. Being able to easily shut down failed initiatives without the pain and wasted resources associated with an inflexible on-premises environment. The KPIs to consider includes at least: New app launched per year Time to market for new app. (Observed 20% gain) Time to provision new environments (days) Deployment frequency Time to deploy to production, to test... Features per release (observed 26% more) Total # of defects % defects found in test MTTR: mean time to resolution Response time to defect Customer retention in % New festure adoption in % Value per release in $ (+34% more revenue per user) Moving to the cloud does not have to be a binary proposition . You can move as much or as little of your infrastructure to the cloud as suits your business. Cloud financial management \u00b6 Includes four key areas: Measurement and accountability : establishing cost transparency to ensure visibility Cost Optimization : identify waste, scale based on demand, improve cost efficiency Right sizing : select the lowest cost instance that meets performance requirements. Look at CPU, RAM, storage and network usage to identify downsizing opportunity. See AWS CloudWatch Increase elasticity : shutdown test and dev instances. Automatic scaling. Choose the right pricing model : on-demand, reserved instances (predictable workload), convertible RIs, spot instance.... Use the right storage : automate aging from different S3 services Planning and forecasting : based on actual and future costs and needs AWS pricing calculator to estimate the cost of your architecture solution. AWS price list API AWS Cost Explorer Cloud financial operations : invest in tools, people, and automation Global Infrastructure \u00b6 AWS is a global infrastructure with 28 regions and 2 to 6 separated availability zones per region. Ex: us-west-1-2a. AZ is one or more Data Center with redundant power, networking and connectivity. Isolated from disasters using different facilities. Interconnected with low latency network. Data centers are independent facilities typically hosting 50k servers up to 80k servers. Larger DCs are not desirable because the economy of scale is not that great but the blast radius is becoming too big. Inbound traffic is 110 Tbps within a single DC. James Hamilton pres about AWS infrastructure AWS services are local or very few are global: EC2 is a regional service. Region-scoped services come with availabiltiy and resiliency. IAM is a global service. AWS Local Zone location is an extension of an AWS Region where you can run your latency sensitive applications in geography close to your end-users. You can extend any VPC from the parent AWS Region into Local Zones. Local Zones have their own connections to the internet and support AWS Direct Connect. AWS Wavelength enables developers to build applications that deliver single-digit millisecond latencies to mobile devices and end-users. AWS infrastructure deployments that embed AWS compute and storage services within the telecommunications providers\u2019 datacenters at the edge of the 5G networks, and seamlessly access the breadth of AWS services in the region. Choose an AWS region, depending of your requirements like: Compliance with data governance and legal requirements. Close to users to reduce latency. Service availability within a region . Pricing. Availability and reliability \u00b6 Be sure to get clear agreement on following definitions: Fault Tolerant : characteristic for a system to stay operational even if some of its component fails High Availability : Ensures that systems are always functioning and accessible and that downtime is minimized as much as possible without the need for human intervention. Durability : A byproduct of storage redundancy, durability ensures that a customer\u2019s data will be saved regardless of what happens Reliability : The ability of a system to recover from infrastructure or service failures and the ability to dynamically acquire computing resources to meet demand and mitigate disruptions. Disaster Recovery : preparing for and recovering from a disaster Interact with AWS \u00b6 Management console: services are placed in categories: compute, serverless, database, analytics... AWS CLI SDK for C++, Go, Java, JavaScript, .NET, Node.js, PHP, Python, and Ruby Some application patterns \u00b6 For solution architecture, we need to assess cost, performance, reliability, security and operational excellence. Stateless App \u00b6 The step to grow a stateless app: add vertical scaling by changing the EC2 profile, but while changing, user has out of service. Second step is to scale horizontal, each EC2 instance has static IP address and DNS is configured with 'A record' to get each EC2 end point. But if one instance is gone, the client App will see it down until TTL expires. The reference architecture includes DNS record set with alias record (to point to ALB. Using alias as ALB address may change over time) with TTL of 1 hour. Use load balancers in 3 AZs (to survive disaster) to hide the horizontal scaling of EC2 instances (managed with auto scaling group) where the app runs. Health checks are added to keep system auto adaptable and hide system down, and restricted security group rules to control EC2 instance accesses. ALB and EC instances are in multi different AZs. The EC instances can be set up with reserved capacity to control cost. Stateful app \u00b6 In this case we will add the pattern of shopping cart. If we apply the same architecture as before, at each interaction of the user, it is possible the traffic will be sent to another EC2 instance that started to process the shopping cart. Using ELB with stickiness will help to keep the traffic to the same EC2, but in case of EC2 failure we still loose the cart. An alternate is to use user cookies to keep the cart at each interaction. It is back to a stateless app as state is managed by client and cookie. For security reason the app needs to validate the cookie content. cookie has a limit of 4K data. Another solution is to keep session data into an elastic cache, like Redis, and use the sessionId as key and persisted in a user cookie. So EC2 managing the interaction can get the cart data from the cache using the sessionID. It can be enhanced with a RDS to keep user data. Which can also support the CQRS pattern with read replicas. Cache can be update with data from RDS so if the user is requesting data in session, it hits the cache. Cache and database are set on multi AZ, as well as EC2 instance and load balancer, all to support disaster. Security groups need to be defined to get all traffic to the ELB and limited traffic between ELB and EC2 and between EC2 and cache and EC2 and DB. Another example of stateful app is the ones using image stored on disk. In this case EC2 EBS volume will work only for one app instance, but for multi app scaling out, we need to have a Elastic FS which can be Multi AZ too. Deploying app \u00b6 The easiest solution is to create AMI containing OS, dependencies and app binary. This is completed with User Data to get dynamic configuration. Database data can be restored from Snapshot, and the same for EFS data. Elastic Beanstalk is a developer centric view of the app, hiding the complexity of the IaaS. From one git repository it can automatically handle the deployment details of capacity provisioning, load balancing, auto-scaling, and application health monitoring. Serverless \u00b6 Serveless on AWS is supported by a lot of services: AWS Lambda : Limited by time - short executions, runs on-demand, and automated scaling. Pay per call, duration and memory used. DynamoDB : no sql db, with HA supported by replication across three AZs. millions req/s, trillions rows, 100s TB storage. low latency on read. Support event driven programming with streams: lambda function can read the stream (24h retention). Table oriented, with dynamic attribute but primary key. 400KB max size for one document. It uses the concept of Read Capacity Unit and Write CU. It supports auto-scaling and on-demand throughput. A burst credit is authorized, when empty we get ProvisionedThroughputException. Finally it use the DynamoDB Accelerator to cache data to authorize micro second latency for cached reads. Supports transactions and bulk tx with up to 10 items. AWS Cognito : gives users an identity to interact with the app. AWS API Gateway : API versioning, websocket support, different environment, support authentication and authorization. Handle request throttling. Cache API response. SDK. Support different security approaches: IAM: Great for users / roles already within your AWS account Handle authentication + authorization Leverages Sig v4 Custom Authorizer: Great for 3 rd party tokens Very flexible in terms of what IAM policy is returned Handle Authentication + Authorization Pay per Lambda invocation Cognito User Pool: You manage your own user pool (can be backed by Facebook, Google login etc\u2026) No need to write any custom code Must implement authorization in the backend Amazon S3 AWS SNS & SQS AWS Kinesis Data Firehose Aurora Serverless Step Functions Fargate Lambda@Edge is used to deploy Lambda functions alongside your CloudFront CDN, it is for building more responsive applications, closer to the end user. Lambda is deployed globally. Here are some use cases: Website security and privacy, dynamic webapp at the edge, search engine optimization (SEO), intelligent route across origins and data centers, bot mitigation at the edge, real-time image transformation, A/B testing, user authentication and authorization, user prioritization, user tracking and analytics. Serverless architecture patterns \u00b6 Few write / Lot of reads app (ToDo) \u00b6 The mobile application access application via REST HTTPS through API gateway. This use serverless and users should be able to directly interact with s3 buckets. They first get JWT token to authenticate and the API gateway validates such token. The Gateway delegates to a Lambda function which goes to Dynamo DB. Each of the component supports auto scaling. To improve read throughput cache is used with DAX. Also some of the REST request could be cached in the API gateway. As the application needs to access S3 directly, Cognito generates temporary credentials with STS so the application can authenticate to S3. User's credentials are not saved on the client app. Restricted policy is set to control access to S3 too. To improve throughput we can add DAX as a caching layer in front of DynamoDB: this will also reduce the sizing for DynamoDB. Some of the responses can also be cached at the API gateway level. Serverless hosted web site (Blog) \u00b6 The public web site should scale globally, focus to scale on read, pure static files with some writes. To secure access to S3 content, we use Origin Access Identity and Bucket policy to authorize read only from OAI. To get a welcome message sent when a user register to the app, we can add dynamoDB streams to get changes to the dynamoDB and then calls a lambda that will send an email with the Simple Email Service. DynamoDB Global Table can be used to expose data in different regions by using DynamoDB streams. Microservice \u00b6 Services use REST api to communicate. The service can be dockerized and run with ECS. Integration via REST is done via API gateway and load balancer. Paid content \u00b6 User has to pay to get content (video). We have a DB for users. This is a Serverless solution. Videos are saved in S3. To serve the video, we use Signed URL. So a lambda will build those URLs. CloudFront is used to access videos globally. OAI for security so users cannot bypass it. We can't use S3 signed URL as they are not efficient for global access. Software update distribution \u00b6 The EC2 will be deployed in multi-zones and all is exposed with CloudFront to cache. Big Data pipeline \u00b6 The solution applies the traditional collect, inject, transform and query pattern. IoT Core allows to collect data from IoT devices. Kinesis is used to get data as streams, and then FireHose upload every minute to S3. A Lambda can already do transformation from FireHose. As new files are added to S3 bucket, it trigger a Lambda to call queries defined in Athena. Athena pull the data and build a report published to another S3 bucket that will be used by QuickSight to visualize the data.","title":"Introduction"},{"location":"#amazon-web-service-studies","text":"Info Updated 12/26/2022 Created in 2002, and launched as AWS in 2004 with S3, SQS as first services offering.","title":"Amazon Web Service Studies"},{"location":"#why-cloud","text":"There are 5 main advantages to AWS: Cost Savings : Only pay for what you use, leveraging economy of scale: EC2 instance with different pricing model. Usage from hundreds of thousands of customers is aggregated in the cloud. Moving from capex to variable expense (OPEX). Agility : Teams can experiment and innovate quickly and frequently at minimum cost. Define infrastructure in minutes, as code, not weeks or even months. Elacticity : Scale up and down so no need to guess resource capacity. Innovation : Focus on business apps, not IT infrastructure and data centers. Global Footprint : Extensible, reliable, and secure on global cloud infrastructure. Reach in a minutes. Notes The cloud transition has happened much faster because it yields great value and has fewer blockers, and bigger customer gains drive higher volume reinvestments into the platform. In 2014, every day, AWS adds enough new server capacity to support Amazon's global infrastructure when it was at $7B annual revenu. Scalability is the ability of an application to accommodate growth without changing design. Scalability ensures that systems remain highly available into the future as the business expands. Elasticity is the power to instantly scale computing resources up or down easily. Elastic Load Balancing and Auto Scaling can automatically scale your AWS cloud-based resources up to meet unexpected demand.","title":"Why cloud"},{"location":"#use-cases","text":"Enable to build scalable apps, adaptable to business demand Extend Enterprise IT Support flexible big data analytics","title":"Use cases"},{"location":"#deep-dive","text":"Six strategies to move to the cloud","title":"Deep dive"},{"location":"#cloud-value-frameworks","text":"Four business value pilars: Cost savings : Total Cost Ownership. -50% is classical Staff productivity : 62% improvement Operational resilience : -32% downtime Business Agility : 47% improvement ( IDC numbers )","title":"Cloud Value Frameworks"},{"location":"#cost-saving","text":"Understand the true cost of existing IT capabilities ROI = Cost saving / (sunk cost + migration cost) For sunk cost : assess the hardware depreciation and the potential recovery value by reselling data center or hardware. Migration costs : more difficult to assess, but we can use the break even migration cost per server by defining a target ROI. Now only one unknown in the previous equation: migration cost = Cost Savings / ROI - sunk cost. OPEX (Operational Expenses) For actual cost, we need to consider: Server cost with HW and SW license Storage cost with HW and SW license Network cost with HW and SW license Facilities cost for each of those machines: power, cooling, space SRE cost Extras: like project management, training, legal, advisors, contractors, cost of capital Think about standard depreciation of 3 or 5 years. Match to 3 year reserved instances Use Reserved Instance volume to assess discount Use realistic metrics and ratios like VM density, servers, racks...) Explore current CPU and memory usage Apply cost saving by using automation and configuration as code. Cost assessment can take from 3 weeks to multi months. Migration Evaluator to do on-premise server analysis to optimize cloud migration planning.","title":"Cost saving"},{"location":"#cloud-readiness","text":"Human skills and experience required to transition to the cloud Application readiness to migrate: dependencies, integrations, translation Each stakeholders (devOps, operations, CFO, procurement) have their point of view","title":"Cloud readiness"},{"location":"#additional-impacts","text":"Cost of delays - risk premium Competition - competitve ability Governance and compliance","title":"Additional impacts"},{"location":"#operational-resilience","text":"It really means security and up time. Impact for downtime is a direct cost on business revenue, but also cost get back up: which include 3nd party fee, equipment replacement, recovery activities, investigation cost.... Customer churns, company's reputation...","title":"Operational resilience"},{"location":"#business-agility","text":"Look at responding faster, experimenting more, and delivering results in the same or less amount of time. Business agility is about delivering more,respond faster to customer\u2019s requests or problems, develop new product, add features more quickly, expend to new market. Business agility allows customers to innovate by increasing \"failfast\" while reducing risks and costs. Being able to easily shut down failed initiatives without the pain and wasted resources associated with an inflexible on-premises environment. The KPIs to consider includes at least: New app launched per year Time to market for new app. (Observed 20% gain) Time to provision new environments (days) Deployment frequency Time to deploy to production, to test... Features per release (observed 26% more) Total # of defects % defects found in test MTTR: mean time to resolution Response time to defect Customer retention in % New festure adoption in % Value per release in $ (+34% more revenue per user) Moving to the cloud does not have to be a binary proposition . You can move as much or as little of your infrastructure to the cloud as suits your business.","title":"Business agility"},{"location":"#cloud-financial-management","text":"Includes four key areas: Measurement and accountability : establishing cost transparency to ensure visibility Cost Optimization : identify waste, scale based on demand, improve cost efficiency Right sizing : select the lowest cost instance that meets performance requirements. Look at CPU, RAM, storage and network usage to identify downsizing opportunity. See AWS CloudWatch Increase elasticity : shutdown test and dev instances. Automatic scaling. Choose the right pricing model : on-demand, reserved instances (predictable workload), convertible RIs, spot instance.... Use the right storage : automate aging from different S3 services Planning and forecasting : based on actual and future costs and needs AWS pricing calculator to estimate the cost of your architecture solution. AWS price list API AWS Cost Explorer Cloud financial operations : invest in tools, people, and automation","title":"Cloud financial management"},{"location":"#global-infrastructure","text":"AWS is a global infrastructure with 28 regions and 2 to 6 separated availability zones per region. Ex: us-west-1-2a. AZ is one or more Data Center with redundant power, networking and connectivity. Isolated from disasters using different facilities. Interconnected with low latency network. Data centers are independent facilities typically hosting 50k servers up to 80k servers. Larger DCs are not desirable because the economy of scale is not that great but the blast radius is becoming too big. Inbound traffic is 110 Tbps within a single DC. James Hamilton pres about AWS infrastructure AWS services are local or very few are global: EC2 is a regional service. Region-scoped services come with availabiltiy and resiliency. IAM is a global service. AWS Local Zone location is an extension of an AWS Region where you can run your latency sensitive applications in geography close to your end-users. You can extend any VPC from the parent AWS Region into Local Zones. Local Zones have their own connections to the internet and support AWS Direct Connect. AWS Wavelength enables developers to build applications that deliver single-digit millisecond latencies to mobile devices and end-users. AWS infrastructure deployments that embed AWS compute and storage services within the telecommunications providers\u2019 datacenters at the edge of the 5G networks, and seamlessly access the breadth of AWS services in the region. Choose an AWS region, depending of your requirements like: Compliance with data governance and legal requirements. Close to users to reduce latency. Service availability within a region . Pricing.","title":"Global Infrastructure"},{"location":"#availability-and-reliability","text":"Be sure to get clear agreement on following definitions: Fault Tolerant : characteristic for a system to stay operational even if some of its component fails High Availability : Ensures that systems are always functioning and accessible and that downtime is minimized as much as possible without the need for human intervention. Durability : A byproduct of storage redundancy, durability ensures that a customer\u2019s data will be saved regardless of what happens Reliability : The ability of a system to recover from infrastructure or service failures and the ability to dynamically acquire computing resources to meet demand and mitigate disruptions. Disaster Recovery : preparing for and recovering from a disaster","title":"Availability and reliability"},{"location":"#interact-with-aws","text":"Management console: services are placed in categories: compute, serverless, database, analytics... AWS CLI SDK for C++, Go, Java, JavaScript, .NET, Node.js, PHP, Python, and Ruby","title":"Interact with AWS"},{"location":"#some-application-patterns","text":"For solution architecture, we need to assess cost, performance, reliability, security and operational excellence.","title":"Some application patterns"},{"location":"#stateless-app","text":"The step to grow a stateless app: add vertical scaling by changing the EC2 profile, but while changing, user has out of service. Second step is to scale horizontal, each EC2 instance has static IP address and DNS is configured with 'A record' to get each EC2 end point. But if one instance is gone, the client App will see it down until TTL expires. The reference architecture includes DNS record set with alias record (to point to ALB. Using alias as ALB address may change over time) with TTL of 1 hour. Use load balancers in 3 AZs (to survive disaster) to hide the horizontal scaling of EC2 instances (managed with auto scaling group) where the app runs. Health checks are added to keep system auto adaptable and hide system down, and restricted security group rules to control EC2 instance accesses. ALB and EC instances are in multi different AZs. The EC instances can be set up with reserved capacity to control cost.","title":"Stateless App"},{"location":"#stateful-app","text":"In this case we will add the pattern of shopping cart. If we apply the same architecture as before, at each interaction of the user, it is possible the traffic will be sent to another EC2 instance that started to process the shopping cart. Using ELB with stickiness will help to keep the traffic to the same EC2, but in case of EC2 failure we still loose the cart. An alternate is to use user cookies to keep the cart at each interaction. It is back to a stateless app as state is managed by client and cookie. For security reason the app needs to validate the cookie content. cookie has a limit of 4K data. Another solution is to keep session data into an elastic cache, like Redis, and use the sessionId as key and persisted in a user cookie. So EC2 managing the interaction can get the cart data from the cache using the sessionID. It can be enhanced with a RDS to keep user data. Which can also support the CQRS pattern with read replicas. Cache can be update with data from RDS so if the user is requesting data in session, it hits the cache. Cache and database are set on multi AZ, as well as EC2 instance and load balancer, all to support disaster. Security groups need to be defined to get all traffic to the ELB and limited traffic between ELB and EC2 and between EC2 and cache and EC2 and DB. Another example of stateful app is the ones using image stored on disk. In this case EC2 EBS volume will work only for one app instance, but for multi app scaling out, we need to have a Elastic FS which can be Multi AZ too.","title":"Stateful app"},{"location":"#deploying-app","text":"The easiest solution is to create AMI containing OS, dependencies and app binary. This is completed with User Data to get dynamic configuration. Database data can be restored from Snapshot, and the same for EFS data. Elastic Beanstalk is a developer centric view of the app, hiding the complexity of the IaaS. From one git repository it can automatically handle the deployment details of capacity provisioning, load balancing, auto-scaling, and application health monitoring.","title":"Deploying app"},{"location":"#serverless","text":"Serveless on AWS is supported by a lot of services: AWS Lambda : Limited by time - short executions, runs on-demand, and automated scaling. Pay per call, duration and memory used. DynamoDB : no sql db, with HA supported by replication across three AZs. millions req/s, trillions rows, 100s TB storage. low latency on read. Support event driven programming with streams: lambda function can read the stream (24h retention). Table oriented, with dynamic attribute but primary key. 400KB max size for one document. It uses the concept of Read Capacity Unit and Write CU. It supports auto-scaling and on-demand throughput. A burst credit is authorized, when empty we get ProvisionedThroughputException. Finally it use the DynamoDB Accelerator to cache data to authorize micro second latency for cached reads. Supports transactions and bulk tx with up to 10 items. AWS Cognito : gives users an identity to interact with the app. AWS API Gateway : API versioning, websocket support, different environment, support authentication and authorization. Handle request throttling. Cache API response. SDK. Support different security approaches: IAM: Great for users / roles already within your AWS account Handle authentication + authorization Leverages Sig v4 Custom Authorizer: Great for 3 rd party tokens Very flexible in terms of what IAM policy is returned Handle Authentication + Authorization Pay per Lambda invocation Cognito User Pool: You manage your own user pool (can be backed by Facebook, Google login etc\u2026) No need to write any custom code Must implement authorization in the backend Amazon S3 AWS SNS & SQS AWS Kinesis Data Firehose Aurora Serverless Step Functions Fargate Lambda@Edge is used to deploy Lambda functions alongside your CloudFront CDN, it is for building more responsive applications, closer to the end user. Lambda is deployed globally. Here are some use cases: Website security and privacy, dynamic webapp at the edge, search engine optimization (SEO), intelligent route across origins and data centers, bot mitigation at the edge, real-time image transformation, A/B testing, user authentication and authorization, user prioritization, user tracking and analytics.","title":"Serverless"},{"location":"#serverless-architecture-patterns","text":"","title":"Serverless architecture patterns"},{"location":"ai-ml/","text":"AI and ML services introduction \u00b6 Rekognition \u00b6 For detection inside of image or video. Used for face detection, labeling, celebrity recognition... Code samples Transcribe \u00b6 Audio to Text. Polly \u00b6 Text to speech. It uses speech synthesis markup language (SSML) to help us put emphasize on words, includes breathing sounds, whispering.... Also it is possible to customize the pronunciation of words and do substitution using Pronunciations lexicons. Translate \u00b6 Text translation service that uses advanced machine learning technologies to provide high-quality translation on demand. Lex \u00b6 Build conversational bots or chatbots. Lex includes speech to text, and NLU. Amazon Connect \u00b6 Cloud contact center. Comprehend \u00b6 Uses natural language processing (NLP) to extract insights about the content of documents. Entities extraction,... SageMaker \u00b6 Fully managed machine learning service, for developer and data scientists to develop machine learning models then directly deploy them into a production-ready hosted environment. Benefits: Labelling raw data and active learning Fully managed notebook Amazon SageMaker Clarify helps improve your machine learning (ML) models by detecting potential bias and helping explain the predictions that models make. The figure below explains how SageMaker works for model training, by using S3 bucket as source of data (Ground Truth), ECR Container registry to get predefined image, models are persisted in S3 bucket output folder: Amazon SageMaker always uses Docker containers when running scripts, training algorithms, and deploying models. We can create a training job with the SageMaker console, AWS CLI, Python notebook, or using the SageMaker Python SDK. After you train your machine learning model, you can deploy it using Amazon SageMaker deployment depending of the use cases: One prediction at a time, use real-time inference hosting service Workloads that tolerate cold start can use serverless inference For large payload > 1GB, long processing, use Asynchronous inference Prediction on a dataset use batch processing. Pricing information SageMaker Studio \u00b6 A single, web-based IDE to do all the ML development tasks. It supports the classical ML development steps of: Prepare Data Build Model using Notbook Train and tune Deploy and manage A Domain consists of an associated Amazon Elastic File System (Amazon EFS) volume; a list of authorized users; and a variety of security, application, policy, and Amazon Virtual Private Cloud (Amazon VPC) configurations. To use Studio, an administrator needs to get a Domain sets up. Each user in a domain receives a personal and private home directory within the EFS for notebooks, Git repositories, and data files. Within a domain data scientists and developers can work on the same data and models. After creating a Domain, we got a User Portal to access the environment. Members given access to Studio have a unique sign-in URL that directly opens Studio, and they sign in with their IAM Identity Center credentials (SSO). Once a user from Identity Service is assigned to a Domain, he can start Studio and reach the home page: A user needs to create an executable environment, for example Data Science one, with all the needed library to do Jupyter notebook with AWS SDK boto3. In the environment we can use Notbook, console or terminal. The environment is a EC2 machine (t3.medium) A Simple tutorial \u00b6 The classical model development and deployment steps are: Be sure to have an IAM Role created so SageMaker studio running in a EC2 instance can access remote AWS services, like S3... Inside Studio create a SageMaker notebook instance - Use Jupyter notebook with Conda and Python3 - Prepare the data: create S3 bucket, load csv source as training data set, build train and test data sets by splitting the source data. Train the model to learn from the data. import sagemaker # Use SageMaker estimator sess = sagemaker . Session () xgb = sagemaker . estimator . Estimator ( xgboost_container , role , instance_count = 1 , instance_type = 'ml.m4.xlarge' , output_path = 's3:// {} / {} /output' . format ( bucket_name , prefix ), sagemaker_session = sess ) xgb . set_hyperparameters ( max_depth = 5 , eta = 0.2 , gamma = 4 , min_child_weight = 6 , subsample = 0.8 , silent = 0 , objective = 'binary:logistic' , num_round = 100 ) # fit on the training set xgb . fit ({ 'train' : s3_input_train }) Deploy the model xgb_predictor = xgb . deploy ( initial_instance_count = 1 , instance_type = 'ml.m4.xlarge' ) Evaluate your ML model's performance from sagemaker.serializers import CSVSerializer test_data_array = test_data . drop ([ 'y_no' , 'y_yes' ], axis = 1 ) . values #load the data into an array xgb_predictor . serializer = CSVSerializer () # set the serializer type predictions = xgb_predictor . predict ( test_data_array ) . decode ( 'utf-8' ) # predict! predictions_array = np . fromstring ( predictions [ 1 :], sep = ',' ) # and turn the prediction into an array print ( predictions_array . shape ) It generates a confusion matrix like: Overall Classification Rate: 89 .5% Predicted No Purchase Purchase Observed No Purchase 90 % ( 10769 ) 37 % ( 167 ) Purchase 10 % ( 1133 ) 63 % ( 288 ) Data Wrangler \u00b6 Data Wrangler is a tool to help do feature engineering, and build data workflow, from defining connection to data source, and import dataset: To perform data analysis: looking at data types, Adding table summary to the flow to get means, min, max... per column like pd.describe() results: We can add more analysis like histograms, or add a custom transformation containing several Python Pandas, like creating new columns, dropping columns, do one_hot_encoding for categorical data. We can also use the Feature Store as a centralized storage for all the features potentially created by multiple teams and that can then also be retrieved consumed by multiple teams. The access is not obvious (Use + on last tranformation for example): Develop with sklearn library and deploy with SageMaker \u00b6 A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to model_dir so that it can be hosted later SageMaker Studio Immersion Day . See the tutorial: Using SKLearn with SageMaker and a code to compute a RandomForest for a SaaS company assessing the risk of their customers to churn big-data-tenant-analytics > CompanyRisk . See also: Using the SageMaker Python SDK : SageMaker Python SDK provides several high-level abstractions for working with Amazon SageMaker. Scripts, training and test data sets are in s3. The SDK helps to access SageMaker constructs and deploy the model as a service with reachable endpoint. The endpoint runs a SageMaker-provided Scikit-learn model server and hosts the model produced by your training script. As SageMaker Pythin SDK uses a Docker image to create sklearn environment we can define our own image with our custom libraries via a requirements.txt . We run Scikit-learn training scripts on SageMaker by creating SKLearn Estimators. Deeper dive \u00b6 Onboard to Amazon SageMaker Domain Using IAM Identity Center to define user in IAM-IC and then use Domain in SageMaker to authorize users to login via SSO. Labs: Creating a scikit-learn Random Forest Classifier in AWS SageMaker . Applied to company risk to churn demo in this folder . Using the SageMaker Python SDK . Examples of using SageMaker Python SDK . Amazon SageMaker Workshops gitHub Amazon SageMaker Examples scikit_learn_data_processing_and_model_evaluation Add permissions to your Amazon SageMaker Studio account when we need to enable access from Studio to SageMakerAPI using the IAM policies AmazonSageMakerFullAccess and AWSCloudFormationFullAccess . The lab is little bit old, so now in SageMaker we need to access user via Domain. Please ensure that the role \"arn:aws:iam::4...:role/service-role/AmazonSageMaker-ExecutionRole-20221207T113525\" exists and that its trust relationship policy allows the action \"sts:AssumeRole\" for the service principal \"sagemaker.amazonaws.com\". Also ensure that the role has \"s3:GetObject\" permissions and that the object is located in us-west-2. Forecast \u00b6 Fully managed service that uses statistical and machine learning algorithms to deliver highly accurate time-series forecasts. Kendra \u00b6 Highly accurate and intelligent search service that enables users to search unstructured and structured data using natural language processing and advanced search algorithms. Personalize \u00b6 Fully managed machine learning service that uses your data to generate item recommendations for your users.","title":"AI-ML"},{"location":"ai-ml/#ai-and-ml-services-introduction","text":"","title":"AI and ML services introduction"},{"location":"ai-ml/#rekognition","text":"For detection inside of image or video. Used for face detection, labeling, celebrity recognition... Code samples","title":"Rekognition"},{"location":"ai-ml/#transcribe","text":"Audio to Text.","title":"Transcribe"},{"location":"ai-ml/#polly","text":"Text to speech. It uses speech synthesis markup language (SSML) to help us put emphasize on words, includes breathing sounds, whispering.... Also it is possible to customize the pronunciation of words and do substitution using Pronunciations lexicons.","title":"Polly"},{"location":"ai-ml/#translate","text":"Text translation service that uses advanced machine learning technologies to provide high-quality translation on demand.","title":"Translate"},{"location":"ai-ml/#lex","text":"Build conversational bots or chatbots. Lex includes speech to text, and NLU.","title":"Lex"},{"location":"ai-ml/#amazon-connect","text":"Cloud contact center.","title":"Amazon Connect"},{"location":"ai-ml/#comprehend","text":"Uses natural language processing (NLP) to extract insights about the content of documents. Entities extraction,...","title":"Comprehend"},{"location":"ai-ml/#sagemaker","text":"Fully managed machine learning service, for developer and data scientists to develop machine learning models then directly deploy them into a production-ready hosted environment. Benefits: Labelling raw data and active learning Fully managed notebook Amazon SageMaker Clarify helps improve your machine learning (ML) models by detecting potential bias and helping explain the predictions that models make. The figure below explains how SageMaker works for model training, by using S3 bucket as source of data (Ground Truth), ECR Container registry to get predefined image, models are persisted in S3 bucket output folder: Amazon SageMaker always uses Docker containers when running scripts, training algorithms, and deploying models. We can create a training job with the SageMaker console, AWS CLI, Python notebook, or using the SageMaker Python SDK. After you train your machine learning model, you can deploy it using Amazon SageMaker deployment depending of the use cases: One prediction at a time, use real-time inference hosting service Workloads that tolerate cold start can use serverless inference For large payload > 1GB, long processing, use Asynchronous inference Prediction on a dataset use batch processing. Pricing information","title":"SageMaker"},{"location":"ai-ml/#sagemaker-studio","text":"A single, web-based IDE to do all the ML development tasks. It supports the classical ML development steps of: Prepare Data Build Model using Notbook Train and tune Deploy and manage A Domain consists of an associated Amazon Elastic File System (Amazon EFS) volume; a list of authorized users; and a variety of security, application, policy, and Amazon Virtual Private Cloud (Amazon VPC) configurations. To use Studio, an administrator needs to get a Domain sets up. Each user in a domain receives a personal and private home directory within the EFS for notebooks, Git repositories, and data files. Within a domain data scientists and developers can work on the same data and models. After creating a Domain, we got a User Portal to access the environment. Members given access to Studio have a unique sign-in URL that directly opens Studio, and they sign in with their IAM Identity Center credentials (SSO). Once a user from Identity Service is assigned to a Domain, he can start Studio and reach the home page: A user needs to create an executable environment, for example Data Science one, with all the needed library to do Jupyter notebook with AWS SDK boto3. In the environment we can use Notbook, console or terminal. The environment is a EC2 machine (t3.medium)","title":"SageMaker Studio"},{"location":"ai-ml/#a-simple-tutorial","text":"The classical model development and deployment steps are: Be sure to have an IAM Role created so SageMaker studio running in a EC2 instance can access remote AWS services, like S3... Inside Studio create a SageMaker notebook instance - Use Jupyter notebook with Conda and Python3 - Prepare the data: create S3 bucket, load csv source as training data set, build train and test data sets by splitting the source data. Train the model to learn from the data. import sagemaker # Use SageMaker estimator sess = sagemaker . Session () xgb = sagemaker . estimator . Estimator ( xgboost_container , role , instance_count = 1 , instance_type = 'ml.m4.xlarge' , output_path = 's3:// {} / {} /output' . format ( bucket_name , prefix ), sagemaker_session = sess ) xgb . set_hyperparameters ( max_depth = 5 , eta = 0.2 , gamma = 4 , min_child_weight = 6 , subsample = 0.8 , silent = 0 , objective = 'binary:logistic' , num_round = 100 ) # fit on the training set xgb . fit ({ 'train' : s3_input_train }) Deploy the model xgb_predictor = xgb . deploy ( initial_instance_count = 1 , instance_type = 'ml.m4.xlarge' ) Evaluate your ML model's performance from sagemaker.serializers import CSVSerializer test_data_array = test_data . drop ([ 'y_no' , 'y_yes' ], axis = 1 ) . values #load the data into an array xgb_predictor . serializer = CSVSerializer () # set the serializer type predictions = xgb_predictor . predict ( test_data_array ) . decode ( 'utf-8' ) # predict! predictions_array = np . fromstring ( predictions [ 1 :], sep = ',' ) # and turn the prediction into an array print ( predictions_array . shape ) It generates a confusion matrix like: Overall Classification Rate: 89 .5% Predicted No Purchase Purchase Observed No Purchase 90 % ( 10769 ) 37 % ( 167 ) Purchase 10 % ( 1133 ) 63 % ( 288 )","title":"A Simple tutorial"},{"location":"ai-ml/#data-wrangler","text":"Data Wrangler is a tool to help do feature engineering, and build data workflow, from defining connection to data source, and import dataset: To perform data analysis: looking at data types, Adding table summary to the flow to get means, min, max... per column like pd.describe() results: We can add more analysis like histograms, or add a custom transformation containing several Python Pandas, like creating new columns, dropping columns, do one_hot_encoding for categorical data. We can also use the Feature Store as a centralized storage for all the features potentially created by multiple teams and that can then also be retrieved consumed by multiple teams. The access is not obvious (Use + on last tranformation for example):","title":"Data Wrangler"},{"location":"ai-ml/#develop-with-sklearn-library-and-deploy-with-sagemaker","text":"A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to model_dir so that it can be hosted later SageMaker Studio Immersion Day . See the tutorial: Using SKLearn with SageMaker and a code to compute a RandomForest for a SaaS company assessing the risk of their customers to churn big-data-tenant-analytics > CompanyRisk . See also: Using the SageMaker Python SDK : SageMaker Python SDK provides several high-level abstractions for working with Amazon SageMaker. Scripts, training and test data sets are in s3. The SDK helps to access SageMaker constructs and deploy the model as a service with reachable endpoint. The endpoint runs a SageMaker-provided Scikit-learn model server and hosts the model produced by your training script. As SageMaker Pythin SDK uses a Docker image to create sklearn environment we can define our own image with our custom libraries via a requirements.txt . We run Scikit-learn training scripts on SageMaker by creating SKLearn Estimators.","title":"Develop with sklearn library and deploy with SageMaker"},{"location":"ai-ml/#deeper-dive","text":"Onboard to Amazon SageMaker Domain Using IAM Identity Center to define user in IAM-IC and then use Domain in SageMaker to authorize users to login via SSO. Labs: Creating a scikit-learn Random Forest Classifier in AWS SageMaker . Applied to company risk to churn demo in this folder . Using the SageMaker Python SDK . Examples of using SageMaker Python SDK . Amazon SageMaker Workshops gitHub Amazon SageMaker Examples scikit_learn_data_processing_and_model_evaluation Add permissions to your Amazon SageMaker Studio account when we need to enable access from Studio to SageMakerAPI using the IAM policies AmazonSageMakerFullAccess and AWSCloudFormationFullAccess . The lab is little bit old, so now in SageMaker we need to access user via Domain. Please ensure that the role \"arn:aws:iam::4...:role/service-role/AmazonSageMaker-ExecutionRole-20221207T113525\" exists and that its trust relationship policy allows the action \"sts:AssumeRole\" for the service principal \"sagemaker.amazonaws.com\". Also ensure that the role has \"s3:GetObject\" permissions and that the object is located in us-west-2.","title":"Deeper dive"},{"location":"ai-ml/#forecast","text":"Fully managed service that uses statistical and machine learning algorithms to deliver highly accurate time-series forecasts.","title":"Forecast"},{"location":"ai-ml/#kendra","text":"Highly accurate and intelligent search service that enables users to search unstructured and structured data using natural language processing and advanced search algorithms.","title":"Kendra"},{"location":"ai-ml/#personalize","text":"Fully managed machine learning service that uses your data to generate item recommendations for your users.","title":"Personalize"},{"location":"analytics/","text":"Analytics Services \u00b6 AWS Athena \u00b6 AWS Athena runs analytics directly on S3 files, using SQL language to query the files (CSV, JSON, Avro, Parquet...). S3 Access Logs log all the requests made to buckets, and Athena can then be used to run serverless analytics on top of the logs files. No need to load the data to Athena, the query is executed on top of S3. Queries are done on high availability capability so will succeed, and scale based on the data size. No need for complex ETL jobs to prepare our data for analytics. Athena integrates with Amazon QuickSight for easy data visualization. Integrated with AWS Glue Data Catalog , allowing us to create a unified metadata repository across various services, crawl data sources to discover schemas and populate our Catalog with new and modified table and partition definitions, and maintain schema versioning. Pricing pet TB of data scanned. It also includes Federated Query to run SQL queries across data stored in relational and non-relational , object, and custom data sources. It uses the Data Source Connectors which executes a Lambda to run the Federated Query. Prefer using Apache Parquet data format for better performance and optimized cost. It is a columnar file format that provides optimizations to speed up queries and is a far more efficient file format than CSV or JSON Partition our data in S3 folder. CSV to Parquet For Python, Pandas support it by reading the csv file into dataframe using read_csv and writing that dataframe to parquet file using to_parquet . Apache Drill has also such tool. In Spark the data frame has write.parquet API. Finaly AWS Glue can also do such transformation. Simple demo script \u00b6 Create a S3 bucket to keep results of Athena queries. Create a second S3 bucket to keep source data, and upload a csv file as data source. Create a database in Athena: CREATE DATABASE mydatabase Define SQL query to create table to match the source (external table) and run it in the Editor. CREATE EXTERNAL TABLE IF NOT EXISTS tablename - datasource ( ` Date ` DATE , Time STRING , Location STRING , ... ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n' LOCATION 's3://url-to-bucket' Lambda example \u00b6 It is possible to run Athena query from Lambda function and get the result pushed in an output S3 bucket or in another place. See the lambda code in the labs/lambdas/s3-aurora folder . See boto3 Athena API. Deeper dive \u00b6 Product documentation Getting started with Athena How do I analyze my Amazon S3 server access logs using Athena? See also code sample . Calling SageMaker function from an Athena Query to do ML . Elastic MapReduce - EMR \u00b6 EMR is a cluster of EC2 instances which are nodes in Hadoop (HDFS). We can use reserved instance and spot instances to reduce costs. There are three node types: Master node : coordinates cluster, and distribution of data and tasks among other nodes. Core node : run tasks and store data in the Hadoop Distributed File System (HDFS) Task node : (optional) runs tasks and does not store data in HDFS It comes bundled with Spark, HBase, Presto, Flink... When launching a cluster, it performs bootstrap actions to install custom software and applications. When the cluster is in running state, we can submit work to it. Work includes a set of steps. The cluster can auto terminate at the end of the last step. You can submit one or more ordered steps to an Amazon EMR cluster. Each step is a unit of work that contains instructions to manipulate data for processing by software installed on the cluster. For auto scaling of the task nodes, it uses Spot instances. Master node should be Reserved instance. Getting started tutorial with Spark, Pyspark script stored in S3. The steps are summarized below and python and data are in the folder: labs/analytics/emr-starting . The goal is to process food establishment inspection data. Create a cluster using the script create-cluster.sh (it uses aws emr create-cluster command). In the console, once the cluster is in waiting mode, add a step with Spark Application, in cluster deployment mode, Or run deploy-app.sh (it uses aws emr add-steps command). The results looks like name,total_red_violations SUBWAY,322 T-MOBILE PARK,315 WHOLE FOODS MARKET,299 ... For other examples see the playground . See Pricing based on EC2 type and region. EMR Serverless \u00b6 The newest and easiest way for data analysts and engineers to run open-source big data analytics frameworks without configuring, managing, and scaling clusters and servers. Mostly work from EMR Studio, we can ubmit jobsv ia APIs or EMR Studio. We an also submit jobs using workflow orchestration services like AWS Step Functions, Apache Airflow, or AWS Managed Workflows for Apache Airflow. Logging: By default, EMR Serverless stores application logs securely in Amazon EMR managed storage for a maximum of 30 days. Before our jobs can send log data to Amazon S3, we must allow s3:PutObject on the arn:aws:s3:::.../* s3 bucket, in the permissions policy for the job runtime role. Monitoring with CloudWatch custom dashboard: See the CloudFormation definition under emr-serverless and using the command ./defineCWdashboard.sh , we can get a dashboard for the Serverless EMR application: So we need to define a dashboard per application. Every minute EMR Serverless emits (CPUAllocated, IdleWorkerCount,MaxCPUAllowed) metrics at the application level as well at the worker-type and capacity-allocation-type levels. Tutorial - getting started . We need an IAM role with a custom trust policy to enable others to perform actions in this account (see role EMRServerlessS3RuntimeRole and security policy EMRServerlessS3AndGlueAccessPolicy ). Use EMR Studio and create an application. We can now use Graviton as CPU Define the PySpark script to be used and put it in a S3 bucket. For example WordCount.py aws s3 cp s3://us-east-1.elasticmapreduce/emr-containers/samples/wordcount/scripts/wordcount.py s3://jb-data-set/scripts/ Define a job using the script, using the Spark properties of: --conf spark.executor.cores=1 --conf spark.executor.memory=4g --conf spark.driver.cores=1 --conf spark.driver.memory=4g --conf spark.executor.instances=1 Once the job run status shows as Success, you can view the output of the job in the S3 bucket. Log should be in logs folder. Delete output from s3 bucket: aws s3 rm s3://jb-data-set/emr-serverless-spark/ --recursive WordCount.py app with CLI: Scripts are under emr-serverless If the application was not created before like in manual step above, use the command like (whicj is in the script createApplication.sh ) aws emr-serverless create-application --release-label emr-6.8.0 --type \"SPARK\" --name My_First_Application Get the application ID: ./getApplicationId.sh My_First_Application Be sure to have the wordcount.py in the scripts folder in s3 bucket aws s3 cp s3://us-east-1.elasticmapreduce/emr-containers/samples/wordcount/scripts/wordcount.py s3://DOC-EXAMPLE-BUCKET/scripts/ Get the role ARN aws iam list-roles | jq -r '.Roles[] | select(.RoleName==\"EMRServerlessS3RuntimeRole\") | .Arn ' * Submit the job: ./submitJob.sh . The output looks like: { \"applicationId\": \"00f6b0eou5biqd0l\", \"jobRunId\": \"00f6b25bek7v3f0l\", \"arn\": \"arn:aws:emr-serverless:us-west-2:....:/applications/00f6b0eou5biqd0l/jobruns/00f6b25bek7v3f0l\" } EMR on EKS \u00b6 Advantages: Run with other workload deployed on EKS. Fully managed lifecycle of the EMR jobs. 3x faster performance. Improves resource utilization and simplifies infrastructure management across multiple Availability Zones. Deploy in seconds instead of minutes. Centrally manage a common computing platform to consolidate EMR workloads with other apps. Access to built-in monitoring and logging functionality. Reduce operational overhead with automated Kubernetes cluster management and OS patching Amazon EMR uses virtual clusters to run jobs and host endpoints. A virtual cluster is a Kubernetes namespace that Amazon EMR is registered with. Deployment and setup \u00b6 Prepare a EKS cluster Amazon EMR on EKS needs CoreDNS for running jobs on EKS cluster. So update CoreDNS if needed. Enable cluster access for Amazon EMR on EKS to a specific namespace by creating a k8s role, role binding to a k8s user, and map this user to the service linked role AWSServiceRoleForAmazonEMRContainers . Enable IAM Roles for Service Accounts (IRSA) on the EKS cluster by creating an OIDC identity provider Create a job execution role Deeper dive \u00b6 QuickSight \u00b6 Dashboard tool, serverless, machine learning powered BI service. Two type of persona: dashboard developers and end-users (read-only on the dashboard). Integrated with RDS, Aurora, Athena, S3, RedShift, OpenSearch, Timestream, with Saleforce, Jira... It can integrate to any JDBC compliant database. It can import CSV, XLSX, JSON, TSV files and log files. If data is imported inside QuickSight, it uses in memory computation using SPICE (Super-fast, Parallel, In-memory Calculation Engine) engine. The development process is described in the figure below (link to AWS doc) Defining a dataset and then working in removing column, applying filters, changing field names or data types, adding calculated fields, or use SQL to joins between tables. Here is an example of integration with a table in Aurora Add Visual. A Visual can include multiple dataset and then multiple sheets. An Interactive Sheet is a collection of data expressed in visuals that users can interact with when the sheet is published to a dashboard. A Paginated Report is a collection of tables, charts, and visuals that are used to convey business critical information. Create Dashboard from an Analysis. Share dashboard so it can be seen by end users. With enterprise edition we can define groups of users, and share dataset, visual and dashboard with group or individual user. The dataset can be shared between developers so they can develop their own analysis. Visualizations can also be shared during development, then the readonly dashboard is shared to end users. To get input data for a dashboard we can define parameters. Parameters can also be used for exchanging context between sheets. QuickSight can generate smart queries that uses only required tables rather than joining in all tables defined in the dataset. Some how to \u00b6 Invite user using the right top menu (human icon), and Manage QuickSight, then invite people by email. Create Group, and then add users. The 3 letters to search really need to be the first 3 letters. Once data are in SPICE, they need to be refreshed from the data source to get the last records updates. A refresh can be scheduled. Add trend and add X axis variable coming from the dataset and Value for Y. To add a widget to filter the data, use filter, select the column and add it to the current sheet (contextual menu) To add a transition from a sheet to another by passing the value of the selected elements. Deeper Dive \u00b6 youtube channel for QuickSight. Demo Central - learning Very good workshop. 5 stars Build your first quicksight dashboard - video Embedding Amazon QuickSight dashboards in your applications. Embedding Analytics dashboard in application. Glue \u00b6 It is a serverless, managed service to do ETL to do data pipeline. You can discover and connect to over 70 diverse data sources, manage our data in a centralized data catalog, and visually create, run, and monitor ETL pipelines to load data into our data lakes. It can also do a data catalog, by starting some crawler to different data sources. To avoid reprocessing data, it use Job Bookmarks. Glue Elastic View is a feature to combine and replicate data across multiple data stores using SQL. It is like virtual table. Glue DataBrew to clean and normalize data using pre-built transformation. Glue Studio, GUI to create, run and monitor ETL jobs Glue Streaming ETL built on Apache Spark Structured Streaming to do data streaming compatible with Kafka, MSK and Kinesis data streaming. Lake Formation \u00b6 An abstraction layer on top of Glue to build data lake in days instead of month. The lake is on S3. It adds access control at the column level. Example of a big data ingestion pipeline \u00b6 The requirements are: Use serverless ingestion pipeline. Collect and transform data in real-time. Support SQL query on transformed data. Persist SQL results into S3. Keep into warehouse and create dashboard. Data could come from IoT devices A potential architecture will be:","title":"Analytics"},{"location":"analytics/#analytics-services","text":"","title":"Analytics Services"},{"location":"analytics/#aws-athena","text":"AWS Athena runs analytics directly on S3 files, using SQL language to query the files (CSV, JSON, Avro, Parquet...). S3 Access Logs log all the requests made to buckets, and Athena can then be used to run serverless analytics on top of the logs files. No need to load the data to Athena, the query is executed on top of S3. Queries are done on high availability capability so will succeed, and scale based on the data size. No need for complex ETL jobs to prepare our data for analytics. Athena integrates with Amazon QuickSight for easy data visualization. Integrated with AWS Glue Data Catalog , allowing us to create a unified metadata repository across various services, crawl data sources to discover schemas and populate our Catalog with new and modified table and partition definitions, and maintain schema versioning. Pricing pet TB of data scanned. It also includes Federated Query to run SQL queries across data stored in relational and non-relational , object, and custom data sources. It uses the Data Source Connectors which executes a Lambda to run the Federated Query. Prefer using Apache Parquet data format for better performance and optimized cost. It is a columnar file format that provides optimizations to speed up queries and is a far more efficient file format than CSV or JSON Partition our data in S3 folder. CSV to Parquet For Python, Pandas support it by reading the csv file into dataframe using read_csv and writing that dataframe to parquet file using to_parquet . Apache Drill has also such tool. In Spark the data frame has write.parquet API. Finaly AWS Glue can also do such transformation.","title":"AWS Athena"},{"location":"analytics/#simple-demo-script","text":"Create a S3 bucket to keep results of Athena queries. Create a second S3 bucket to keep source data, and upload a csv file as data source. Create a database in Athena: CREATE DATABASE mydatabase Define SQL query to create table to match the source (external table) and run it in the Editor. CREATE EXTERNAL TABLE IF NOT EXISTS tablename - datasource ( ` Date ` DATE , Time STRING , Location STRING , ... ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n' LOCATION 's3://url-to-bucket'","title":"Simple demo script"},{"location":"analytics/#lambda-example","text":"It is possible to run Athena query from Lambda function and get the result pushed in an output S3 bucket or in another place. See the lambda code in the labs/lambdas/s3-aurora folder . See boto3 Athena API.","title":"Lambda example"},{"location":"analytics/#deeper-dive","text":"Product documentation Getting started with Athena How do I analyze my Amazon S3 server access logs using Athena? See also code sample . Calling SageMaker function from an Athena Query to do ML .","title":"Deeper dive"},{"location":"analytics/#elastic-mapreduce-emr","text":"EMR is a cluster of EC2 instances which are nodes in Hadoop (HDFS). We can use reserved instance and spot instances to reduce costs. There are three node types: Master node : coordinates cluster, and distribution of data and tasks among other nodes. Core node : run tasks and store data in the Hadoop Distributed File System (HDFS) Task node : (optional) runs tasks and does not store data in HDFS It comes bundled with Spark, HBase, Presto, Flink... When launching a cluster, it performs bootstrap actions to install custom software and applications. When the cluster is in running state, we can submit work to it. Work includes a set of steps. The cluster can auto terminate at the end of the last step. You can submit one or more ordered steps to an Amazon EMR cluster. Each step is a unit of work that contains instructions to manipulate data for processing by software installed on the cluster. For auto scaling of the task nodes, it uses Spot instances. Master node should be Reserved instance. Getting started tutorial with Spark, Pyspark script stored in S3. The steps are summarized below and python and data are in the folder: labs/analytics/emr-starting . The goal is to process food establishment inspection data. Create a cluster using the script create-cluster.sh (it uses aws emr create-cluster command). In the console, once the cluster is in waiting mode, add a step with Spark Application, in cluster deployment mode, Or run deploy-app.sh (it uses aws emr add-steps command). The results looks like name,total_red_violations SUBWAY,322 T-MOBILE PARK,315 WHOLE FOODS MARKET,299 ... For other examples see the playground . See Pricing based on EC2 type and region.","title":"Elastic MapReduce - EMR"},{"location":"analytics/#emr-serverless","text":"The newest and easiest way for data analysts and engineers to run open-source big data analytics frameworks without configuring, managing, and scaling clusters and servers. Mostly work from EMR Studio, we can ubmit jobsv ia APIs or EMR Studio. We an also submit jobs using workflow orchestration services like AWS Step Functions, Apache Airflow, or AWS Managed Workflows for Apache Airflow. Logging: By default, EMR Serverless stores application logs securely in Amazon EMR managed storage for a maximum of 30 days. Before our jobs can send log data to Amazon S3, we must allow s3:PutObject on the arn:aws:s3:::.../* s3 bucket, in the permissions policy for the job runtime role. Monitoring with CloudWatch custom dashboard: See the CloudFormation definition under emr-serverless and using the command ./defineCWdashboard.sh , we can get a dashboard for the Serverless EMR application: So we need to define a dashboard per application. Every minute EMR Serverless emits (CPUAllocated, IdleWorkerCount,MaxCPUAllowed) metrics at the application level as well at the worker-type and capacity-allocation-type levels. Tutorial - getting started . We need an IAM role with a custom trust policy to enable others to perform actions in this account (see role EMRServerlessS3RuntimeRole and security policy EMRServerlessS3AndGlueAccessPolicy ). Use EMR Studio and create an application. We can now use Graviton as CPU Define the PySpark script to be used and put it in a S3 bucket. For example WordCount.py aws s3 cp s3://us-east-1.elasticmapreduce/emr-containers/samples/wordcount/scripts/wordcount.py s3://jb-data-set/scripts/ Define a job using the script, using the Spark properties of: --conf spark.executor.cores=1 --conf spark.executor.memory=4g --conf spark.driver.cores=1 --conf spark.driver.memory=4g --conf spark.executor.instances=1 Once the job run status shows as Success, you can view the output of the job in the S3 bucket. Log should be in logs folder. Delete output from s3 bucket: aws s3 rm s3://jb-data-set/emr-serverless-spark/ --recursive WordCount.py app with CLI: Scripts are under emr-serverless If the application was not created before like in manual step above, use the command like (whicj is in the script createApplication.sh ) aws emr-serverless create-application --release-label emr-6.8.0 --type \"SPARK\" --name My_First_Application Get the application ID: ./getApplicationId.sh My_First_Application Be sure to have the wordcount.py in the scripts folder in s3 bucket aws s3 cp s3://us-east-1.elasticmapreduce/emr-containers/samples/wordcount/scripts/wordcount.py s3://DOC-EXAMPLE-BUCKET/scripts/ Get the role ARN aws iam list-roles | jq -r '.Roles[] | select(.RoleName==\"EMRServerlessS3RuntimeRole\") | .Arn ' * Submit the job: ./submitJob.sh . The output looks like: { \"applicationId\": \"00f6b0eou5biqd0l\", \"jobRunId\": \"00f6b25bek7v3f0l\", \"arn\": \"arn:aws:emr-serverless:us-west-2:....:/applications/00f6b0eou5biqd0l/jobruns/00f6b25bek7v3f0l\" }","title":"EMR Serverless"},{"location":"analytics/#emr-on-eks","text":"Advantages: Run with other workload deployed on EKS. Fully managed lifecycle of the EMR jobs. 3x faster performance. Improves resource utilization and simplifies infrastructure management across multiple Availability Zones. Deploy in seconds instead of minutes. Centrally manage a common computing platform to consolidate EMR workloads with other apps. Access to built-in monitoring and logging functionality. Reduce operational overhead with automated Kubernetes cluster management and OS patching Amazon EMR uses virtual clusters to run jobs and host endpoints. A virtual cluster is a Kubernetes namespace that Amazon EMR is registered with.","title":"EMR on EKS"},{"location":"analytics/#quicksight","text":"Dashboard tool, serverless, machine learning powered BI service. Two type of persona: dashboard developers and end-users (read-only on the dashboard). Integrated with RDS, Aurora, Athena, S3, RedShift, OpenSearch, Timestream, with Saleforce, Jira... It can integrate to any JDBC compliant database. It can import CSV, XLSX, JSON, TSV files and log files. If data is imported inside QuickSight, it uses in memory computation using SPICE (Super-fast, Parallel, In-memory Calculation Engine) engine. The development process is described in the figure below (link to AWS doc) Defining a dataset and then working in removing column, applying filters, changing field names or data types, adding calculated fields, or use SQL to joins between tables. Here is an example of integration with a table in Aurora Add Visual. A Visual can include multiple dataset and then multiple sheets. An Interactive Sheet is a collection of data expressed in visuals that users can interact with when the sheet is published to a dashboard. A Paginated Report is a collection of tables, charts, and visuals that are used to convey business critical information. Create Dashboard from an Analysis. Share dashboard so it can be seen by end users. With enterprise edition we can define groups of users, and share dataset, visual and dashboard with group or individual user. The dataset can be shared between developers so they can develop their own analysis. Visualizations can also be shared during development, then the readonly dashboard is shared to end users. To get input data for a dashboard we can define parameters. Parameters can also be used for exchanging context between sheets. QuickSight can generate smart queries that uses only required tables rather than joining in all tables defined in the dataset.","title":"QuickSight"},{"location":"analytics/#some-how-to","text":"Invite user using the right top menu (human icon), and Manage QuickSight, then invite people by email. Create Group, and then add users. The 3 letters to search really need to be the first 3 letters. Once data are in SPICE, they need to be refreshed from the data source to get the last records updates. A refresh can be scheduled. Add trend and add X axis variable coming from the dataset and Value for Y. To add a widget to filter the data, use filter, select the column and add it to the current sheet (contextual menu) To add a transition from a sheet to another by passing the value of the selected elements.","title":"Some how to"},{"location":"analytics/#deeper-dive_2","text":"youtube channel for QuickSight. Demo Central - learning Very good workshop. 5 stars Build your first quicksight dashboard - video Embedding Amazon QuickSight dashboards in your applications. Embedding Analytics dashboard in application.","title":"Deeper Dive"},{"location":"analytics/#glue","text":"It is a serverless, managed service to do ETL to do data pipeline. You can discover and connect to over 70 diverse data sources, manage our data in a centralized data catalog, and visually create, run, and monitor ETL pipelines to load data into our data lakes. It can also do a data catalog, by starting some crawler to different data sources. To avoid reprocessing data, it use Job Bookmarks. Glue Elastic View is a feature to combine and replicate data across multiple data stores using SQL. It is like virtual table. Glue DataBrew to clean and normalize data using pre-built transformation. Glue Studio, GUI to create, run and monitor ETL jobs Glue Streaming ETL built on Apache Spark Structured Streaming to do data streaming compatible with Kafka, MSK and Kinesis data streaming.","title":"Glue"},{"location":"analytics/#lake-formation","text":"An abstraction layer on top of Glue to build data lake in days instead of month. The lake is on S3. It adds access control at the column level.","title":"Lake Formation"},{"location":"analytics/#example-of-a-big-data-ingestion-pipeline","text":"The requirements are: Use serverless ingestion pipeline. Collect and transform data in real-time. Support SQL query on transformed data. Persist SQL results into S3. Keep into warehouse and create dashboard. Data could come from IoT devices A potential architecture will be:","title":"Example of a big data ingestion pipeline"},{"location":"coding/","text":"Coding practices \u00b6 SDK \u00b6 Supports a lot of languages to integrate with a lot of managed services from your business application. DevOps \u00b6 CloudFormation \u00b6 See separate note . CodeCommit \u00b6 Version control fully managed service to manage Git repositories. HA, secured, encryption at rest and in transit. Be sure to get the Git Credentials for the IAM user we will use to do the Git repository actions. Setup SSH connection to CodeCommit Elastic Beanstalk \u00b6 Elastic Beanstalk is a developer centric view of the deployment of web apps on AWS using EC2, ALB, ELB, RDS, ASG... It is a managed service and it automatically manages capacity provisioning, load balancing, scaling, health, configuration... An application is a collection of Beanstalk components (environments, versions, configurations). It defines two preconfigured environments: Web Server Tier: classical ELB, Auto scaling group and EC2s. Worker environment with the use of SQS queue. It uses CloudFormation to deploy the application and the environment. A custom AMI can improve provisioning times when instances are launched in your environment if you need to install a lot of software that isn't included in the standard AMIs. Elastic Container Registry \u00b6 AWS managed container image registry service that is secure, scalable, and reliable. An Amazon ECR repository contains your Docker images , Open Container Initiative (OCI) images, and OCI compatible artifacts. One repository per app. Client must authenticate to Amazon ECR registries as an AWS user before it can push and pull images. You can control access to your repositories and the images within them with repository policies. As a developer you need AWS CLI and Docker. Pricing : pay for the amount of data you store in your repositories and for the data transfer from your image pushes and pulls. 50 GB per month of always-free storage for their public repositories. For private 500MB first year. Data transfer to services within the same region is free of charge. Demonstration \u00b6 Create one ECR repository per app or microservice. From you Laptop you docker build with the ECR repo URL. aws ecr help # Get the authentication token and authenticate the docker client aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin <...>.amazonaws.com # Can also use the docker cli, see The View push commands for your repository docker tag jbcodeforce/autonomous-car-ride:latest <...>.amazonaws.com/jbcodeforce/autonomous-car-ride:latest docker push <...>.amazonaws.com/jbcodeforce/autonomous-car-ride:latest If you want to run your application using docker engine inside of EC2, create a simple EC2 and then ssh to it and add docker, and do a docker run. Here are the installation you need: sudo apt-get update sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg-agent \\ software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo apt-get install docker-ce docker-ce-cli containerd.io apt-cache madison docker-ce sudo apt-get install docker-ce docker-ce-cli containerd.io sudo apt install docker.io App Runner \u00b6 Chalice \u00b6 A python framework to build serverless applications. We can have a REST API deployed to Amazon API Gateway and AWS Lambda in minutes. Serverless Application Model \u00b6 SAM is an open-source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. During deployment, SAM transforms and expands the SAM syntax into AWS CloudFormation syntax. SAM CLI provides a Lambda-like execution environment that lets you locally build, test, and debug applications defined by SAM templates or through the AWS Cloud Development Kit (CDK). Install , which can be summarized as: brew install aws-sam-cli # or upgrade brew upgrade aws-sam-cli sam --version Serverless pattern collection CodePipeline \u00b6 AWS CodePipeline is a continuous delivery service. Getting started Pricing 1$ / month per pipeline. All pipelines are free for the first 30 days. CodeBuild \u00b6 AWS CodeBuild is a fully managed build service that compiles source code, runs tests, and produces software packages that are ready to deploy. CodeDeploy \u00b6 CodeStar \u00b6 AWS CodeStar provides a unified user interface, enabling you to easily manage your software development activities in one place. To start a project, you can choose from a variety of AWS CodeStar templates for Amazon EC2, AWS Lambda, and AWS Elastic Beanstalk. You have the option to choose AWS CodeCommit or GitHub to use as your project\u2019s source control. There is no additional charge for AWS CodeStar. Getting started Product documentation CloudWatch \u00b6 AWS Proton \u00b6 Automated infrastructure as code provisioning and deployment of serverless and container-based applications.","title":"Practices"},{"location":"coding/#coding-practices","text":"","title":"Coding practices"},{"location":"coding/#sdk","text":"Supports a lot of languages to integrate with a lot of managed services from your business application.","title":"SDK"},{"location":"coding/#devops","text":"","title":"DevOps"},{"location":"coding/#cloudformation","text":"See separate note .","title":"CloudFormation"},{"location":"coding/#codecommit","text":"Version control fully managed service to manage Git repositories. HA, secured, encryption at rest and in transit. Be sure to get the Git Credentials for the IAM user we will use to do the Git repository actions. Setup SSH connection to CodeCommit","title":"CodeCommit"},{"location":"coding/#elastic-beanstalk","text":"Elastic Beanstalk is a developer centric view of the deployment of web apps on AWS using EC2, ALB, ELB, RDS, ASG... It is a managed service and it automatically manages capacity provisioning, load balancing, scaling, health, configuration... An application is a collection of Beanstalk components (environments, versions, configurations). It defines two preconfigured environments: Web Server Tier: classical ELB, Auto scaling group and EC2s. Worker environment with the use of SQS queue. It uses CloudFormation to deploy the application and the environment. A custom AMI can improve provisioning times when instances are launched in your environment if you need to install a lot of software that isn't included in the standard AMIs.","title":"Elastic Beanstalk"},{"location":"coding/#elastic-container-registry","text":"AWS managed container image registry service that is secure, scalable, and reliable. An Amazon ECR repository contains your Docker images , Open Container Initiative (OCI) images, and OCI compatible artifacts. One repository per app. Client must authenticate to Amazon ECR registries as an AWS user before it can push and pull images. You can control access to your repositories and the images within them with repository policies. As a developer you need AWS CLI and Docker. Pricing : pay for the amount of data you store in your repositories and for the data transfer from your image pushes and pulls. 50 GB per month of always-free storage for their public repositories. For private 500MB first year. Data transfer to services within the same region is free of charge.","title":"Elastic Container Registry"},{"location":"coding/#demonstration","text":"Create one ECR repository per app or microservice. From you Laptop you docker build with the ECR repo URL. aws ecr help # Get the authentication token and authenticate the docker client aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin <...>.amazonaws.com # Can also use the docker cli, see The View push commands for your repository docker tag jbcodeforce/autonomous-car-ride:latest <...>.amazonaws.com/jbcodeforce/autonomous-car-ride:latest docker push <...>.amazonaws.com/jbcodeforce/autonomous-car-ride:latest If you want to run your application using docker engine inside of EC2, create a simple EC2 and then ssh to it and add docker, and do a docker run. Here are the installation you need: sudo apt-get update sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg-agent \\ software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo apt-get install docker-ce docker-ce-cli containerd.io apt-cache madison docker-ce sudo apt-get install docker-ce docker-ce-cli containerd.io sudo apt install docker.io","title":"Demonstration"},{"location":"coding/#app-runner","text":"","title":"App Runner"},{"location":"coding/#chalice","text":"A python framework to build serverless applications. We can have a REST API deployed to Amazon API Gateway and AWS Lambda in minutes.","title":"Chalice"},{"location":"coding/#serverless-application-model","text":"SAM is an open-source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. During deployment, SAM transforms and expands the SAM syntax into AWS CloudFormation syntax. SAM CLI provides a Lambda-like execution environment that lets you locally build, test, and debug applications defined by SAM templates or through the AWS Cloud Development Kit (CDK). Install , which can be summarized as: brew install aws-sam-cli # or upgrade brew upgrade aws-sam-cli sam --version Serverless pattern collection","title":"Serverless Application Model"},{"location":"coding/#codepipeline","text":"AWS CodePipeline is a continuous delivery service. Getting started Pricing 1$ / month per pipeline. All pipelines are free for the first 30 days.","title":"CodePipeline"},{"location":"coding/#codebuild","text":"AWS CodeBuild is a fully managed build service that compiles source code, runs tests, and produces software packages that are ready to deploy.","title":"CodeBuild"},{"location":"coding/#codedeploy","text":"","title":"CodeDeploy"},{"location":"coding/#codestar","text":"AWS CodeStar provides a unified user interface, enabling you to easily manage your software development activities in one place. To start a project, you can choose from a variety of AWS CodeStar templates for Amazon EC2, AWS Lambda, and AWS Elastic Beanstalk. You have the option to choose AWS CodeCommit or GitHub to use as your project\u2019s source control. There is no additional charge for AWS CodeStar. Getting started Product documentation","title":"CodeStar"},{"location":"coding/#cloudwatch","text":"","title":"CloudWatch"},{"location":"coding/#aws-proton","text":"Automated infrastructure as code provisioning and deployment of serverless and container-based applications.","title":"AWS Proton"},{"location":"coding/cdk/","text":"Cloud Development Kit - CDK \u00b6 The AWS CDK lets you build reliable, scalable, cost-effective applications in the cloud with the considerable expressive power of a programming language (Go, Java, Python, C#, JavaScript, Typescript). Build with high-level constructs that automatically provide sensible, secure defaults for your AWS resources, defining more infrastructure with less code. It uses AWS CloudFormation to perform infrastructure deployments predictably and repeatedly, with rollback on error. The output of an AWS CDK program is an AWS CloudFormation template. Install \u00b6 The CDK cli is packaged as nodejs app. npm install -g aws-cdk A docker file exists under the labs folder to get an environment with nodes, python 3.9, AWS CLI, CDK CLI... # build the docker docker build -t jbcodeforce/aws-python . # Start the env ./startPythonDocker.sh # verify installation aws s3 ls cdk --version # In app folder and where the cdk.json file is, do cdk synth Concepts \u00b6 A CDK app defines one or more Stacks (= CloudFormation stack). A Stack includes Constructs. Each construct defines one or more concrete AWS resources. Constructs (and also stacks and apps) are represented as classes (types) in your programming language of choice. You instantiate constructs within a stack to declare them to AWS, and connect them to each other using well-defined interfaces. Here is an example of constructs defined in a python class constructor for a lambda function and an API Gateway class MyLambdaStack ( Stack ): def __init__ ( self , scope : Construct , construct_id : str , ** kwargs ) -> None : super () . __init__ ( scope , construct_id , ** kwargs ) my_lambda = _lambda . Function ( self , 'HelloHandler' , runtime = _lambda . Runtime . PYTHON_3_7 , code = _lambda . Code . from_asset ( 'lambda' ), handler = 'hello.handler' , ) apigw . LambdaRestApi ( self , 'Endpoint' , handler = my_lambda , ) The AWS CDK Toolkit is a command line tool for interacting with CDK apps. Need to be in the folder of the cdk.json file. cdk --version # Get the CloudFormation template cdk ls The AWS CDK is shipped with an extensive library of constructs called the AWS Construct Library . The construct library is divided into modules, one for each AWS service The first time you deploy an AWS CDK app into an environment (account/region), you\u2019ll need to install a \u201cbootstrap stack\u201d. This stack includes resources that are needed for the toolkit\u2019s operation. It requires dedicated Amazon S3 buckets to store template and assets. See CDK workshops CDK for Python API CDK Python for an EC2 \u00b6 Summary of the actions to jumpstart a CDK sample app in python # Create a python CDK project under a new created folder. The name of the folder defines the name of the app. # It uses the sample-app template. If template is not specified the stack will have only constructor cdk init sample-app --language python # cdk init --language python # create virtual env python3 -m venv .venv # Active the virtual env source .venv/bin/activate # Install dependencies pip install -r requirements.txt Then develop the CDK class to define the configuration like a simple EC2: from aws_cdk import ( Stack , aws_ec2 as ec2 ) amzn_linux = ec2 . MachineImage . latest_amazon_linux ( generation = ec2 . AmazonLinuxGeneration . AMAZON_LINUX_2 , edition = ec2 . AmazonLinuxEdition . STANDARD , virtualization = ec2 . AmazonLinuxVirt . HVM , storage = ec2 . AmazonLinuxStorage . GENERAL_PURPOSE ) with open ( \"./user_data/user_data.sh\" ) as f : user_data = f . read () class Ec2Stack ( Stack ): self . instance = ec2 . Instance ( self , \"myHttpdEC2\" , instance_type = ec2 . InstanceType ( \"t2.micro\" ), instance_name = \"mySimpleHTTPserver\" , machine_image = amzn_linux , user_data = ec2 . UserData . custom ( user_data ), ) See more information on the Instance API . Run the generation of the CD and then deploy to your account / region # Synthesize the Cloud Formation template cdk synth # The first time bootstrap the stack - which will create a CF CDKToolkit cdk bootstrap # or using account and region cdk bootstrap aws://4....../us-west-2 # Deploy the stack cdk deploy # Update the code and do a partial (hotswappable) deployment cdk deploy --hotswap Then go to the CloudFormation console and look at the deployed stack, and resources. See the labs/cdk folder for some examples of CDK stack definitions: Folder Description labs/cdk/ec2-vpc EC2 with VPC and public & private subnets, NAT, IGW, Bastion Host labs/cdk/cdk_workhop Lambda functions in python with an API gateway and TableViewer. lab ECS fargate Flask App VPC with ECS fargate for a Flask APP where container is created during deployment lab EKS VPC with EKS cluster deployment Useful commands \u00b6 cdk ls list all stacks in the app cdk synth emits the synthesized CloudFormation template cdk deploy deploy this stack to your default AWS account/region cdk diff compare deployed stack with current state cdk docs open CDK documentation cdk watch monitors your code and assets for changes and attempts to perform a deployment automatically when a change is detected cdk destroy remove all the resources/stacks. Most resources will get deleted upon stack deletion. CloudWatch logs that are permanently retained CDK Blueprint for EKS \u00b6 Blog introduction EKS blueprint AWS CDK EKS blueprint git repo Secrets \u00b6 Some how to \u00b6 Declare RDS postgres with secrets The following python CDK code declare a Postgresql DB in a private subnet within a VPC created before. The access is public. CDK will create a secret in AWS Secret manager. self . postgres = aws_rds . DatabaseInstance ( self , \"PostgresqlInstance\" , database_name = \"tenantdb\" , engine = aws_rds . DatabaseInstanceEngine . postgres ( version = aws_rds . PostgresEngineVersion . VER_14_5 ), vpc_subnets = aws_ec2 . SubnetSelection ( subnet_type = aws_ec2 . SubnetType . PRIVATE_WITH_EGRESS ), vpc = self . vpc , port = 5432 , removal_policy = RemovalPolicy . DESTROY , deletion_protection = False , max_allocated_storage = 200 , publicly_accessible = True ) Other tools - samples \u00b6 CDK API v2 for Python cdk-dynamo-table-viewer An AWS CDK construct which exposes a public HTTP endpoint which displays an HTML page with the contents of a DynamoDB table in your stack. AWS CDK samples in Python Constructs HUB A Flask app for orders management with DynamoDB as persistence - ECR - CDK Big Data SaaS demo","title":"CDK"},{"location":"coding/cdk/#cloud-development-kit-cdk","text":"The AWS CDK lets you build reliable, scalable, cost-effective applications in the cloud with the considerable expressive power of a programming language (Go, Java, Python, C#, JavaScript, Typescript). Build with high-level constructs that automatically provide sensible, secure defaults for your AWS resources, defining more infrastructure with less code. It uses AWS CloudFormation to perform infrastructure deployments predictably and repeatedly, with rollback on error. The output of an AWS CDK program is an AWS CloudFormation template.","title":"Cloud Development Kit - CDK"},{"location":"coding/cdk/#install","text":"The CDK cli is packaged as nodejs app. npm install -g aws-cdk A docker file exists under the labs folder to get an environment with nodes, python 3.9, AWS CLI, CDK CLI... # build the docker docker build -t jbcodeforce/aws-python . # Start the env ./startPythonDocker.sh # verify installation aws s3 ls cdk --version # In app folder and where the cdk.json file is, do cdk synth","title":"Install"},{"location":"coding/cdk/#concepts","text":"A CDK app defines one or more Stacks (= CloudFormation stack). A Stack includes Constructs. Each construct defines one or more concrete AWS resources. Constructs (and also stacks and apps) are represented as classes (types) in your programming language of choice. You instantiate constructs within a stack to declare them to AWS, and connect them to each other using well-defined interfaces. Here is an example of constructs defined in a python class constructor for a lambda function and an API Gateway class MyLambdaStack ( Stack ): def __init__ ( self , scope : Construct , construct_id : str , ** kwargs ) -> None : super () . __init__ ( scope , construct_id , ** kwargs ) my_lambda = _lambda . Function ( self , 'HelloHandler' , runtime = _lambda . Runtime . PYTHON_3_7 , code = _lambda . Code . from_asset ( 'lambda' ), handler = 'hello.handler' , ) apigw . LambdaRestApi ( self , 'Endpoint' , handler = my_lambda , ) The AWS CDK Toolkit is a command line tool for interacting with CDK apps. Need to be in the folder of the cdk.json file. cdk --version # Get the CloudFormation template cdk ls The AWS CDK is shipped with an extensive library of constructs called the AWS Construct Library . The construct library is divided into modules, one for each AWS service The first time you deploy an AWS CDK app into an environment (account/region), you\u2019ll need to install a \u201cbootstrap stack\u201d. This stack includes resources that are needed for the toolkit\u2019s operation. It requires dedicated Amazon S3 buckets to store template and assets. See CDK workshops CDK for Python API","title":"Concepts"},{"location":"coding/cdk/#cdk-python-for-an-ec2","text":"Summary of the actions to jumpstart a CDK sample app in python # Create a python CDK project under a new created folder. The name of the folder defines the name of the app. # It uses the sample-app template. If template is not specified the stack will have only constructor cdk init sample-app --language python # cdk init --language python # create virtual env python3 -m venv .venv # Active the virtual env source .venv/bin/activate # Install dependencies pip install -r requirements.txt Then develop the CDK class to define the configuration like a simple EC2: from aws_cdk import ( Stack , aws_ec2 as ec2 ) amzn_linux = ec2 . MachineImage . latest_amazon_linux ( generation = ec2 . AmazonLinuxGeneration . AMAZON_LINUX_2 , edition = ec2 . AmazonLinuxEdition . STANDARD , virtualization = ec2 . AmazonLinuxVirt . HVM , storage = ec2 . AmazonLinuxStorage . GENERAL_PURPOSE ) with open ( \"./user_data/user_data.sh\" ) as f : user_data = f . read () class Ec2Stack ( Stack ): self . instance = ec2 . Instance ( self , \"myHttpdEC2\" , instance_type = ec2 . InstanceType ( \"t2.micro\" ), instance_name = \"mySimpleHTTPserver\" , machine_image = amzn_linux , user_data = ec2 . UserData . custom ( user_data ), ) See more information on the Instance API . Run the generation of the CD and then deploy to your account / region # Synthesize the Cloud Formation template cdk synth # The first time bootstrap the stack - which will create a CF CDKToolkit cdk bootstrap # or using account and region cdk bootstrap aws://4....../us-west-2 # Deploy the stack cdk deploy # Update the code and do a partial (hotswappable) deployment cdk deploy --hotswap Then go to the CloudFormation console and look at the deployed stack, and resources. See the labs/cdk folder for some examples of CDK stack definitions: Folder Description labs/cdk/ec2-vpc EC2 with VPC and public & private subnets, NAT, IGW, Bastion Host labs/cdk/cdk_workhop Lambda functions in python with an API gateway and TableViewer. lab ECS fargate Flask App VPC with ECS fargate for a Flask APP where container is created during deployment lab EKS VPC with EKS cluster deployment","title":"CDK Python for an EC2"},{"location":"coding/cdk/#useful-commands","text":"cdk ls list all stacks in the app cdk synth emits the synthesized CloudFormation template cdk deploy deploy this stack to your default AWS account/region cdk diff compare deployed stack with current state cdk docs open CDK documentation cdk watch monitors your code and assets for changes and attempts to perform a deployment automatically when a change is detected cdk destroy remove all the resources/stacks. Most resources will get deleted upon stack deletion. CloudWatch logs that are permanently retained","title":"Useful commands"},{"location":"coding/cdk/#cdk-blueprint-for-eks","text":"Blog introduction EKS blueprint AWS CDK EKS blueprint git repo","title":"CDK Blueprint for EKS"},{"location":"coding/cdk/#secrets","text":"","title":"Secrets"},{"location":"coding/cdk/#some-how-to","text":"Declare RDS postgres with secrets The following python CDK code declare a Postgresql DB in a private subnet within a VPC created before. The access is public. CDK will create a secret in AWS Secret manager. self . postgres = aws_rds . DatabaseInstance ( self , \"PostgresqlInstance\" , database_name = \"tenantdb\" , engine = aws_rds . DatabaseInstanceEngine . postgres ( version = aws_rds . PostgresEngineVersion . VER_14_5 ), vpc_subnets = aws_ec2 . SubnetSelection ( subnet_type = aws_ec2 . SubnetType . PRIVATE_WITH_EGRESS ), vpc = self . vpc , port = 5432 , removal_policy = RemovalPolicy . DESTROY , deletion_protection = False , max_allocated_storage = 200 , publicly_accessible = True )","title":"Some how to"},{"location":"coding/cdk/#other-tools-samples","text":"CDK API v2 for Python cdk-dynamo-table-viewer An AWS CDK construct which exposes a public HTTP endpoint which displays an HTML page with the contents of a DynamoDB table in your stack. AWS CDK samples in Python Constructs HUB A Flask app for orders management with DynamoDB as persistence - ECR - CDK Big Data SaaS demo","title":"Other tools - samples"},{"location":"coding/cloudFormation/","text":"CloudFormation \u00b6 Create and manage a collection of related AWS resources as code. The template defines AWS resources, called a stack, as Yaml or JSON. Can be uploaded from a S3 bucket or from our local computer. The goal is to use Infrastructure as code and repeat infrastructure setup between regions or accounts. One of the greatest benefits of templates and CloudFormation is the ability to create a set of resources that work together to create an application or solution. Stacks are defined in region, but StackSets helps to share stacks between accounts and regions. Stack can be created with other stacks (nested). To create a stack from AWS templates we can use CLI, API, the Console or start from one of the samples. Once stack is created, Change Sets may be applied to update the running resources. It is like a summary of the proposed changes. There is also the Drift detection feature to identify configuration changes between live resources and template. It is possible to use a CloudFormation public registry, with 3nd party resources published in APN. Pay for what the resources it uses. Get started \u00b6 The infrastructure is defined in Stack. See example of EC2 with a webserver and a security group in the folder labs/CF . Resources : WebServer : Type : AWS::EC2::Instance Properties : ImageId : !FindInMap [ AWSRegionArch2AMI , !Ref 'AWS::Region' , !FindInMap [ AWSInstanceType2Arch , !Ref InstanceType , Arch ]] InstanceType : Ref : t2-micro KeyName : Ref : my-key-pair SecurityGroups : - Ref : WebServerSecurityGroup UserData : Fn::Base64 : !Sub | #!/bin/bash -xe yum update -y yum install -y httpd systemctl start httpd systemctl enable httpd EC2-AZ=$(curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone) echo \"<h3>Hello World from $(hostname -f) in AZ= $EC2_AZ </h3>\" > /var/www/html/index.html The KeyName property is a literal for an existing keyname in the region where the stack is being created. We use the Parameters section to declare values that can be passed to the template when we create the stack. Parameters : KeyName : ConstraintDescription : must be the name of an existing EC2 KeyPair. Description : Name of an existing EC2 KeyPair to enable SSH access to the instances Type : AWS::EC2::KeyPair::KeyName See the getting started guide . The Ref function returns the value of the object it refers to. Use Mappings to declare conditional values that are evaluated in a similar manner as a look up table statement Ensure that all dependent resources that the template requires are available The Fn::GetAtt function helps to get attribute of a resource. Mappings enable us to use an input value as a condition that determines another value. Similar to a switch statement, a mapping associates one set of values with another. Below the ImageId property of the resource Ec2Instance uses the Fn::FindInMap function to determine its value by specifying RegionMap as the map to use, AWS::Region as the input value to map from, and AMI as the label to identify the value to map to. Mappings : RegionMap : us-east-1 : AMI : ami-76f0061f us-west-1 : AMI : ami-655a0a20 Resources : Ec2Instance : Type : 'AWS::EC2::Instance' Properties : ImageId : !FindInMap - RegionMap - !Ref 'AWS::Region' - AMI See template details . We can associate the CreationPolicy attribute with a resource to prevent its status from reaching create complete until AWS CloudFormation receives a specified number of success signals or the timeout period is exceeded. Example for S3 bucket and website: Resources : HelloBucket : Type : 'AWS::S3::Bucket' Properties : AccessControl : PublicRead WebsiteConfiguration : IndexDocument : index.html ErrorDocument : error.html Tools \u00b6 Use Cloud formation linter to validate the yaml declaration Json to Yaml online tool Consider CDK as a higher abstraction level to generate Cloud Formation stacks. Deeper dive \u00b6 Introduction from Tutorial Dojo AWS CloudFormation Workshop with Git repo aws-samples/cfn101-workshop cloned in Code/Studies folder. Best practices Sample templates for some AWS services","title":"CloudFormation"},{"location":"coding/cloudFormation/#cloudformation","text":"Create and manage a collection of related AWS resources as code. The template defines AWS resources, called a stack, as Yaml or JSON. Can be uploaded from a S3 bucket or from our local computer. The goal is to use Infrastructure as code and repeat infrastructure setup between regions or accounts. One of the greatest benefits of templates and CloudFormation is the ability to create a set of resources that work together to create an application or solution. Stacks are defined in region, but StackSets helps to share stacks between accounts and regions. Stack can be created with other stacks (nested). To create a stack from AWS templates we can use CLI, API, the Console or start from one of the samples. Once stack is created, Change Sets may be applied to update the running resources. It is like a summary of the proposed changes. There is also the Drift detection feature to identify configuration changes between live resources and template. It is possible to use a CloudFormation public registry, with 3nd party resources published in APN. Pay for what the resources it uses.","title":"CloudFormation"},{"location":"coding/cloudFormation/#get-started","text":"The infrastructure is defined in Stack. See example of EC2 with a webserver and a security group in the folder labs/CF . Resources : WebServer : Type : AWS::EC2::Instance Properties : ImageId : !FindInMap [ AWSRegionArch2AMI , !Ref 'AWS::Region' , !FindInMap [ AWSInstanceType2Arch , !Ref InstanceType , Arch ]] InstanceType : Ref : t2-micro KeyName : Ref : my-key-pair SecurityGroups : - Ref : WebServerSecurityGroup UserData : Fn::Base64 : !Sub | #!/bin/bash -xe yum update -y yum install -y httpd systemctl start httpd systemctl enable httpd EC2-AZ=$(curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone) echo \"<h3>Hello World from $(hostname -f) in AZ= $EC2_AZ </h3>\" > /var/www/html/index.html The KeyName property is a literal for an existing keyname in the region where the stack is being created. We use the Parameters section to declare values that can be passed to the template when we create the stack. Parameters : KeyName : ConstraintDescription : must be the name of an existing EC2 KeyPair. Description : Name of an existing EC2 KeyPair to enable SSH access to the instances Type : AWS::EC2::KeyPair::KeyName See the getting started guide . The Ref function returns the value of the object it refers to. Use Mappings to declare conditional values that are evaluated in a similar manner as a look up table statement Ensure that all dependent resources that the template requires are available The Fn::GetAtt function helps to get attribute of a resource. Mappings enable us to use an input value as a condition that determines another value. Similar to a switch statement, a mapping associates one set of values with another. Below the ImageId property of the resource Ec2Instance uses the Fn::FindInMap function to determine its value by specifying RegionMap as the map to use, AWS::Region as the input value to map from, and AMI as the label to identify the value to map to. Mappings : RegionMap : us-east-1 : AMI : ami-76f0061f us-west-1 : AMI : ami-655a0a20 Resources : Ec2Instance : Type : 'AWS::EC2::Instance' Properties : ImageId : !FindInMap - RegionMap - !Ref 'AWS::Region' - AMI See template details . We can associate the CreationPolicy attribute with a resource to prevent its status from reaching create complete until AWS CloudFormation receives a specified number of success signals or the timeout period is exceeded. Example for S3 bucket and website: Resources : HelloBucket : Type : 'AWS::S3::Bucket' Properties : AccessControl : PublicRead WebsiteConfiguration : IndexDocument : index.html ErrorDocument : error.html","title":"Get started"},{"location":"coding/cloudFormation/#tools","text":"Use Cloud formation linter to validate the yaml declaration Json to Yaml online tool Consider CDK as a higher abstraction level to generate Cloud Formation stacks.","title":"Tools"},{"location":"coding/cloudFormation/#deeper-dive","text":"Introduction from Tutorial Dojo AWS CloudFormation Workshop with Git repo aws-samples/cfn101-workshop cloned in Code/Studies folder. Best practices Sample templates for some AWS services","title":"Deeper dive"},{"location":"data/","text":"Data services \u00b6 Info Updated \u215b/2023 Relational Database Service - RDS \u00b6 Managed service for SQL based database (MariaDB, MySQL, PostgreSQL, SQL server, Oracle, Amazon Aurora), the only things customers do is to define their schema and optimize their queries, AWS manages scaling, HA, backups, software patching, server maintenance... RDS is used for On Line Transaction Processing (OLTP). As an exception, RDS Custom for Oracle (or MSSQL) allows SRE to access and customize the database server host and operating system. A DB instance provides a network address called an endpoint . When deployed in own VPC, be sure to have at least two subnets, each in a separate Availability Zone. See VPC and RDS . Prefer installation in private subnet with no public IP address (avoid it at least). It uses general purpose SSD (gp2, gp3), provisioned IOPS (io1) or magnetic storage (magnetic storages do not have consistent performance). See instance storage . Amazon RDS volumes are built using Amazon EBS volumes. HA supported with Primary, in-synch replication to secondary instance. Read replicas can be used to scale reading operations and they are asynchronously replicated. Resources aren't replicated across AWS Regions unless you do so specifically. Support multi AZs for Reliability Availability with automatic failover to standby, app uses one unique DNS name. Continuous backup and restore to specific point of time restore. Transaction logs are backed-up every 5 minutes. Support user triggered snapshot. Supports Storage Auto Scaling to increase storage dynamically with automatic detections of running out of free storage and scale it. (Free storage < 10%, low storage last at least 5 minutes, 6 hours have passed since last notification) For Oracle and MS SQL it is possible to setup a RDS custom. Where you have access to OS and Database. From a solution architecture point of view: Operations : small downtime when failover happens. For maintenance, scaling with read replicas, updating underlying EC2 instances, or restore EBS, there will be manual interventions. Security : AWS is responsible for OS security, we are responsible for setting up KMS, security groups, IAM policies, authorizing user accesses to the DB, and enforcing SSL. Reliability : Multi AZ feature helps to address reliability, with automatic failover to the standby instance in case of failures. Performance : depends on EC2 instance type, EBS volume type, ability to add Read Replicas. Doesn\u2019t auto-scale, adapt to workload manually. Fit for purpose : for OLAP prefer Redshift. Multi AZ DB Cluster - Read Replicas \u00b6 Read replicas helps to scale the read operations. Can create up to 5 read-replicas within a AZ, across AZ and across regions. Replication is asynch (eventually consistent). Use cases include, reporting, analytics on existing DB, or develop a ML model. The application which needs to access a read-replica DB needs to change the connection parameters. AWS charges for network when for example data goes from one AZ to another. Replicas in the same region in RDS managed services are for free, cross regions has a network cost. Read replica is great for read-heavy workloads and takes the load off the primary database (BI jobs). DR - Multi AZ DB instance \u00b6 RDS supports instance standby with synchronous replication from master to standby. The application talks to one DNS name, and there will be automatic failover, if connection to master RDS fails. Read-replica DB can be setup as a multi AZ for DR, but RTO is not 0. Read replicas can be promoted to become a primary database, but this is a manual procedure. It is possible to move from a Single-AZ to a Multi-AZ with zero downtime, by changing the multi-AZ parameter in the RDS. The internal process is creating a snapshot from master, create a DB via restore operation in target AZ and start synchronous replication from master to standby. With Multi-AZ RDS, a failure of one node generally recovers within a minute by switching to the standby instance with the only availability impact being the time it takes for the DNS pointer to update. For a single-AZ RDS instance, the RTO of an outage where the data isn't impacted, just the EC2 instance, the availability impact would be the length of time it takes for the underlying EC2 instance to recover, which could take several minutes plus the time to replay any transaction logs. Outpost integration \u00b6 AWS Outposts uses the same hardware as in public AWS Regions to bring AWS services, infrastructure, and operation models on-premises. With RDS on Outposts, you can provision managed DB instances close to the business applications that must run on-premises. Security \u00b6 Support at rest Encryption (need to specify it at launch time). Master needs to be encrypted to get encrypted replicas. We can create a snapshot from unencrypted DB and then copy it by enabling the encryption for this snapshot. From there, we can create an Encrypted DB For in-flight encryption use AWS TLS root certificates on the client side. To authenticate, traditional user/pwd can be used but also IAM roles to connect to your DB. Enable IAM Database Authentication. Audit Logs are sent to CloudWatch. Customer's responsibilities: Check the ports / IP / security group inbound rules in DB\u2019s SG In-database user creation and permissions or manage through IAM Creating a database with or without public access Ensure parameter groups or DB is configured to only allow SSL connections Specify a time window to do maintenance, for version to version migration for example. Backup \u00b6 RDS has automatic bakcup executed daily during the maintenance window. But the transaction logs are backed-up every 5 minutes. It is possible to restore the DB states from oldest backup to 5mn ago. 1 to 35 days of retention for automatic backup (0 to disable). If you use manual backup with snapshot, then retention is defined as long as you want. Note The manual snapshot can be used when using the database rarely, and the cost of keeping a snapshot, is far less than letting the DB running. RDS Proxy \u00b6 This is a managed service to keep a pool of database connections between the DB and the clients, so it will improve the performance to access to the database servers. The connections are kept alive. The client connects to the proxy as it will do with the database. It is valuable with Lambda to DB connection. This is a serverless, autoscaling, HA over multi-AZ. With this failover on EDS or Aurora is reduced by up to 66%. Enforce IAM Authentication for DB and securely store credentials in AWS secrets manager. Not expose to public internet. Only visible in the VPC. Code Examples \u00b6 See Playground RDS Autonomous Car Ride uses PostgreSQL in RDS and quarkus app. SaaS demo - Tenant manager Deeper dive \u00b6 Burst vs Baseline with RDS and GP2 : Dimensions that matter are the size, latency, throughput, and IOPS of the volume. With gp2, IOPS is a function of the size of the volume: 3x GiB with a min of 100 IOPS and a max of 10k IOPS Starting with I/O credit at 3000 IOPS, it can be consumed at burst and replinished at the rate of 3 IOPS per GiB per s. With disk above 1TB, the baseline performance > burst perf. Aurora \u00b6 Proprietary SQL database storage engine, works using PostgreSQL and mySQL drivers. Operations : less operation, auto scaling storage: it can grow up by increment of 10GB from 10GB to 128 TB. Security : AWS responsible for OS security, we are responsible for setting up KMS, security groups, IAM policies, authorizing users in DB, enforcing SSL. Reliability : Multi AZ, HA with minimum of 3 AZs, with 2 copies of the data in each AZ. 6 copies overall. Performance : Sub 10ms replica lag, up to 15 replicas (MySQL has only 5 replicas). It costs 20% more than RDS. 5x performance improvement over mySQL on RDS, and 3x for PostgreSQL. The compute resources can scale up to 96 vCPUs and 768 GB of memory. HA and Read Scaling \u00b6 Failover in Aurora is instantaneous. It\u2019s HA (High Availability) native. Use 1 master - 5 readers to create 6 copies of the data over 3 AZs. It supports cross-region replications. It needs 4 copies out of 6 to consider write operation as successful. And 3 copies out of 6 needed for read operations. There is a self-healing capability in case of data corruption with peer-to-peer replication. Storage is done across 100s of volumes. Autoscaling on the read operation from 1 to 15 read-replicas. It is CQRS at DB level, and read can be global. Each connection is handled by a specific DB instance. When you connect to an Aurora cluster, the host name and port that you specify point to an intermediate handler called an endpoint . Aurora uses the endpoint mechanism to abstract these connections. Use writer endpoint for write operation and reader endpoint to access read-replicas. Aurora automatically performing load-balancing among all the Aurora Replicas. It is also possible to design replicas to run on different EC2 server type, and then custom endpoints can be defined to access to those servers. This could be interesting for analytic queries. It also supports one write with multiple readers and parallel query, multiple writes and serverless to automate scaling down to zero (No capacity planning needed and pay per second). Other capabilities \u00b6 With Aurora Global Database, one primary region is used for write and then up to 5 read only regions with replica lag up to 1 s. Promoting another region (for disaster recovery) has an RTO of < 1 minute. Serverless : Amazon Aurora Serverless is an on-demand, auto-scaling configuration for Amazon Aurora. An Aurora Serverless DB cluster is a DB cluster that automatically starts up, shuts down, and scales up or down its compute capacity based on your application\u2019s needs. Aurora Serverless provides a relatively simple, cost-effective option for infrequent, intermittent, sporadic or unpredictable workloads. Multi-master, to protect on write node failure. Every node supports read and write operations. The client has multiple DB connection definitions for failover. Global Aurora : across region replicas, or use Global Database with one primary region for R/W and up to 5 secondary regions (Read-only), with a replica lag < 1s and up to 16 read replicas per secondary region. Promoting a region for DR should lead to a RTO < 1 mn. It takes less than a second to do replicas cross region. Aurora has integration with ML services like SageMaker and Comprehend. Backup \u00b6 Could not be disabled, and automatic is up to 35 days retention. It is possible to clone an existing Aurora DB, which is fast and cost-effective. For example to create a 'Staging' DB from production one. We can share snapshot with other AWS accounts. Code examples \u00b6 Building serverless applications with Amazon Aurora Serverless Fit for purpose \u00b6 Needs Considerations Single digit latency DynamoDB SQL based OLTP RDS SQL based OLAP RedShift ElastiCache \u00b6 Get a managed Redis or Memcached cluster. Applications queries ElastiCache, if data is not available, gets it from RDS and store it in ElastiCache. Key-Value store. It can be used for user session store so user interaction can go to different application instances. Redis is a multi AZ with Auto-Failover, supports read replicas to scale on the read operations and to have high availability. It has data durability using AOF persistence, and has backup and restore features. Redis authentication tokens enable Redis to require a token (password) before allowing clients to execute commands, thereby improving data security. IAM Auth is not supported by ElastiCache. Memcached is a multi-node for partitioning of data (sharding), and no persistence, no backup and restore, not HA via replication. It is based on a multi-threaded architecture. Some patterns for ElastiCache: Lazy Loading : all the read data is cached, data can become stale in cache Write Through : Adds or update data in the cache when written to a DB (no stale data) Session Store : store temporary session data in a cache (using Time To Level features) Sub millisecond performance, in memory read replicas for sharding. Configuration \u00b6 TBD add screen shots with some explanation For the configuration, it can also being deployed on premises via AWS Outposts. It does not support IAM authentication. But we can set a security token at the Redis cluster creation. For Redis you can use Redis auth to force user to enter a password to connect. It supports SSL for in-flight encryption. Coding \u00b6 There is the Redis Sorted Sets to guarantee both uniqueness and element ordering. Each time a new element is added to the cache, it is ranked and added in the correct order. DocumentDB \u00b6 Amazon DocumentDB is a fast, scalable, highly available, and fully managed document database service that supports MongoDB workloads. As a document database, Amazon DocumentDB makes it easy to store, query, and index JSON data. Neptune - GraphDB \u00b6 Fully managed graph database service that makes it easy to build and run applications that work with highly connected datasets. The core of Amazon Neptune is a purpose-built, high-performance graph database engine optimized for storing billions of relationships and querying the graph with milliseconds latency. Neptune powers graph use cases such as recommendation engines, fraud detection, knowledge graphs, drug discovery, and network security. AppSync \u00b6 AWS AppSync provides a robust, scalable GraphQL interface for application developers to combine data from multiple sources, including Amazon DynamoDB, AWS Lambda, and HTTP APIs. AppSync may be combined with DynamoDB to make it easy to build collaborative apps that keep shared data updated in real-time. Amazon QLDB \u00b6 OpenSearch \u00b6 OpenSearch is a managed services to search data with indexing. It is the new name of open source project, ElasticSearch. Can use up to 3 PB of attached storage. Pricing is pay for each hour of use of an EC2 instance and for the cumulative size of any EBS storage volumes attached to your instances. If a domain uses multiple Availability Zones, OpenSearch Service does not bill for traffic between the Availability Zones.","title":"RDS-Aurora-ES"},{"location":"data/#data-services","text":"Info Updated \u215b/2023","title":"Data services"},{"location":"data/#relational-database-service-rds","text":"Managed service for SQL based database (MariaDB, MySQL, PostgreSQL, SQL server, Oracle, Amazon Aurora), the only things customers do is to define their schema and optimize their queries, AWS manages scaling, HA, backups, software patching, server maintenance... RDS is used for On Line Transaction Processing (OLTP). As an exception, RDS Custom for Oracle (or MSSQL) allows SRE to access and customize the database server host and operating system. A DB instance provides a network address called an endpoint . When deployed in own VPC, be sure to have at least two subnets, each in a separate Availability Zone. See VPC and RDS . Prefer installation in private subnet with no public IP address (avoid it at least). It uses general purpose SSD (gp2, gp3), provisioned IOPS (io1) or magnetic storage (magnetic storages do not have consistent performance). See instance storage . Amazon RDS volumes are built using Amazon EBS volumes. HA supported with Primary, in-synch replication to secondary instance. Read replicas can be used to scale reading operations and they are asynchronously replicated. Resources aren't replicated across AWS Regions unless you do so specifically. Support multi AZs for Reliability Availability with automatic failover to standby, app uses one unique DNS name. Continuous backup and restore to specific point of time restore. Transaction logs are backed-up every 5 minutes. Support user triggered snapshot. Supports Storage Auto Scaling to increase storage dynamically with automatic detections of running out of free storage and scale it. (Free storage < 10%, low storage last at least 5 minutes, 6 hours have passed since last notification) For Oracle and MS SQL it is possible to setup a RDS custom. Where you have access to OS and Database. From a solution architecture point of view: Operations : small downtime when failover happens. For maintenance, scaling with read replicas, updating underlying EC2 instances, or restore EBS, there will be manual interventions. Security : AWS is responsible for OS security, we are responsible for setting up KMS, security groups, IAM policies, authorizing user accesses to the DB, and enforcing SSL. Reliability : Multi AZ feature helps to address reliability, with automatic failover to the standby instance in case of failures. Performance : depends on EC2 instance type, EBS volume type, ability to add Read Replicas. Doesn\u2019t auto-scale, adapt to workload manually. Fit for purpose : for OLAP prefer Redshift.","title":"Relational Database Service - RDS"},{"location":"data/#multi-az-db-cluster-read-replicas","text":"Read replicas helps to scale the read operations. Can create up to 5 read-replicas within a AZ, across AZ and across regions. Replication is asynch (eventually consistent). Use cases include, reporting, analytics on existing DB, or develop a ML model. The application which needs to access a read-replica DB needs to change the connection parameters. AWS charges for network when for example data goes from one AZ to another. Replicas in the same region in RDS managed services are for free, cross regions has a network cost. Read replica is great for read-heavy workloads and takes the load off the primary database (BI jobs).","title":"Multi AZ DB Cluster - Read Replicas"},{"location":"data/#dr-multi-az-db-instance","text":"RDS supports instance standby with synchronous replication from master to standby. The application talks to one DNS name, and there will be automatic failover, if connection to master RDS fails. Read-replica DB can be setup as a multi AZ for DR, but RTO is not 0. Read replicas can be promoted to become a primary database, but this is a manual procedure. It is possible to move from a Single-AZ to a Multi-AZ with zero downtime, by changing the multi-AZ parameter in the RDS. The internal process is creating a snapshot from master, create a DB via restore operation in target AZ and start synchronous replication from master to standby. With Multi-AZ RDS, a failure of one node generally recovers within a minute by switching to the standby instance with the only availability impact being the time it takes for the DNS pointer to update. For a single-AZ RDS instance, the RTO of an outage where the data isn't impacted, just the EC2 instance, the availability impact would be the length of time it takes for the underlying EC2 instance to recover, which could take several minutes plus the time to replay any transaction logs.","title":"DR - Multi AZ DB instance"},{"location":"data/#outpost-integration","text":"AWS Outposts uses the same hardware as in public AWS Regions to bring AWS services, infrastructure, and operation models on-premises. With RDS on Outposts, you can provision managed DB instances close to the business applications that must run on-premises.","title":"Outpost integration"},{"location":"data/#security","text":"Support at rest Encryption (need to specify it at launch time). Master needs to be encrypted to get encrypted replicas. We can create a snapshot from unencrypted DB and then copy it by enabling the encryption for this snapshot. From there, we can create an Encrypted DB For in-flight encryption use AWS TLS root certificates on the client side. To authenticate, traditional user/pwd can be used but also IAM roles to connect to your DB. Enable IAM Database Authentication. Audit Logs are sent to CloudWatch. Customer's responsibilities: Check the ports / IP / security group inbound rules in DB\u2019s SG In-database user creation and permissions or manage through IAM Creating a database with or without public access Ensure parameter groups or DB is configured to only allow SSL connections Specify a time window to do maintenance, for version to version migration for example.","title":"Security"},{"location":"data/#backup","text":"RDS has automatic bakcup executed daily during the maintenance window. But the transaction logs are backed-up every 5 minutes. It is possible to restore the DB states from oldest backup to 5mn ago. 1 to 35 days of retention for automatic backup (0 to disable). If you use manual backup with snapshot, then retention is defined as long as you want. Note The manual snapshot can be used when using the database rarely, and the cost of keeping a snapshot, is far less than letting the DB running.","title":"Backup"},{"location":"data/#rds-proxy","text":"This is a managed service to keep a pool of database connections between the DB and the clients, so it will improve the performance to access to the database servers. The connections are kept alive. The client connects to the proxy as it will do with the database. It is valuable with Lambda to DB connection. This is a serverless, autoscaling, HA over multi-AZ. With this failover on EDS or Aurora is reduced by up to 66%. Enforce IAM Authentication for DB and securely store credentials in AWS secrets manager. Not expose to public internet. Only visible in the VPC.","title":"RDS Proxy"},{"location":"data/#code-examples","text":"See Playground RDS Autonomous Car Ride uses PostgreSQL in RDS and quarkus app. SaaS demo - Tenant manager","title":"Code Examples"},{"location":"data/#deeper-dive","text":"Burst vs Baseline with RDS and GP2 : Dimensions that matter are the size, latency, throughput, and IOPS of the volume. With gp2, IOPS is a function of the size of the volume: 3x GiB with a min of 100 IOPS and a max of 10k IOPS Starting with I/O credit at 3000 IOPS, it can be consumed at burst and replinished at the rate of 3 IOPS per GiB per s. With disk above 1TB, the baseline performance > burst perf.","title":"Deeper dive"},{"location":"data/#aurora","text":"Proprietary SQL database storage engine, works using PostgreSQL and mySQL drivers. Operations : less operation, auto scaling storage: it can grow up by increment of 10GB from 10GB to 128 TB. Security : AWS responsible for OS security, we are responsible for setting up KMS, security groups, IAM policies, authorizing users in DB, enforcing SSL. Reliability : Multi AZ, HA with minimum of 3 AZs, with 2 copies of the data in each AZ. 6 copies overall. Performance : Sub 10ms replica lag, up to 15 replicas (MySQL has only 5 replicas). It costs 20% more than RDS. 5x performance improvement over mySQL on RDS, and 3x for PostgreSQL. The compute resources can scale up to 96 vCPUs and 768 GB of memory.","title":"Aurora"},{"location":"data/#ha-and-read-scaling","text":"Failover in Aurora is instantaneous. It\u2019s HA (High Availability) native. Use 1 master - 5 readers to create 6 copies of the data over 3 AZs. It supports cross-region replications. It needs 4 copies out of 6 to consider write operation as successful. And 3 copies out of 6 needed for read operations. There is a self-healing capability in case of data corruption with peer-to-peer replication. Storage is done across 100s of volumes. Autoscaling on the read operation from 1 to 15 read-replicas. It is CQRS at DB level, and read can be global. Each connection is handled by a specific DB instance. When you connect to an Aurora cluster, the host name and port that you specify point to an intermediate handler called an endpoint . Aurora uses the endpoint mechanism to abstract these connections. Use writer endpoint for write operation and reader endpoint to access read-replicas. Aurora automatically performing load-balancing among all the Aurora Replicas. It is also possible to design replicas to run on different EC2 server type, and then custom endpoints can be defined to access to those servers. This could be interesting for analytic queries. It also supports one write with multiple readers and parallel query, multiple writes and serverless to automate scaling down to zero (No capacity planning needed and pay per second).","title":"HA and Read Scaling"},{"location":"data/#other-capabilities","text":"With Aurora Global Database, one primary region is used for write and then up to 5 read only regions with replica lag up to 1 s. Promoting another region (for disaster recovery) has an RTO of < 1 minute. Serverless : Amazon Aurora Serverless is an on-demand, auto-scaling configuration for Amazon Aurora. An Aurora Serverless DB cluster is a DB cluster that automatically starts up, shuts down, and scales up or down its compute capacity based on your application\u2019s needs. Aurora Serverless provides a relatively simple, cost-effective option for infrequent, intermittent, sporadic or unpredictable workloads. Multi-master, to protect on write node failure. Every node supports read and write operations. The client has multiple DB connection definitions for failover. Global Aurora : across region replicas, or use Global Database with one primary region for R/W and up to 5 secondary regions (Read-only), with a replica lag < 1s and up to 16 read replicas per secondary region. Promoting a region for DR should lead to a RTO < 1 mn. It takes less than a second to do replicas cross region. Aurora has integration with ML services like SageMaker and Comprehend.","title":"Other capabilities"},{"location":"data/#backup_1","text":"Could not be disabled, and automatic is up to 35 days retention. It is possible to clone an existing Aurora DB, which is fast and cost-effective. For example to create a 'Staging' DB from production one. We can share snapshot with other AWS accounts.","title":"Backup"},{"location":"data/#code-examples_1","text":"Building serverless applications with Amazon Aurora Serverless","title":"Code examples"},{"location":"data/#fit-for-purpose","text":"Needs Considerations Single digit latency DynamoDB SQL based OLTP RDS SQL based OLAP RedShift","title":"Fit for purpose"},{"location":"data/#elasticache","text":"Get a managed Redis or Memcached cluster. Applications queries ElastiCache, if data is not available, gets it from RDS and store it in ElastiCache. Key-Value store. It can be used for user session store so user interaction can go to different application instances. Redis is a multi AZ with Auto-Failover, supports read replicas to scale on the read operations and to have high availability. It has data durability using AOF persistence, and has backup and restore features. Redis authentication tokens enable Redis to require a token (password) before allowing clients to execute commands, thereby improving data security. IAM Auth is not supported by ElastiCache. Memcached is a multi-node for partitioning of data (sharding), and no persistence, no backup and restore, not HA via replication. It is based on a multi-threaded architecture. Some patterns for ElastiCache: Lazy Loading : all the read data is cached, data can become stale in cache Write Through : Adds or update data in the cache when written to a DB (no stale data) Session Store : store temporary session data in a cache (using Time To Level features) Sub millisecond performance, in memory read replicas for sharding.","title":"ElastiCache"},{"location":"data/#configuration","text":"TBD add screen shots with some explanation For the configuration, it can also being deployed on premises via AWS Outposts. It does not support IAM authentication. But we can set a security token at the Redis cluster creation. For Redis you can use Redis auth to force user to enter a password to connect. It supports SSL for in-flight encryption.","title":"Configuration"},{"location":"data/#coding","text":"There is the Redis Sorted Sets to guarantee both uniqueness and element ordering. Each time a new element is added to the cache, it is ranked and added in the correct order.","title":"Coding"},{"location":"data/#documentdb","text":"Amazon DocumentDB is a fast, scalable, highly available, and fully managed document database service that supports MongoDB workloads. As a document database, Amazon DocumentDB makes it easy to store, query, and index JSON data.","title":"DocumentDB"},{"location":"data/#neptune-graphdb","text":"Fully managed graph database service that makes it easy to build and run applications that work with highly connected datasets. The core of Amazon Neptune is a purpose-built, high-performance graph database engine optimized for storing billions of relationships and querying the graph with milliseconds latency. Neptune powers graph use cases such as recommendation engines, fraud detection, knowledge graphs, drug discovery, and network security.","title":"Neptune - GraphDB"},{"location":"data/#appsync","text":"AWS AppSync provides a robust, scalable GraphQL interface for application developers to combine data from multiple sources, including Amazon DynamoDB, AWS Lambda, and HTTP APIs. AppSync may be combined with DynamoDB to make it easy to build collaborative apps that keep shared data updated in real-time.","title":"AppSync"},{"location":"data/#amazon-qldb","text":"","title":"Amazon QLDB"},{"location":"data/#opensearch","text":"OpenSearch is a managed services to search data with indexing. It is the new name of open source project, ElasticSearch. Can use up to 3 PB of attached storage. Pricing is pay for each hour of use of an EC2 instance and for the cumulative size of any EBS storage volumes attached to your instances. If a domain uses multiple Availability Zones, OpenSearch Service does not bill for traffic between the Availability Zones.","title":"OpenSearch"},{"location":"data/data-lake/","text":"Data Lake with AWS \u00b6 Here is a mapping with AWS services to support your data strategy: Amazon Redshift S3 Big Data \u00b6 The 3 V's of big data are Volume: we are at the terabytes and petabytes level. Variety: includes data from a wide range of sources and formats Velocity: data needs to be collected, stored, processed and analyzed withn a short period of time. EMR \u00b6 Created in 2009, it is a managed service to run Spark, Haddop, Hive, Presto, HBase... Per-second prcing and save 50%-80% with Amazon EC2 Spot and reserved instances. OpenSearch \u00b6 Fully managed service Glue \u00b6 Serverless data integration and data pipeline to do ETL jobs. You can discover and connect to over 70 diverse data sources, manage your data in a centralized data catalog, and visually create, run, and monitor ETL pipelines to load data into your data lakes. Pay only for resources used while running. Product documentation AWS Glue samples repository Lake Formation \u00b6 AWS Lake Formation is a service that makes it easy to set up a secure data lake in days. A data lake is a centralized, curated, and secured repository that stores all your data, both in its original form and prepared for analysis. Amazon S3 forms the storage layer for Lake Formation. AWS Lake Formation is integrated with AWS Glue which you can use to create a data catalog that describes available datasets and their appropriate business applications. Lake Formation lets you define policies and control data access with simple \u201cgrant and revoke permissions to data\u201d sets at granular levels Deeper Dive \u00b6 How it works Building secured data lakes on AWS","title":"Data Lake"},{"location":"data/data-lake/#data-lake-with-aws","text":"Here is a mapping with AWS services to support your data strategy: Amazon Redshift S3","title":"Data Lake with AWS"},{"location":"data/data-lake/#big-data","text":"The 3 V's of big data are Volume: we are at the terabytes and petabytes level. Variety: includes data from a wide range of sources and formats Velocity: data needs to be collected, stored, processed and analyzed withn a short period of time.","title":"Big Data"},{"location":"data/data-lake/#emr","text":"Created in 2009, it is a managed service to run Spark, Haddop, Hive, Presto, HBase... Per-second prcing and save 50%-80% with Amazon EC2 Spot and reserved instances.","title":"EMR"},{"location":"data/data-lake/#opensearch","text":"Fully managed service","title":"OpenSearch"},{"location":"data/data-lake/#glue","text":"Serverless data integration and data pipeline to do ETL jobs. You can discover and connect to over 70 diverse data sources, manage your data in a centralized data catalog, and visually create, run, and monitor ETL pipelines to load data into your data lakes. Pay only for resources used while running. Product documentation AWS Glue samples repository","title":"Glue"},{"location":"data/data-lake/#lake-formation","text":"AWS Lake Formation is a service that makes it easy to set up a secure data lake in days. A data lake is a centralized, curated, and secured repository that stores all your data, both in its original form and prepared for analysis. Amazon S3 forms the storage layer for Lake Formation. AWS Lake Formation is integrated with AWS Glue which you can use to create a data catalog that describes available datasets and their appropriate business applications. Lake Formation lets you define policies and control data access with simple \u201cgrant and revoke permissions to data\u201d sets at granular levels","title":"Lake Formation"},{"location":"data/data-lake/#deeper-dive","text":"How it works Building secured data lakes on AWS","title":"Deeper Dive"},{"location":"data/dynamodb/","text":"DynamoDB \u00b6 AWS proprietary NoSQL database, Serverless, provisioned capacity, auto scaling, on demand capacity. Fully managed, Highly Available with replication across multiple AZs in one AWS Region by default, Read and Writes are decoupled, and DAX can be used for delivering a read cache. Single digit ms latency, even with increased number of requests. Can support millions of requests per second, trillions of row, 100s of TB storage. Data is stored on solid-state disks (SSDs) and may be encrypted at rest. The maximum size for an item in a table is 400k. It is integrated with IAM for authentication and authorization. A table is a collection of items, and each item is a collection of attributes. DynamoDB uses primary keys to uniquely identify each item in a table and secondary indexes to provide more querying flexibility. Useful when the solution does not want to design a data schema upfront and which may change overtime. The read operations can be eventually consistent or strongly consistent. There is two capacities modes: provisioned : where you specify and pay for read capacity units and write capacity units. Need to plan beforehand. Less expensive on-demand : read and writes automatically scale up/down with your workloads. Better for unpredictable workloads. More expensive. Coding \u00b6 With the aws CLI: aws dynamodb create-table --table-name Orders \\ --attribute-definitions AttributeName = orderID,AttributeType = S \\ --key-schema AttributeName = orderID,KeyType = HASH \\ --provisioned-throughput ReadCapacityUnits = 1 ,WriteCapacityUnits = 1 CDK \u00b6 Before you can use the AWS SDKs with DynamoDB, you must get an AWS access key ID and secret access key. Create a dynamodb instance with CDK: class CdkStack ( Stack ): def __init__ ( self , scope : Construct , construct_id : str , ** kwargs ) -> None : super () . __init__ ( scope , construct_id , ** kwargs ) # create dynamo table order_table = aws_dynamodb . Table ( self , \"orders\" , partition_key = aws_dynamodb . Attribute ( name = \"orderID\" , type = aws_dynamodb . AttributeType . STRING , ), table_class = aws_dynamodb . TableClass . STANDARD_INFREQUENT_ACCESS , billing_mode = aws_dynamodb . BillingMode . PAY_PER_REQUEST , # make it global #replicationRegions= ['us-west-1', 'us-east-1', 'us-west-2'], ) Or using the SDK like boto3 import boto3 Other coding pattern \u00b6 Using quarkus, docker image for dynamodb, : Add dynamodb extension, and do data transformation between item and the business entity managed by the resource. DynamoDB Accelerator - DAX \u00b6 To address read congestion of the read operation, DAX is a managed service of a distributed cache cluster in front of DynamoDB. It brings microsecond latency. The API is the same as DynamoDB. It caches the most frequently used data, thus offloading the heavy reads on hot keys off your DynamoDB table, hence preventing the \"ProvisionedThroughputExceededException\" exception. DynamoDB Stream processing \u00b6 It is possible to get ordered stream of item-level modification such as Create, Update, Delete in a DynamoDB table. It is relevant for: React on changes in real-time. Real-time usage analytics Insert into derivative tables Implement cross-region replication Invoke Lambda on changes DynamoDB global table \u00b6 The goal is to make a table accessible with low latency in different regions. It uses two-way replication, active-active so application can read and write to the table in any region. We need to enable streaming to get the replication activated. DynamoDB Time to live \u00b6 This feature helps to remove records after a specified timestamp. It is used to clean old records, like older than 2 years, or for session data, to be removed after 2 hours. Backup \u00b6 DynamoDB supports continuous backup using point-in-time recovery (PITR). It can go up to the last 35 days. Recovery creates new table.","title":"DynamoDB"},{"location":"data/dynamodb/#dynamodb","text":"AWS proprietary NoSQL database, Serverless, provisioned capacity, auto scaling, on demand capacity. Fully managed, Highly Available with replication across multiple AZs in one AWS Region by default, Read and Writes are decoupled, and DAX can be used for delivering a read cache. Single digit ms latency, even with increased number of requests. Can support millions of requests per second, trillions of row, 100s of TB storage. Data is stored on solid-state disks (SSDs) and may be encrypted at rest. The maximum size for an item in a table is 400k. It is integrated with IAM for authentication and authorization. A table is a collection of items, and each item is a collection of attributes. DynamoDB uses primary keys to uniquely identify each item in a table and secondary indexes to provide more querying flexibility. Useful when the solution does not want to design a data schema upfront and which may change overtime. The read operations can be eventually consistent or strongly consistent. There is two capacities modes: provisioned : where you specify and pay for read capacity units and write capacity units. Need to plan beforehand. Less expensive on-demand : read and writes automatically scale up/down with your workloads. Better for unpredictable workloads. More expensive.","title":"DynamoDB"},{"location":"data/dynamodb/#coding","text":"With the aws CLI: aws dynamodb create-table --table-name Orders \\ --attribute-definitions AttributeName = orderID,AttributeType = S \\ --key-schema AttributeName = orderID,KeyType = HASH \\ --provisioned-throughput ReadCapacityUnits = 1 ,WriteCapacityUnits = 1","title":"Coding"},{"location":"data/dynamodb/#cdk","text":"Before you can use the AWS SDKs with DynamoDB, you must get an AWS access key ID and secret access key. Create a dynamodb instance with CDK: class CdkStack ( Stack ): def __init__ ( self , scope : Construct , construct_id : str , ** kwargs ) -> None : super () . __init__ ( scope , construct_id , ** kwargs ) # create dynamo table order_table = aws_dynamodb . Table ( self , \"orders\" , partition_key = aws_dynamodb . Attribute ( name = \"orderID\" , type = aws_dynamodb . AttributeType . STRING , ), table_class = aws_dynamodb . TableClass . STANDARD_INFREQUENT_ACCESS , billing_mode = aws_dynamodb . BillingMode . PAY_PER_REQUEST , # make it global #replicationRegions= ['us-west-1', 'us-east-1', 'us-west-2'], ) Or using the SDK like boto3 import boto3","title":"CDK"},{"location":"data/dynamodb/#other-coding-pattern","text":"Using quarkus, docker image for dynamodb, : Add dynamodb extension, and do data transformation between item and the business entity managed by the resource.","title":"Other coding pattern"},{"location":"data/dynamodb/#dynamodb-accelerator-dax","text":"To address read congestion of the read operation, DAX is a managed service of a distributed cache cluster in front of DynamoDB. It brings microsecond latency. The API is the same as DynamoDB. It caches the most frequently used data, thus offloading the heavy reads on hot keys off your DynamoDB table, hence preventing the \"ProvisionedThroughputExceededException\" exception.","title":"DynamoDB Accelerator - DAX"},{"location":"data/dynamodb/#dynamodb-stream-processing","text":"It is possible to get ordered stream of item-level modification such as Create, Update, Delete in a DynamoDB table. It is relevant for: React on changes in real-time. Real-time usage analytics Insert into derivative tables Implement cross-region replication Invoke Lambda on changes","title":"DynamoDB Stream processing"},{"location":"data/dynamodb/#dynamodb-global-table","text":"The goal is to make a table accessible with low latency in different regions. It uses two-way replication, active-active so application can read and write to the table in any region. We need to enable streaming to get the replication activated.","title":"DynamoDB global table"},{"location":"data/dynamodb/#dynamodb-time-to-live","text":"This feature helps to remove records after a specified timestamp. It is used to clean old records, like older than 2 years, or for session data, to be removed after 2 hours.","title":"DynamoDB Time to live"},{"location":"data/dynamodb/#backup","text":"DynamoDB supports continuous backup using point-in-time recovery (PITR). It can go up to the last 35 days. Recovery creates new table.","title":"Backup"},{"location":"data/redshift/","text":"Amazon RedShift \u00b6 Redshift is the cloud data warehouse managed services, with no data movement or data transformation. Its goal is to manage up to 16 PB of data (it is like 500 billion pages of standard printed text, or 11,000 4k movies). Use cases \u00b6 Unified data warehouse and data lake. Get insights by running real-time and predictive analytics on all of your data, across operational databases, data lake, data warehouse, and thousands of third-party datasets. Support all analytics workload. Scalable, managed services, serverless, cost effective. Architecture \u00b6 Amazon Redshift uses SQL to analyze structured and semi-structured data across data warehouses, operational databases, and data lakes, using AWS-designed hardware and machine learning to deliver the best price performance at any scale. It is based on Postgresql but is not used for OLTP. It is used for analytical processing and data warehousing, scale to Peta Bytes. It is Columnar storage of data. It uses massively parallel query execution. Data can be loaded from S3, DynamoDB, DMS and other DBs. It can scale from 1 to 128 nodes, and each node has 160GB per node. Redshift spectrum performs queries directly on top of S3. The cluster architecture is based on a leader node to support query planning and aggregate results, manages metadata, and compute nodes to perform the queries, distribute the data and send results back. Provision node size in advance, and we can use Reserved Instances for cost saving. Node types include Dense Compute (dc2.large ...) with a number of slices and storage in SSD, and Dense Storage (ds2.xlarge) based on HDD up to 2 PB, or RA3 with more vCPU and total capacity for very large datahouse. Slice is compute and storage and receives query script, executes those scripts and return results. During the cluster creation an tool can help size the cluster according to the data needs: It will then propose on-demand, reserved (1 year), or 3 years pricing model. RedShift is not multi-AZ, but we can use snapshots to do point-in-time backup and store in S3. Snapshots are incremental. And backup can be restored in a new cluster. For DR purpose snapshot can be copied to another region. RedShift Spectrum helps to query data already in S3 without loading them. The query is submitted to thousands of redshift spectrum nodes. The configuration includes network, VPC, subnet, security group, enhanced VPC routing to keep traffic through a VPC. Database name and port and if we want encryption using KMS or HSM. Specify maintenance window, monitoring and backup. For monitoring we can use CloudWatch alarm to monitor the disk usage of our cluster and send notification to a SNS topic. Once create the managed service offers a JDBC endpoint, and ODBC endpoint and HTTP API. Redshift Spectrum pushes many compute-intensive tasks, such as predicate filtering and aggregation, down to the Redshift Spectrum layer. Thus, Redshift Spectrum queries use much less of your cluster's processing capacity than other queries. Features \u00b6 Serverless or create and manage your own cluster Set up SQL Workbench/J to access your data in the Amazon Redshift cluster using the Amazon Redshift JDBC driver. Analyze the data with standard SQL from SQL Workbench/J. Deeper Dive \u00b6 Product documentation Deploy a data warehouse","title":"RedShift"},{"location":"data/redshift/#amazon-redshift","text":"Redshift is the cloud data warehouse managed services, with no data movement or data transformation. Its goal is to manage up to 16 PB of data (it is like 500 billion pages of standard printed text, or 11,000 4k movies).","title":"Amazon RedShift"},{"location":"data/redshift/#use-cases","text":"Unified data warehouse and data lake. Get insights by running real-time and predictive analytics on all of your data, across operational databases, data lake, data warehouse, and thousands of third-party datasets. Support all analytics workload. Scalable, managed services, serverless, cost effective.","title":"Use cases"},{"location":"data/redshift/#architecture","text":"Amazon Redshift uses SQL to analyze structured and semi-structured data across data warehouses, operational databases, and data lakes, using AWS-designed hardware and machine learning to deliver the best price performance at any scale. It is based on Postgresql but is not used for OLTP. It is used for analytical processing and data warehousing, scale to Peta Bytes. It is Columnar storage of data. It uses massively parallel query execution. Data can be loaded from S3, DynamoDB, DMS and other DBs. It can scale from 1 to 128 nodes, and each node has 160GB per node. Redshift spectrum performs queries directly on top of S3. The cluster architecture is based on a leader node to support query planning and aggregate results, manages metadata, and compute nodes to perform the queries, distribute the data and send results back. Provision node size in advance, and we can use Reserved Instances for cost saving. Node types include Dense Compute (dc2.large ...) with a number of slices and storage in SSD, and Dense Storage (ds2.xlarge) based on HDD up to 2 PB, or RA3 with more vCPU and total capacity for very large datahouse. Slice is compute and storage and receives query script, executes those scripts and return results. During the cluster creation an tool can help size the cluster according to the data needs: It will then propose on-demand, reserved (1 year), or 3 years pricing model. RedShift is not multi-AZ, but we can use snapshots to do point-in-time backup and store in S3. Snapshots are incremental. And backup can be restored in a new cluster. For DR purpose snapshot can be copied to another region. RedShift Spectrum helps to query data already in S3 without loading them. The query is submitted to thousands of redshift spectrum nodes. The configuration includes network, VPC, subnet, security group, enhanced VPC routing to keep traffic through a VPC. Database name and port and if we want encryption using KMS or HSM. Specify maintenance window, monitoring and backup. For monitoring we can use CloudWatch alarm to monitor the disk usage of our cluster and send notification to a SNS topic. Once create the managed service offers a JDBC endpoint, and ODBC endpoint and HTTP API. Redshift Spectrum pushes many compute-intensive tasks, such as predicate filtering and aggregation, down to the Redshift Spectrum layer. Thus, Redshift Spectrum queries use much less of your cluster's processing capacity than other queries.","title":"Architecture"},{"location":"data/redshift/#features","text":"Serverless or create and manage your own cluster Set up SQL Workbench/J to access your data in the Amazon Redshift cluster using the Amazon Redshift JDBC driver. Analyze the data with standard SQL from SQL Workbench/J.","title":"Features"},{"location":"data/redshift/#deeper-dive","text":"Product documentation Deploy a data warehouse","title":"Deeper Dive"},{"location":"infra/","text":"Major infrastructure services \u00b6 Info Updated 12/26/2022 Amazon Elastic Compute Cloud - EC2 components \u00b6 EC2 is a renting machine. Amazon EC2 instances are a combination of virtual processors (vCPUs), memory, network, graphics processing units (GPUs), and, in some cases, instance storage. Only size for what we plan to use. Storing data on virtual drives: EBS . Distribute load across machines using ELB . Auto scale the service via group: ASG . EC2 can have MacOS, Linux or Windows OS. Amazon Machine Image (AMI) is the OS image with preinstalled softwares. Amazon Linux 2 for linux base image. See AMI Catalog within the selected region to get what AMI could be used. Figure 1: EC2 instance When creating an instance, we can select the OS, CPU, RAM, the VPC, the AZ subnet, the storage (EBS) for root folder, the network card, and the firewall rules defined as Security Group . The security group helps to isolate the instance, for example, authorizing traffic for ssh on port 22 (TCP) and HTTP on port 80. Get the public ssh key, and when the instance is started, use a command like: ssh -i EC2key.pem ec2-user@ec2-52-8-75-8.us-west-1.compute.amazonaws.com to connect to the EC2 via ssh. On the client side, the downloaded .pem file needs to be restricted with chmod 0400 . We can also use EC2 Instance Connect to open a terminal in the web browser. Still needs to get SSH port accessible in the security group. See this EC2 playground for demonstrating the deployment of a HTTP server. EC2 life cycle \u00b6 When we launch an instance, it enters in the pending state. Billing is started when in running state. During rebooting, instance remains on the same host computer, and maintains its public and private IP addresses, in addition to any data on its instance store. With stopped instance, the instance is shut down and cannot be used. The instance can be restarted at any time. When we terminate an instance, the instance stores are erased, and we lose both the public IP and private IP addresses of the machine. Storage for any Amazon EBS volumes is still charged. Note that Reserved Instances that applied to terminated instances are still billed until the end of their term according to their payment option. We will be billed when our On-Demand instance is preparing to hibernate with a stopping state. When we launch a new EC2 instance, the EC2 service attempts to place the instance in such a way that all of our instances are spread out across underlying hardware to minimize correlated failures. We may use placement groups to influence the placement of a group of interdependent instances to meet the needs of our workload. EC2 types \u00b6 EC2 has a section to add User data , which could be used to define a bash script to install dependent software and to start some services at boot time (like httpd). EC2 instance types like t2.micro or c5.2xlarge define CPU, memory... (see ec2instances.info or the reference AWS ec2/instance-types ). The first letter defines the class as: R: (memory) applications that needs a lot of RAM \u2013 in-memory caches. C: (Compute Optimized) applications that needs good CPU \u2013 compute / databases, ETL media transcoding, High Perf web servers, scientific modeling. M: applications that are balanced (think \u201cmedium\u201d) \u2013 general / web app. I: (storage) applications that need good local I/O (instance storage) \u2013 databases, NoSQL, cache like Redis, data warehousing, distributed file systems. G: applications that need a GPU. T2/T3 for burstable instance: When the machine needs to process something unexpected (a spike in load for example), it can burst. Use burst credits to control CPU usage. Graviton processors are designed by AWS for cloud workloads to optimize cost and energy consumption. (t4g. , M6g. , C7*) EC2 Nitro System \u00b6 Next generation of EC2. It uses new virtualization infrastructure and hypervisor. Supports IPv6, better I/O on EBS and better security. Name type starts with C5, D5,... vCPU represents thread running on core CPU. We can optimize vCPU allocation on the EC2 instance, once created, by updating the launch configuration. There is a vCPU-based On-Demand Instance limit per region which may impact the creation of new instance. Just submit the limit increase form to AWS and retry the failed requests once approved. Launch types \u00b6 On demand : short workload, predictable pricing, pay per second after first minute. No long term commitment. Reserved for one or 3 years term, used for long workloads like database. Get discounted rate from on-demand. Up to 72% discount. Upfront cost and pay monthly. We can buy and sell it in the marketplace. When no more needed, Terminate the Reserved instances as soon as possible to avoid getting billed at the on-demand price when it expires. Convertible reserved instance for changing resource capacity over time. Scheduled reserved instance for job based workload. Dedicated hosts to book entire physical server and control instance placement. # years. BYOL . (Used to port Microsoft license) Can be on-demand or reserved. Most expensive solution. Use it when we deploy a database technology on an EC2 instance and the vendor license bills based on the physical cores. Baremetal is part of this option. Capacity reservations : reserve capacity in a specific AZ for any duration. Spot instance for very short - 90% discount on on-demand - used for work resilient to failure like batch job, data analysis, image processing, stateless, containerized... Define a max spot price and get the instance while the current spot price < max price wanting to pay. The hourly spot price varies based on offer and capacity. If the current spot price > max, then instance will be stopped in a 2 minutes. With spot block we can define a time frame without interruptions from 1 to 6 hours. The expected state is defined in a 'spot request' which can be cancelled. One time or persistent request types are supported. Cancel a spot request does not terminate instance, but need to be the first thing to do to avoid cost. Spot fleets allow to automatically request spot instance and on-demand instance with the lowest price to meet the target capacity within the price constraints. Use EC2 launch templates to automate instance launches, to simplify permission policies, and to enforce best practices across the organization. Look very similar to docker image. Metadata \u00b6 When in a EC2 instance shell, we can get access to EC2 metadata by going to the URL: http://169.254.169.254/latest/meta-data/ . We can also review scripts used to bootstrap the instances at runtime using http://169.254.169.254/latest/user-data/ AMI \u00b6 Bring our own image. Shareable on Amazon Marketplace. Can be saved on S3 storage. By default, our AMIs are privates, and locked for our account / region. AMIs can be copied and shared See AWS doc - copying an AMI . EC2 Hibernate \u00b6 The in memory state is preserved, persisted to a file in the root EBS volume. It helps to make the instance startup time quicker. The root EBS volume needs to be encrypted. Memory is constrained by 150GB RAM. No more than 60 days. No instance store volume possible. Basic Fault Tolerance \u00b6 The following diagram illustrates some fault tolerance principles offered by the basic AWS services: Figure 2 AMI defines image for the EC2 with static or dynamic configuration. From one AMI, we can scale by adding new EC2 based on same image. Instance failure can be replaced by starting a new instance from the same AMI. Auto Scaling Group defines a set of EC2 instances, and can start new EC2 instance automatically. Auto scaling adjusts the capacity of EC2 and EC2 instance within the group. To minimize down time, we can have one EC2 instance in Standby mode, and use elastic IP addresses to be reassigned in case of the primary EC2 failure. Data is saved on EBS and replicated to other EBS inside the same availabiltiy zone. Snapshot backup can be done to replicate data between AZs and/or regions, and persisted for long retention in S3. Need to flush data from memory to disk before any snapshot. Applications can be deployed between AZs. Elastic Load Balancer balances traffic among servers in multiple AZs and DNS will route traffic to the good server. Elastic IP addresses are static and defined at the AWS account level. New EC2 instance can be reallocated to Elastic IP @, but they are mapped by internet gateway to the private address of the EC2. The service may be down until new EC2 instace is restarted. ELB ensures higher fault tolerance for EC2s, containers, lambdas, IP addresses and physical servers. Application LB load balances at the HTTP, HTTPS level, and within a VPC based on the content of the request. NLB is for TCP, UDP, TLS routing and load balancing. Placement groups \u00b6 Define strategy to place EC2 instances: Cluster : groups instances into a low-latency group in a single Availability Zone. Highest performance while talking to each other as when performing big data analysis. Spread : groups across underlying hardware (max 7 instances per group per AZ). Reduced risk in case of simultaneous failure. EC2 Instances are on different physical hardware. For application that needs to maximize high availability. Critical Applications where each instance must be isolated from failure from each other. Partition : spreads instances across many different partitions (which rely on different sets of racks) within an AZ. Partition is a set of racks. Up to 100s of EC2 instances. The instances in a partition do not share racks with the instances in the other partitions. A partition failure can affect many EC2s but won\u2019t affect other partitions. EC2 instances get access to the partition information as metadata. HDFS, HBase, Cassandra, Kafka Access from network and policies menu, define the group with expected strategy, and then use it when creating the EC2 instance by adding the instance to a placement group. EC2 network bandwidth \u00b6 Each EC2 instance has a maximum bandwidth for aggregate inbound and outbound traffic, based on instance type and size. The network bandwidth available to an EC2 instance depends on several factors. Bandwidth depends on the number of vCPUs configured, and for less than 32 vCPUs the limit (single flow of 5-tuple) (source IP address/port number, destination IP address/port number and the protocol) is around 5 Gbps when instances are not in the same cluster placement group. Also bandwidth depends on the flow type: within on region or cross-regions. To meet additional demand, they can use a network I/O credit mechanism to burst beyond their baseline bandwidth. However, instances might not achieve their available network bandwidth, if they exceed network allowances at the instance level, such as packet per second or number of tracked connections. One EC2 connected to EC2 instance in same region. Recalls that Enterprise-grade WAN and DIA links more commonly have symmetrical bandwidth, the data capacity is the same in both directions. EC2 Instance Store \u00b6 When disk performance is a strong requirement, use EC2 Instance Store. Millions IOPS read or even write. It loses data when stopped. Good use for buffer, cache, scratch data, or distributed systems with their own replication like Kafka. Backup and replication are the user's responsability Quota \u00b6 Service Quotas is an AWS service that helps manage our quotas for over 100 AWS services from one location. The AWS account has default quotas, formerly referred as limits, defined for each AWS service. Unless otherwise noted, each quota is Region specific. We can request increases for some quotas, and other quotas cannot be increased. Each EC2 instance can have a variance of the number of vCPUs, depending on its type and configuration, so it's always wise to calculate the vCPU needs to make sure we are not going to hit quotas too early. Along with looking up the quota values, we can also request a quota increase from the Service Quotas console. Security group \u00b6 Define inbound and outbound security rules. It is like a virtual firewall inside an EC2 instance. SGs regulate access to ports, authorized IP ranges IPv4 and IPv6, control inbound and outbound network. By default all inbound traffic is denied and outbound authorized. They contain allow rules only. Can be attached to multiple EC2 instances and to load balancers. Locked down to a region / VPC combination. Live outside of the EC2. Define one separate security group for SSH access where we can authorize only one IP@. Connect refused is an application error or the app is not launched - Spinning wheel in the web browser is an access rules error. Instances with the same security group can access each other. Security group can reference other security groups, on IP address using CIDR in the form 192.45.23.12/32 but not any DNS server. Figure 3: security group with inbound rules Important Ports: 22 for SSH (Secure Shell) and SFTP. 21 for FTP. 80 for HTTP. 443 for https. 3389: Remote desktop protocol. Auto Scaling Group (ASG) \u00b6 The goal of an ASG is to scale out (add EC2 instances) to match an increased load, or scale in (remove EC2 instances) to match a decreased load. It helps to provision and balance capacity across Availability Zones to optimize availability. It can also ensure we have a minimum and a maximum number of machines running. It detects when an instance is unhealthy. Automatically Register new instances to a load balancer. ASG has the following attributes: AMI + Instance Type with EC2 User Data (Can use template to define instances). EBS Volumes. Security Groups. SSH Key Pair. Min Size / Max Size / Initial Capacity to control number of instances . Network + Subnets Information to specify where to run the EC2 instances. Load Balancer Information, with target groups to be used as a grouping mechanism of the newly created instances. Scaling Policies help to define rules to manage instance life cycle, based for example on CPU usage or network bandwidth used. It is not possible to modify a launch configuration once it is created. If we need to change the EC2 instance type for example, we need to create a new launch configuration to use the correct instance type. Modify the Auto Scaling group to use this new launch configuration. Delete the old launch configuration as it is no longer needed. The Default Termination Policy is designed to help ensure that our instances span Availability Zones evenly for high availability. Determine which Availability Zones have the most instances, and at least one instance that is not protected from scale in. Determine which instances to terminate so as to align the remaining instances to the allocation strategy for the on-demand or spot instance that is terminating. This only applies to an Auto Scaling Group that specifies allocation strategies. For example, after our instances launch, we change the priority order of our preferred instance types. When a scale-in event occurs, Amazon EC2 Auto Scaling tries to gradually shift the on-demand instances away from instance types that are lower priority. Determine whether any of the instances use the oldest launch template or configuration: (For Auto Scaling Groups that use a launch template) Determine whether any of the instances use the oldest launch template unless there are instances that use a launch configuration. Amazon EC2 Auto Scaling terminates instances that use a launch configuration before instances that use a launch template. For Auto Scaling Groups that use a launch configuration: Determine whether any of the instances use the oldest launch configuration. After applying all of the above criteria, if there are multiple unprotected instances to terminate, determine which instances are closest to the next billing hour. If there are multiple unprotected instances closest to the next billing hour, terminate one of these instances at random. When creating scaling policies, CloudWatch alarms are created. Ex: \"Create an alarm if: CPUUtilization < 36 for 15 data points within 15 minutes\". Target tracking scaling: we want average CPU to be under 40%. Scheduled action: increase capacity after 5 PM. Predictive scaling by looking at historical behavior to build forecast rules. ASG tries to balance the number of instances across AZs by default, and then deletes based on the age of the launch configuration. The capacity of our ASG cannot go over the maximum capacity we have allocated during scale out events. Cool down period is set to 5 mn and will not change the number of instance until this period is reached. When an ALB validates an health check issue, ASG terminates the EC2 instance. CloudFront \u00b6 Content Delivery Network service with DDoS protection. It caches data to the edge to improve web browsing and application performance. 410+ Edge locations. This is a global service. The origins of those files are S3 bucket objects, or Custom Origin resource accessible via HTTP (ALB, EC2...). CloudFront keeps cache for the data read. For the edge to access the S3 bucket, it uses an origin access identity (OAI), managed as IAM role. For EC2 instance, the security group needs to accept traffic from edge location IP addresses. It is possible to control with geographic restriction using whitelist or blacklist. It also supports the concept of signed URL. When we want to distribute content to different user groups over the world, attach a policy with: URL expiration. IP range to access the data from. Trusted signers (which AWS accounts can create signed URLs). How long should the URL be valid for? Shared content (movie, music): make it short (a few minutes). Private content (private to the user): we can make it last for years. Signed URL = access to individual files (one signed URL per file). Signed Cookies = access to multiple files (one signed cookie for many files). When the backend content is modified, CloudFront will not get it until its TTL has expired. But we can force an entire cache refresh with CloudFront Invalidation. CloudFront supports HTTP/RTMP protocol based requests only. FAQs Pricing AWS Outposts \u00b6 Outpost is fully managed service that extends AWS infrastructure, services, APIs, and tools to customer premises. It is a Rack with n server blades installed by AWS team, on site, and then maintained by AWS. An Outpost is a pool of AWS compute and storage capacity deployed at a customer site. It extends a VPC from one AWS region, it is owned by one AZ and you can use it for increasing resilience. See pricing for Outpost rack High Performance Computing (HCP) \u00b6 The services that helps to design HPC solutions are: For data management and transfer: Direct Connect : moves GB of data over private secure network. Snowball and & snowmobile at PB data level. DataSync moves large dataset between on-premises and S3, FSx for Windows, EFS. Computation and network: EC2 instance type using CPU or GPU optimized. Spot fleet and spot instances for cost savings and using Auto Scaling Group. Cluster Placement groups in same rack and AZ to get best network performance. Enhanced Networking (SR-IOV): with high bandwidth, higher PPS, lower latency. Elastic Network Adapter (ENA) is up to 100 Gbps. Intel 82599 VF up to 10 Gbps (more legacy solution). Elastic Fabric Adapter (EFA), dedicated ENA for HPC, only for Linux. Improve inter-node communication, for tightly coupled workload. It leverages Message Passing Interface standard to bypass the Linux kernel to provide low-latency transport. Storage: Instance storage with EBS scale to 256k IOPS with io2 Block Express or instance store to scale to millions of IOPS. Metwork storage: S3, EFS and FSx for Lustre. Automate and orchestrate: AWS Batch to support multi-node parallel jobs. Easily schedule jobs and launch EC2 instance accordingly. ParallelCluster: open source project for cluster management, using infrastructure as code, and EFA. Simple Email Service - SES \u00b6 Fully managed service to send and receive emails. Used to send email from applications using API, SMTP or AWS console. Amazon PinPoint \u00b6 Marketing communication services, SMS, email, push, voice, and in-app messaging. It supports customer segmentation and personalized messages. Systems Manager \u00b6 AWS Systems Manager is a collection of capabilities to help manage our applications and infrastructure running. Systems Manager simplifies application and resource management, shortens the time to detect and resolve operational problems, and helps us manage our AWS resources securely at scale. Ability to control AWS infrastructure like EC2, Amazon Relational Database Service (RDS), Amazon Elastic Container Service (ECS), and Amazon Elastic Kubernetes Service (EKS) instances, with an unified user's experience. It includes a set of services like Session Manager, Patch Manager, Run Commands, Inventory or define maintenance windows. Automation is the motivation to simplify maintenance and deployment tasks of EC2 instances, like automating runbooks. It includes, Parameter Store which provides secure, hierarchical storage for configuration data and secrets management. You can store data such as passwords, database strings, Amazon Elastic Compute Cloud (Amazon EC2) instance IDs and Amazon Machine Image (AMI) IDs, and license codes as parameter values. Parameter Store is also integrated with Secrets Manager Cost Explorer \u00b6 Tool to view and analyze our costs and usage. Trusted Advisor \u00b6 Trusted Advisor inspects your AWS environment, and then makes recommendations when opportunities exist to save money, to improve system availability and performance, or to help close security gaps. AWS Health \u00b6 AWS Health provides ongoing visibility into your resource performance and the availability of your AWS services and accounts. Can be used as a way to automate the start and stop of the Amazon EC2 instance: set up an Amazon EventBridge rule that is triggered by the AWS Health event. Target a Lambda function to parse the incoming event and reference the Amazon EC2 instance, ID included. Have the function perform a stop and start of the instance. AWS Compute Optimizer \u00b6 AWS Compute Optimizer allows you to automate the collection of metrics for underutilized and underperforming compute instances. It can then generate recommendations for you to save money. AWS Application Migration Service \u00b6 AWS MGN is the primary migration service recommended for lift-and-shift migrations to AWS without having to make any changes to the applications, the architecture, or the migrated servers. Implementation begins by installing the AWS Replication Agent on the source servers. When launch Test or Cutover instances are laucned, AWS Application Migration Service automatically converts the source servers to boot and run natively on AWS. Can be used to migrate Amazon Elastic Compute Cloud (EC2) workloads across AWS Regions, Availability Zones, or accounts.","title":"EC2-others"},{"location":"infra/#major-infrastructure-services","text":"Info Updated 12/26/2022","title":"Major infrastructure services"},{"location":"infra/#amazon-elastic-compute-cloud-ec2-components","text":"EC2 is a renting machine. Amazon EC2 instances are a combination of virtual processors (vCPUs), memory, network, graphics processing units (GPUs), and, in some cases, instance storage. Only size for what we plan to use. Storing data on virtual drives: EBS . Distribute load across machines using ELB . Auto scale the service via group: ASG . EC2 can have MacOS, Linux or Windows OS. Amazon Machine Image (AMI) is the OS image with preinstalled softwares. Amazon Linux 2 for linux base image. See AMI Catalog within the selected region to get what AMI could be used. Figure 1: EC2 instance When creating an instance, we can select the OS, CPU, RAM, the VPC, the AZ subnet, the storage (EBS) for root folder, the network card, and the firewall rules defined as Security Group . The security group helps to isolate the instance, for example, authorizing traffic for ssh on port 22 (TCP) and HTTP on port 80. Get the public ssh key, and when the instance is started, use a command like: ssh -i EC2key.pem ec2-user@ec2-52-8-75-8.us-west-1.compute.amazonaws.com to connect to the EC2 via ssh. On the client side, the downloaded .pem file needs to be restricted with chmod 0400 . We can also use EC2 Instance Connect to open a terminal in the web browser. Still needs to get SSH port accessible in the security group. See this EC2 playground for demonstrating the deployment of a HTTP server.","title":"Amazon Elastic Compute Cloud - EC2 components"},{"location":"infra/#ec2-life-cycle","text":"When we launch an instance, it enters in the pending state. Billing is started when in running state. During rebooting, instance remains on the same host computer, and maintains its public and private IP addresses, in addition to any data on its instance store. With stopped instance, the instance is shut down and cannot be used. The instance can be restarted at any time. When we terminate an instance, the instance stores are erased, and we lose both the public IP and private IP addresses of the machine. Storage for any Amazon EBS volumes is still charged. Note that Reserved Instances that applied to terminated instances are still billed until the end of their term according to their payment option. We will be billed when our On-Demand instance is preparing to hibernate with a stopping state. When we launch a new EC2 instance, the EC2 service attempts to place the instance in such a way that all of our instances are spread out across underlying hardware to minimize correlated failures. We may use placement groups to influence the placement of a group of interdependent instances to meet the needs of our workload.","title":"EC2 life cycle"},{"location":"infra/#ec2-types","text":"EC2 has a section to add User data , which could be used to define a bash script to install dependent software and to start some services at boot time (like httpd). EC2 instance types like t2.micro or c5.2xlarge define CPU, memory... (see ec2instances.info or the reference AWS ec2/instance-types ). The first letter defines the class as: R: (memory) applications that needs a lot of RAM \u2013 in-memory caches. C: (Compute Optimized) applications that needs good CPU \u2013 compute / databases, ETL media transcoding, High Perf web servers, scientific modeling. M: applications that are balanced (think \u201cmedium\u201d) \u2013 general / web app. I: (storage) applications that need good local I/O (instance storage) \u2013 databases, NoSQL, cache like Redis, data warehousing, distributed file systems. G: applications that need a GPU. T2/T3 for burstable instance: When the machine needs to process something unexpected (a spike in load for example), it can burst. Use burst credits to control CPU usage. Graviton processors are designed by AWS for cloud workloads to optimize cost and energy consumption. (t4g. , M6g. , C7*)","title":"EC2 types"},{"location":"infra/#ec2-nitro-system","text":"Next generation of EC2. It uses new virtualization infrastructure and hypervisor. Supports IPv6, better I/O on EBS and better security. Name type starts with C5, D5,... vCPU represents thread running on core CPU. We can optimize vCPU allocation on the EC2 instance, once created, by updating the launch configuration. There is a vCPU-based On-Demand Instance limit per region which may impact the creation of new instance. Just submit the limit increase form to AWS and retry the failed requests once approved.","title":"EC2 Nitro System"},{"location":"infra/#launch-types","text":"On demand : short workload, predictable pricing, pay per second after first minute. No long term commitment. Reserved for one or 3 years term, used for long workloads like database. Get discounted rate from on-demand. Up to 72% discount. Upfront cost and pay monthly. We can buy and sell it in the marketplace. When no more needed, Terminate the Reserved instances as soon as possible to avoid getting billed at the on-demand price when it expires. Convertible reserved instance for changing resource capacity over time. Scheduled reserved instance for job based workload. Dedicated hosts to book entire physical server and control instance placement. # years. BYOL . (Used to port Microsoft license) Can be on-demand or reserved. Most expensive solution. Use it when we deploy a database technology on an EC2 instance and the vendor license bills based on the physical cores. Baremetal is part of this option. Capacity reservations : reserve capacity in a specific AZ for any duration. Spot instance for very short - 90% discount on on-demand - used for work resilient to failure like batch job, data analysis, image processing, stateless, containerized... Define a max spot price and get the instance while the current spot price < max price wanting to pay. The hourly spot price varies based on offer and capacity. If the current spot price > max, then instance will be stopped in a 2 minutes. With spot block we can define a time frame without interruptions from 1 to 6 hours. The expected state is defined in a 'spot request' which can be cancelled. One time or persistent request types are supported. Cancel a spot request does not terminate instance, but need to be the first thing to do to avoid cost. Spot fleets allow to automatically request spot instance and on-demand instance with the lowest price to meet the target capacity within the price constraints. Use EC2 launch templates to automate instance launches, to simplify permission policies, and to enforce best practices across the organization. Look very similar to docker image.","title":"Launch types"},{"location":"infra/#metadata","text":"When in a EC2 instance shell, we can get access to EC2 metadata by going to the URL: http://169.254.169.254/latest/meta-data/ . We can also review scripts used to bootstrap the instances at runtime using http://169.254.169.254/latest/user-data/","title":"Metadata"},{"location":"infra/#ami","text":"Bring our own image. Shareable on Amazon Marketplace. Can be saved on S3 storage. By default, our AMIs are privates, and locked for our account / region. AMIs can be copied and shared See AWS doc - copying an AMI .","title":"AMI"},{"location":"infra/#ec2-hibernate","text":"The in memory state is preserved, persisted to a file in the root EBS volume. It helps to make the instance startup time quicker. The root EBS volume needs to be encrypted. Memory is constrained by 150GB RAM. No more than 60 days. No instance store volume possible.","title":"EC2 Hibernate"},{"location":"infra/#basic-fault-tolerance","text":"The following diagram illustrates some fault tolerance principles offered by the basic AWS services: Figure 2 AMI defines image for the EC2 with static or dynamic configuration. From one AMI, we can scale by adding new EC2 based on same image. Instance failure can be replaced by starting a new instance from the same AMI. Auto Scaling Group defines a set of EC2 instances, and can start new EC2 instance automatically. Auto scaling adjusts the capacity of EC2 and EC2 instance within the group. To minimize down time, we can have one EC2 instance in Standby mode, and use elastic IP addresses to be reassigned in case of the primary EC2 failure. Data is saved on EBS and replicated to other EBS inside the same availabiltiy zone. Snapshot backup can be done to replicate data between AZs and/or regions, and persisted for long retention in S3. Need to flush data from memory to disk before any snapshot. Applications can be deployed between AZs. Elastic Load Balancer balances traffic among servers in multiple AZs and DNS will route traffic to the good server. Elastic IP addresses are static and defined at the AWS account level. New EC2 instance can be reallocated to Elastic IP @, but they are mapped by internet gateway to the private address of the EC2. The service may be down until new EC2 instace is restarted. ELB ensures higher fault tolerance for EC2s, containers, lambdas, IP addresses and physical servers. Application LB load balances at the HTTP, HTTPS level, and within a VPC based on the content of the request. NLB is for TCP, UDP, TLS routing and load balancing.","title":"Basic Fault Tolerance"},{"location":"infra/#placement-groups","text":"Define strategy to place EC2 instances: Cluster : groups instances into a low-latency group in a single Availability Zone. Highest performance while talking to each other as when performing big data analysis. Spread : groups across underlying hardware (max 7 instances per group per AZ). Reduced risk in case of simultaneous failure. EC2 Instances are on different physical hardware. For application that needs to maximize high availability. Critical Applications where each instance must be isolated from failure from each other. Partition : spreads instances across many different partitions (which rely on different sets of racks) within an AZ. Partition is a set of racks. Up to 100s of EC2 instances. The instances in a partition do not share racks with the instances in the other partitions. A partition failure can affect many EC2s but won\u2019t affect other partitions. EC2 instances get access to the partition information as metadata. HDFS, HBase, Cassandra, Kafka Access from network and policies menu, define the group with expected strategy, and then use it when creating the EC2 instance by adding the instance to a placement group.","title":"Placement groups"},{"location":"infra/#ec2-network-bandwidth","text":"Each EC2 instance has a maximum bandwidth for aggregate inbound and outbound traffic, based on instance type and size. The network bandwidth available to an EC2 instance depends on several factors. Bandwidth depends on the number of vCPUs configured, and for less than 32 vCPUs the limit (single flow of 5-tuple) (source IP address/port number, destination IP address/port number and the protocol) is around 5 Gbps when instances are not in the same cluster placement group. Also bandwidth depends on the flow type: within on region or cross-regions. To meet additional demand, they can use a network I/O credit mechanism to burst beyond their baseline bandwidth. However, instances might not achieve their available network bandwidth, if they exceed network allowances at the instance level, such as packet per second or number of tracked connections. One EC2 connected to EC2 instance in same region. Recalls that Enterprise-grade WAN and DIA links more commonly have symmetrical bandwidth, the data capacity is the same in both directions.","title":"EC2 network bandwidth"},{"location":"infra/#ec2-instance-store","text":"When disk performance is a strong requirement, use EC2 Instance Store. Millions IOPS read or even write. It loses data when stopped. Good use for buffer, cache, scratch data, or distributed systems with their own replication like Kafka. Backup and replication are the user's responsability","title":"EC2 Instance Store"},{"location":"infra/#quota","text":"Service Quotas is an AWS service that helps manage our quotas for over 100 AWS services from one location. The AWS account has default quotas, formerly referred as limits, defined for each AWS service. Unless otherwise noted, each quota is Region specific. We can request increases for some quotas, and other quotas cannot be increased. Each EC2 instance can have a variance of the number of vCPUs, depending on its type and configuration, so it's always wise to calculate the vCPU needs to make sure we are not going to hit quotas too early. Along with looking up the quota values, we can also request a quota increase from the Service Quotas console.","title":"Quota"},{"location":"infra/#security-group","text":"Define inbound and outbound security rules. It is like a virtual firewall inside an EC2 instance. SGs regulate access to ports, authorized IP ranges IPv4 and IPv6, control inbound and outbound network. By default all inbound traffic is denied and outbound authorized. They contain allow rules only. Can be attached to multiple EC2 instances and to load balancers. Locked down to a region / VPC combination. Live outside of the EC2. Define one separate security group for SSH access where we can authorize only one IP@. Connect refused is an application error or the app is not launched - Spinning wheel in the web browser is an access rules error. Instances with the same security group can access each other. Security group can reference other security groups, on IP address using CIDR in the form 192.45.23.12/32 but not any DNS server. Figure 3: security group with inbound rules Important Ports: 22 for SSH (Secure Shell) and SFTP. 21 for FTP. 80 for HTTP. 443 for https. 3389: Remote desktop protocol.","title":"Security group"},{"location":"infra/#auto-scaling-group-asg","text":"The goal of an ASG is to scale out (add EC2 instances) to match an increased load, or scale in (remove EC2 instances) to match a decreased load. It helps to provision and balance capacity across Availability Zones to optimize availability. It can also ensure we have a minimum and a maximum number of machines running. It detects when an instance is unhealthy. Automatically Register new instances to a load balancer. ASG has the following attributes: AMI + Instance Type with EC2 User Data (Can use template to define instances). EBS Volumes. Security Groups. SSH Key Pair. Min Size / Max Size / Initial Capacity to control number of instances . Network + Subnets Information to specify where to run the EC2 instances. Load Balancer Information, with target groups to be used as a grouping mechanism of the newly created instances. Scaling Policies help to define rules to manage instance life cycle, based for example on CPU usage or network bandwidth used. It is not possible to modify a launch configuration once it is created. If we need to change the EC2 instance type for example, we need to create a new launch configuration to use the correct instance type. Modify the Auto Scaling group to use this new launch configuration. Delete the old launch configuration as it is no longer needed. The Default Termination Policy is designed to help ensure that our instances span Availability Zones evenly for high availability. Determine which Availability Zones have the most instances, and at least one instance that is not protected from scale in. Determine which instances to terminate so as to align the remaining instances to the allocation strategy for the on-demand or spot instance that is terminating. This only applies to an Auto Scaling Group that specifies allocation strategies. For example, after our instances launch, we change the priority order of our preferred instance types. When a scale-in event occurs, Amazon EC2 Auto Scaling tries to gradually shift the on-demand instances away from instance types that are lower priority. Determine whether any of the instances use the oldest launch template or configuration: (For Auto Scaling Groups that use a launch template) Determine whether any of the instances use the oldest launch template unless there are instances that use a launch configuration. Amazon EC2 Auto Scaling terminates instances that use a launch configuration before instances that use a launch template. For Auto Scaling Groups that use a launch configuration: Determine whether any of the instances use the oldest launch configuration. After applying all of the above criteria, if there are multiple unprotected instances to terminate, determine which instances are closest to the next billing hour. If there are multiple unprotected instances closest to the next billing hour, terminate one of these instances at random. When creating scaling policies, CloudWatch alarms are created. Ex: \"Create an alarm if: CPUUtilization < 36 for 15 data points within 15 minutes\". Target tracking scaling: we want average CPU to be under 40%. Scheduled action: increase capacity after 5 PM. Predictive scaling by looking at historical behavior to build forecast rules. ASG tries to balance the number of instances across AZs by default, and then deletes based on the age of the launch configuration. The capacity of our ASG cannot go over the maximum capacity we have allocated during scale out events. Cool down period is set to 5 mn and will not change the number of instance until this period is reached. When an ALB validates an health check issue, ASG terminates the EC2 instance.","title":"Auto Scaling Group (ASG)"},{"location":"infra/#cloudfront","text":"Content Delivery Network service with DDoS protection. It caches data to the edge to improve web browsing and application performance. 410+ Edge locations. This is a global service. The origins of those files are S3 bucket objects, or Custom Origin resource accessible via HTTP (ALB, EC2...). CloudFront keeps cache for the data read. For the edge to access the S3 bucket, it uses an origin access identity (OAI), managed as IAM role. For EC2 instance, the security group needs to accept traffic from edge location IP addresses. It is possible to control with geographic restriction using whitelist or blacklist. It also supports the concept of signed URL. When we want to distribute content to different user groups over the world, attach a policy with: URL expiration. IP range to access the data from. Trusted signers (which AWS accounts can create signed URLs). How long should the URL be valid for? Shared content (movie, music): make it short (a few minutes). Private content (private to the user): we can make it last for years. Signed URL = access to individual files (one signed URL per file). Signed Cookies = access to multiple files (one signed cookie for many files). When the backend content is modified, CloudFront will not get it until its TTL has expired. But we can force an entire cache refresh with CloudFront Invalidation. CloudFront supports HTTP/RTMP protocol based requests only. FAQs Pricing","title":"CloudFront"},{"location":"infra/#aws-outposts","text":"Outpost is fully managed service that extends AWS infrastructure, services, APIs, and tools to customer premises. It is a Rack with n server blades installed by AWS team, on site, and then maintained by AWS. An Outpost is a pool of AWS compute and storage capacity deployed at a customer site. It extends a VPC from one AWS region, it is owned by one AZ and you can use it for increasing resilience. See pricing for Outpost rack","title":"AWS Outposts"},{"location":"infra/#high-performance-computing-hcp","text":"The services that helps to design HPC solutions are: For data management and transfer: Direct Connect : moves GB of data over private secure network. Snowball and & snowmobile at PB data level. DataSync moves large dataset between on-premises and S3, FSx for Windows, EFS. Computation and network: EC2 instance type using CPU or GPU optimized. Spot fleet and spot instances for cost savings and using Auto Scaling Group. Cluster Placement groups in same rack and AZ to get best network performance. Enhanced Networking (SR-IOV): with high bandwidth, higher PPS, lower latency. Elastic Network Adapter (ENA) is up to 100 Gbps. Intel 82599 VF up to 10 Gbps (more legacy solution). Elastic Fabric Adapter (EFA), dedicated ENA for HPC, only for Linux. Improve inter-node communication, for tightly coupled workload. It leverages Message Passing Interface standard to bypass the Linux kernel to provide low-latency transport. Storage: Instance storage with EBS scale to 256k IOPS with io2 Block Express or instance store to scale to millions of IOPS. Metwork storage: S3, EFS and FSx for Lustre. Automate and orchestrate: AWS Batch to support multi-node parallel jobs. Easily schedule jobs and launch EC2 instance accordingly. ParallelCluster: open source project for cluster management, using infrastructure as code, and EFA.","title":"High Performance Computing (HCP)"},{"location":"infra/#simple-email-service-ses","text":"Fully managed service to send and receive emails. Used to send email from applications using API, SMTP or AWS console.","title":"Simple Email Service - SES"},{"location":"infra/#amazon-pinpoint","text":"Marketing communication services, SMS, email, push, voice, and in-app messaging. It supports customer segmentation and personalized messages.","title":"Amazon PinPoint"},{"location":"infra/#systems-manager","text":"AWS Systems Manager is a collection of capabilities to help manage our applications and infrastructure running. Systems Manager simplifies application and resource management, shortens the time to detect and resolve operational problems, and helps us manage our AWS resources securely at scale. Ability to control AWS infrastructure like EC2, Amazon Relational Database Service (RDS), Amazon Elastic Container Service (ECS), and Amazon Elastic Kubernetes Service (EKS) instances, with an unified user's experience. It includes a set of services like Session Manager, Patch Manager, Run Commands, Inventory or define maintenance windows. Automation is the motivation to simplify maintenance and deployment tasks of EC2 instances, like automating runbooks. It includes, Parameter Store which provides secure, hierarchical storage for configuration data and secrets management. You can store data such as passwords, database strings, Amazon Elastic Compute Cloud (Amazon EC2) instance IDs and Amazon Machine Image (AMI) IDs, and license codes as parameter values. Parameter Store is also integrated with Secrets Manager","title":"Systems Manager"},{"location":"infra/#cost-explorer","text":"Tool to view and analyze our costs and usage.","title":"Cost Explorer"},{"location":"infra/#trusted-advisor","text":"Trusted Advisor inspects your AWS environment, and then makes recommendations when opportunities exist to save money, to improve system availability and performance, or to help close security gaps.","title":"Trusted Advisor"},{"location":"infra/#aws-health","text":"AWS Health provides ongoing visibility into your resource performance and the availability of your AWS services and accounts. Can be used as a way to automate the start and stop of the Amazon EC2 instance: set up an Amazon EventBridge rule that is triggered by the AWS Health event. Target a Lambda function to parse the incoming event and reference the Amazon EC2 instance, ID included. Have the function perform a stop and start of the instance.","title":"AWS Health"},{"location":"infra/#aws-compute-optimizer","text":"AWS Compute Optimizer allows you to automate the collection of metrics for underutilized and underperforming compute instances. It can then generate recommendations for you to save money.","title":"AWS Compute Optimizer"},{"location":"infra/#aws-application-migration-service","text":"AWS MGN is the primary migration service recommended for lift-and-shift migrations to AWS without having to make any changes to the applications, the architecture, or the migrated servers. Implementation begins by installing the AWS Replication Agent on the source servers. When launch Test or Cutover instances are laucned, AWS Application Migration Service automatically converts the source servers to boot and run natively on AWS. Can be used to migrate Amazon Elastic Compute Cloud (EC2) workloads across AWS Regions, Availability Zones, or accounts.","title":"AWS Application Migration Service"},{"location":"infra/messaging/","text":"Integration and middleware: SQS, SNS \u00b6 SQS: Standard queue \u00b6 Oldest queueing service on AWS. Full managed service. Unlimited throughput and unlimited number of message in queue The default retention is 4 days up to 14 days. low latency < 10ms. Max mesage size is 256KB. Duplicate messages is possible (at least once delivery) and out of order too (best effort). Consumer deletes the message. Supports automatic scaling. Specific SDK to integrate to SendMessage , GetMessage... Consumers receive, process and then delete the messages. Parallelism is possible. The consumers can be in an auto scaling group (ASG) and with CloudWatch, it is possible to monitor the queue size / # of instances and in the CloudWatch alarm action, trigger EC2 scaling. Message has metadata out of the box. After a message is polled by a consumer, it becomes invisible to other consumers. Fan-out pattern \u00b6 When we need to send the same message to more than one SQS consumer, we need to combine SNS and SQS: message is sent to the SNS topic and then \"fan-out\" to multiple SQS queues. It's fully decoupled, no data loss, and you have the ability to add more SQS queues (more applications) over time. This is the pattern to use if consumer may be down for a long period: solution based on SNS only will make some messages not processed and lost. Visibility timeout \u00b6 By default, the \u201cmessage visibility timeout\u201d is 30 seconds, which means the message has 30 seconds to be processed (Amazon SQS prevents other consumers from receiving and processing the message). If a consumer fails to process a message within the Visibility Timeout, the message goes back to the queue. After the message visibility timeout is over, the message is \u201cvisible\u201d in SQS, therefore the message may be processed twice. But a consumer could call the ChangeMessageVisibility API to get more time to process. When the visibility timeout is high (hours), and the consumer crashes then the re-processing of all the messages will take time. If it is set too low (seconds), we may get duplicates. To reduce the number of API call to request message (improve latency and app performance), consumer can use the long polling API and wait for message arrival. Dead Letter Queue \u00b6 We can set a threshold for how many times a message can go back to the queue. After the MaximumReceives threshold is exceeded, the message goes into a dead letter queue (DLQ) (which has a limit of 14 days to process). Delay queues let you postpone the delivery of new messages to a queue for a number of seconds. If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period. The default (minimum) delay for a queue is 0 seconds. The maximum is 15 minutes. For security, there is encryption in fight via HTTPS, and at rest with KMS keys. SQL API access control via IAM policies, and SQS Access Policies for cross-account access and for other AWS services to access SQS. It comes with monitoring. FIFO Queue \u00b6 Queue can be set as FIFO to guaranty the order: limited to throughput at 300 msg/s without batching or 3000 msg/s with batching. It can also support exactly once delivery. While configuring the FIFO queue a parameter can be set to remove duplicate by looking at the content. The name of the queue has to end with .fifo . If we don't use a Group ID, messages are consumed in the order they are sent, with only one consumer. But using Group ID we can have as many consumers as there is groups. It looks like partition key in kinesis data streams. Each consumer will get ordered records. Security \u00b6 Access policies can be defined at the queue level or at the user level (using IAM policies) that can grant rights to perform queue-based actions like get/put/list. Sample Code \u00b6 Python boto3 SQS See Python aws folder SNS - Simple Notification Service \u00b6 Used for pub/sub communication. Producer sends message to one SNS Topic. SNS pushes data to subscribers. SNS supports up to 12,500,000 subscriptions per topic, 100,000 topics limit. Each subscriber to the topic will get all the messages. As data is not persisted, we may lose messages not processed in a time window. The producers can publish to topic via SDK and can use different protocols like: HTTP / HTTPS (with delivery retries \u2013 how many times), SMTP, SMS, ... The subscribers can be a SQS, a Lambda, Kinesis Firehose, Emails... But not Kinesis Data Streams Many AWS services can send data directly to SNS for notifications: CloudWatch (for alarms), AWS budget, Lambda, Auto Scaling Groups notifications, Amazon S3 (on bucket events), DynamoDB, CloudFormation, AWS DMS, RDS Event. SNS can be combined with SQS: Producers push once in SNS, receive in all SQS queues that they subscribed to. It is fully decoupled without any data loss. SQS allows for data persistence, delayed processing and retries. SNS cannot send messages to SQS FIFO queues. For security it supports HTTPS, and encryption at REST with KSM keys. For access control, IAM policies can be defined for the SNS API (looks like S# policies). Same as SQS, used for cross account access and with other services. Combining with SQS - Fan Out pattern \u00b6 An application puch once in a SNS Service, and the SQS queues are subscribers to the topic and then get the messages. Fan Out. Fully decoupled with no data loss SQS adds data persistence, delayed processing and retries of work Increase the number of subscriber over time. Using SNS FIFO and SQS FIFO it will keep ordering. Can use Message Filtering using a JSON policy Kinesis \u00b6 See dedicated note Amazon MQ \u00b6 Amazon MQ is a managed message broker for RabbitMQ or ActiveMQ. It run on EC2 servers, and support multi-AZs deployment with failover. To support High availability, one server in a different AZ will be in passive mode, and will get data from Amazon EFS.","title":"Messaging"},{"location":"infra/messaging/#integration-and-middleware-sqs-sns","text":"","title":"Integration and middleware: SQS, SNS"},{"location":"infra/messaging/#sqs-standard-queue","text":"Oldest queueing service on AWS. Full managed service. Unlimited throughput and unlimited number of message in queue The default retention is 4 days up to 14 days. low latency < 10ms. Max mesage size is 256KB. Duplicate messages is possible (at least once delivery) and out of order too (best effort). Consumer deletes the message. Supports automatic scaling. Specific SDK to integrate to SendMessage , GetMessage... Consumers receive, process and then delete the messages. Parallelism is possible. The consumers can be in an auto scaling group (ASG) and with CloudWatch, it is possible to monitor the queue size / # of instances and in the CloudWatch alarm action, trigger EC2 scaling. Message has metadata out of the box. After a message is polled by a consumer, it becomes invisible to other consumers.","title":"SQS: Standard queue"},{"location":"infra/messaging/#fan-out-pattern","text":"When we need to send the same message to more than one SQS consumer, we need to combine SNS and SQS: message is sent to the SNS topic and then \"fan-out\" to multiple SQS queues. It's fully decoupled, no data loss, and you have the ability to add more SQS queues (more applications) over time. This is the pattern to use if consumer may be down for a long period: solution based on SNS only will make some messages not processed and lost.","title":"Fan-out pattern"},{"location":"infra/messaging/#visibility-timeout","text":"By default, the \u201cmessage visibility timeout\u201d is 30 seconds, which means the message has 30 seconds to be processed (Amazon SQS prevents other consumers from receiving and processing the message). If a consumer fails to process a message within the Visibility Timeout, the message goes back to the queue. After the message visibility timeout is over, the message is \u201cvisible\u201d in SQS, therefore the message may be processed twice. But a consumer could call the ChangeMessageVisibility API to get more time to process. When the visibility timeout is high (hours), and the consumer crashes then the re-processing of all the messages will take time. If it is set too low (seconds), we may get duplicates. To reduce the number of API call to request message (improve latency and app performance), consumer can use the long polling API and wait for message arrival.","title":"Visibility timeout"},{"location":"infra/messaging/#dead-letter-queue","text":"We can set a threshold for how many times a message can go back to the queue. After the MaximumReceives threshold is exceeded, the message goes into a dead letter queue (DLQ) (which has a limit of 14 days to process). Delay queues let you postpone the delivery of new messages to a queue for a number of seconds. If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period. The default (minimum) delay for a queue is 0 seconds. The maximum is 15 minutes. For security, there is encryption in fight via HTTPS, and at rest with KMS keys. SQL API access control via IAM policies, and SQS Access Policies for cross-account access and for other AWS services to access SQS. It comes with monitoring.","title":"Dead Letter Queue"},{"location":"infra/messaging/#fifo-queue","text":"Queue can be set as FIFO to guaranty the order: limited to throughput at 300 msg/s without batching or 3000 msg/s with batching. It can also support exactly once delivery. While configuring the FIFO queue a parameter can be set to remove duplicate by looking at the content. The name of the queue has to end with .fifo . If we don't use a Group ID, messages are consumed in the order they are sent, with only one consumer. But using Group ID we can have as many consumers as there is groups. It looks like partition key in kinesis data streams. Each consumer will get ordered records.","title":"FIFO Queue"},{"location":"infra/messaging/#security","text":"Access policies can be defined at the queue level or at the user level (using IAM policies) that can grant rights to perform queue-based actions like get/put/list.","title":"Security"},{"location":"infra/messaging/#sample-code","text":"Python boto3 SQS See Python aws folder","title":"Sample Code"},{"location":"infra/messaging/#sns-simple-notification-service","text":"Used for pub/sub communication. Producer sends message to one SNS Topic. SNS pushes data to subscribers. SNS supports up to 12,500,000 subscriptions per topic, 100,000 topics limit. Each subscriber to the topic will get all the messages. As data is not persisted, we may lose messages not processed in a time window. The producers can publish to topic via SDK and can use different protocols like: HTTP / HTTPS (with delivery retries \u2013 how many times), SMTP, SMS, ... The subscribers can be a SQS, a Lambda, Kinesis Firehose, Emails... But not Kinesis Data Streams Many AWS services can send data directly to SNS for notifications: CloudWatch (for alarms), AWS budget, Lambda, Auto Scaling Groups notifications, Amazon S3 (on bucket events), DynamoDB, CloudFormation, AWS DMS, RDS Event. SNS can be combined with SQS: Producers push once in SNS, receive in all SQS queues that they subscribed to. It is fully decoupled without any data loss. SQS allows for data persistence, delayed processing and retries. SNS cannot send messages to SQS FIFO queues. For security it supports HTTPS, and encryption at REST with KSM keys. For access control, IAM policies can be defined for the SNS API (looks like S# policies). Same as SQS, used for cross account access and with other services.","title":"SNS - Simple Notification Service"},{"location":"infra/messaging/#combining-with-sqs-fan-out-pattern","text":"An application puch once in a SNS Service, and the SQS queues are subscribers to the topic and then get the messages. Fan Out. Fully decoupled with no data loss SQS adds data persistence, delayed processing and retries of work Increase the number of subscriber over time. Using SNS FIFO and SQS FIFO it will keep ordering. Can use Message Filtering using a JSON policy","title":"Combining with SQS - Fan Out pattern"},{"location":"infra/messaging/#kinesis","text":"See dedicated note","title":"Kinesis"},{"location":"infra/messaging/#amazon-mq","text":"Amazon MQ is a managed message broker for RabbitMQ or ActiveMQ. It run on EC2 servers, and support multi-AZs deployment with failover. To support High availability, one server in a different AZ will be in passive mode, and will get data from Amazon EFS.","title":"Amazon MQ"},{"location":"infra/networking/","text":"Networking \u00b6 All regions are interconnected via private AWS fiber links. This drives better availability, higher performance, lower jitter and reduced costs. Each region has redundant path to transit centers, which connect to private links to other AWS regions, and to AWS Direct Connect customers' data centers, internet via peering and paid transit. The connections between AZs is a metro area over DWDM (Dense wavelength division multiplexing) links. 82k fibers in a region. single digit milisecond latency. 25Tbps peak inter AZs traffic. IPv4 allows 3.7 billions of different addresses. Private IP @ is for private network connections. Internet gateway has public and private connections. Public IP can be geo-located. When connected to an EC2 the prompt lists the private IP ( ec2-user@ip-172-31-18-48 ). Private IP stays stable on instance restart, while public may change. With Elastic IP address, we can mask an EC2 instance failure by rapidly remapping the address to another instance. But better to use DNS. Elastic IP is a public IPv4 that we own as long as we want and we can attach it to one EC2 instance at a time. It is not free. In the context of High Performance Computing (HPC) and ML applications, or OS-bypass, we can use EFA. Virtual Private Cloud \u00b6 A virtual private cloud (VPC) is a virtual network dedicated to our AWS account. All new accounts have a default VPC. It is logically isolated from other virtual networks in the AWS Cloud. We can launch our AWS resources, such as Amazon EC2 instances, within our VPC. New EC2 instances are launched into the default VPC if no subnet is specified. When defining new VPC, we can specify CIDR, add subnets, associate security groups, ACL, and configure route tables. Figure 4: VPC By default, AWS creates a VPC with default public subnets, one per AZs, they are public because the main route table sends the subnet's traffic that is destined for the internet to the internet gateway. VPC Helps to: Assign static IP addresses, potentially multiple addresses for the same EC2 instance. Change security group membership for our instances while they're running. Control the outbound traffic from our instances (egress filtering) in addition to controlling the inbound traffic to them (ingress filtering). We can have multiple VPCs per region (max to 5 but this is a soft limit). 5 maximum CIDR per VPC. The IP range is min /28 and max /16. Default VPC includes an Internet Gateway . Internet gateway is a managed service and automatically scales, is redundant and highly available. Network Access Control List can be defined at the VPC level, so will be shared between subnets. The default network ACL is configured to allow all traffic to flow in and out of the subnets which it is associated. Each network ACL also includes a rule whose rule number is an asterisk. This rule ensures that if a packet doesn't match any of the other numbered rules, it's denied. The following diagram illustrates classical VPC, as defined years ago, with one vpc, two availability zones, two public subnets with EC2 instances within those subnets and two AZs. An internet gateway connected to a router. Subnet is defined within a VPC and in one availability zone. It defines an IP CIDR range: we should have less IP on public subnet as they are used for ELB and very few resources. Most of the services should be in private subnet. Figure 5: EC2s in public subnets A subnet is assigned a /24 CIDR block, which means 8 bits encoding (32-24), but AWS uses 5 IP addresses in each subnet for gateway, LB,... so the number of available addresses is 256 - 5 = 251. To identify a single 32 bit IPv4 address, we can use /32 CIDR convention Internet Gateway needs to be defined to any newly created VPC to get internet hosts being able to access EC2s. Route Tables need to be defined: one public associated with the public subnets and one private associated with private subnets. Non-default subnet has a private IPv4 address, but no public IPv4. We can make a default subnet into a private subnet by removing the route from the destination 0.0.0.0/0 to the Internet Gateway. The table in figure above defines such route, therefore the subnets are public. EC2 Instances should have either public IP or elastic IP and the subnet they belong to, must have a route to the internet gateway. The figure 5 above illustrates a route coming from any IP @ (0.0.0.0/0) goes to the internet gateway ( igw-id ). Any host in the private network 172.31.0.0/16 can communicate with other hosts in the local network. Route tables defines 172.31 as local with /20 CIDR address range, internal to the VPC. Default route to internet goes to the IGW, which has an elastic IP address assigned to it. Because the VPC is cross AZs, we need a router to route between subnets. (See TCP/IP summary ) Routing Tables \u00b6 As illustrated in the following diagram, the main routing table addresses internal to the VPC traffic, while custom tables define how inbound and outbound traffic can be structured within a subnet. Figure 6: Routing tables EC2 instances in subnet 2 are in private subnet. The corporate network, or on-premises network, can be connected to the VPC via customer gateway deployed on-premises, vitual private GTW in AWS region, and VPN connection between the two gateways. Hands-on work \u00b6 The CDK example in the ec2-vpc folder supports the following definitions: Figure 7: More classical VPC We can enable internet access for an EC2 instance launched into a non-default subnet by attaching an internet gateway to its VPC and configure routing tables for each subnets. The routes needs to define that traffic from internet goes to the IGW. The following route is associated to the public subnet-1: Figure 8: Route to Internet Gateway Alternatively, to allow an instance in our VPC to initiate outbound connections to the internet but prevents unsolicited inbound connections from the internet, we can use a Network Address Translation (NAT) service for IPv4 traffic. NAT maps multiple private IPv4 addresses to a single public IPv4 address, and does IP translation so host in the internet sees only public IP, original IP @ is in the header. In the private subnet, outbound to reach the internet has to go to the NAT Gateway, while traffic from machines in the subnets stays local, as illustrated in the following routing table: Figure 9: Route to NAT Gateway To use a Bastion Host , we attach a security group, to the EC2, to authorize inbound SSH and outbound HTTP traffic. CDK creates this SG automatically. So we can use Instance Connect to this instance, and within the terminal a ping to amazon.com will work. The bastion has a public IP address, and the VPC has a IGW with a route table. Figure 10: Security group for bastion host authorize public access to port 22 In the EC2 instance running in the private network, we need to add a Security Group with an inbound rule to specify SSH traffic from the SG of the Bastion. With this settings a SSH to the Bastion, then a copy of the pem file in the bastion host and a command like: ssh ec2-user@10.10.2.239 -i ec2.pem on the private IP @ of the EC2 instance (10.10.2.239) will make the connection from Bastion to EC2. Figure 11: Security group for EC2 to accept port 22 from Bastion only IPv6 uses Egress only Internet Gateway for outbound requests from a private Subnet. For IPv4 oubtound internet traffic from a private subnet, we use a NAT instance or NAT Gateway. NAT gateway is deployed inside a subnet and it can scale only inside that subnet. For fault tolerance, it is recommended that we deploy one NAT gateway per availability zone. As we have set up a NAT Gateway in each public subnet, and the route in the private network route all IP to the NAT gateway, we can ping from the EC2 running in the private subnet to the internet: Figure 12: Route to NAT to reach internet Here is the two NAT gateways, one in each subnet/AZ: Figure 13: Two NAT gateways for HA A NAT device has an Elastic IP address and is connected to the internet through an internet gateway. Deeper Dive \u00b6 VPC FAQs . NAT gateway . Elastic Network Insterfaces \u00b6 ENI is a logical component in a VPC that represents a virtual network card. It has the following attributes: One primary private IPv4, one or more secondary IPv4. One Elastic IP (IPv4) per private IPv4. One Public IPv4. One or more security groups. A MAC address. We can create ENI independently and attach them on the fly (move them) on EC2 instances for failover purpose. Bound to a specific availability zone (AZ), We cannot attach ENI to an EC2 instance in a different AZ. New ENI doc. Bastion Host \u00b6 The goal is to be able to access any EC2 instances running in the private subnets from outside of the VPC, using SSH. The bastion is running on public subnet, and then connected to the private subnets. The security group for the Bastion Host authorizies inbound on port 22 from restricted public CIDR. Security group of the EC2 instance allows the SG of the bastion host to accept connection on port 22. NAT Gateway \u00b6 Use a NAT gateway so that instances in a private subnet can connect to services outside our VPC but external services cannot initiate a connection with those instances. Charged for each hour that our NAT gateway is available and each Gigabyte of data that it processes. It is created in a specified AZ and uses an Elastic IP and can only be used by EC2 in other subnets. The route is from the private subnet to the NATGW to the IGW. To get HA we need one NATG per AZ. The bandwidth is from 5 Gbps to automatic scale up 45Gbps. Network ACLs \u00b6 Defines traffic rule at the subnet level. One NACL per subnet. A NACL specifies rules with number that defines evaluation priority. The last rule is an asterisk and denies a request in case of no rule conditions match. As soon as a rule matches traffic, it\u2019s applied immediately regardless of any higher-numbered rule that may contradict it Figure 14: Default NACL It is used to block a specific external IP address. The VPC automatically comes with a modifiable default network ACL. By default, it allows all inbound and outbound IPv4 traffic and, if applicable, IPv6 traffic. We can create a custom network ACL and associate it with a subnet. By default, each custom network ACL denies all inbound and outbound traffic until we add rules. Below is a complete figure to explain the process: A web server is initiating a connection to a DB on port 3306 and get response to an ephemeral port (allocated from 1024 to 65535): Figure 15: Connection flow between public to private Apps Each subnet in your VPC must be associated with a network ACL. If you don't explicitly associate a subnet with a network ACL, the subnet is automatically associated with the default network ACL. Security Group Network ACL Acts as firewall for EC2 Acts as firewall for subnet Controls inbound & outbound traffic at instance level Controls inbound & outbound traffic at subnet level Supports allow rules Supports deny and allow rules Evaluates all rules before deciding to allow traffic Evaluate rules in order Instances associated with a SG can't talk to each other unless we add a rule to allow it Each subnet must be associated with a NACL. VPC peering \u00b6 The goal of VPC peering is to connect two VPCs using AWS network and let them behave as if they were in the same network. When defining the VPC, the CICDs should not overlap. It could be used to connect VPC cross Regions and even cross Accounts. Once the VPC peering connection is defined, we still need to specify the route to the CIDR to reach the VPC peering created. VPC sharing (part of Resource Access Manager) allows multiple AWS accounts to create their application resources such as EC2 instances, RDS databases, Redshift clusters, and Lambda functions, into shared and centrally-managed Amazon Virtual Private Clouds (VPCs). To set this up, the account that owns the VPC (owner) shares one or more subnets with other accounts (participants) that belong to the same organization from AWS Organizations. After a subnet is shared, the participants can view, create, modify, and delete their application resources in the subnets shared with them. Participants cannot view, modify, or delete resources that belong to other participants or to the VPC owner. VPC Endpoint \u00b6 An interface VPC endpoint allows us to privately connect our Amazon VPC to supported AWS services. Interface VPC endpoints also connect to endpoint services hosted by other AWS customers and partners and AWS Marketplace partner services. VPC Endpoints remove the need of IGW, NATGW to access AWS Services. The service is redundant and scale horizontally. Two types of endpoint: Interface endpoints powered by PrivateLink: it provisions an ENI in our VPC, with a security group. Pay per hour and GB of data transferred. It can access AWS services such as Simple Queue Service (SQS), Simple Notification Service (SNS), Amazon Kinesis. Gateway endpoints : provision a GTW and setup routes in route tables. Used for S3 and DynamoDB only. Free. VPC Flow Logs \u00b6 Capture IP traffic at the ENI, VPC, subnet level. This is used to monitor and troubleshoot connectivity issues. Can be save in S3 and CloudWatch logs. It can work on managed service like ELB, RDS, ElastiCache, RedShift, NATGW, Transit Gateway. The log includes src and destination addresses, port number and action done (REJECT/ ACCEPT). When an inboud request is rejected it could be a NACL or SG issue, while if the inbound is accepted but outbound reject it is only a NACL issue. VPC Flog log is defined within the VPC: Figure 16: VPC flow logs The Flow can target S3, Firehouse or CloudWatch Figure 17: VPC flow definition to CloudWatch Extended picture \u00b6 The following animation is presenting external integration from on-premises servers to AWS services and VPCs via site to site VPN, Private Gateway, Customer Gateway. Figure 18: Full VPC diagram We need to have VPC endpoint service to access the AWS services , like S3, privately as they are in our VPC. We need to ensure there is one interface endpoint for each availability zone. We need to pick a subnet in each AZ and add an interface endpoint to that subnet. TCP traffic is isolated. It is part of a larger offering called AWS PrivateLink to establish private connectivity between VPCs and services hosted on AWS or on-premises, without exposing data to the internet (No internet gateway, no NAT, no public IP @). CIDR Blocks should not overlap between VPCs for setting up a peering connection. Peering connection is allowed within a region, across regions, across different accounts. We can optionally connect our VPC to our own corporate data center using an IPsec AWS managed VPN connection, making the AWS Cloud an extension of our data center. A VPN connection consists of a virtual private gateway (VGW) attached to our VPC and a customer gateway located in the data center. A Virtual Private Gateway is the VPN concentrator on the Amazon side of the VPN connection. A customer gateway is a physical device or software appliance on the on-premise side of the VPN connection. We need to create a Site-to-site VPN connection between the CP GTW and Customer GTW. As seen in Figure 18 \"Full VPC diagram\", the VPC peering helps to connect between VPCs in different region, or within the same region. And Transit GTW is used to interconnect our virtual private clouds (VPCs) and on-premises networks. In fact Transit Gateway is a more modern and easier approach to link VPCs. Using Transit Gateway route tables, We can control the traffic flow between VPCs. The peering connection would work; however, it requires a lot of point-to-point connections. If we have multiple AWS Site-to-Site VPN connections, we can provide secure communication between sites using the AWS VPN CloudHub . This enables our remote sites to communicate with each other, and not just with the VPC. AWS VPN CloudHub operates on a simple hub-and-spoke model that we can use with or without a VPC. Direct Connect \u00b6 It provides a dedicated connection from a remote network to the VPC bypassing public internet. Need to setup a Virtual Private GTW. It is a private connection so it supports better bandwidth throughput at lower cost. It is not encrypted by default. Figure 19: Direct Connect between on-premise and VPC Get a direct connection setup can take more than a month. If we need to access two VPCs in two different regions from the corporate data center then we need a Direct Connect Gateway. To get reliability we can setup a VPN site to site connection in parallel to the Direct Connect link. For maximum resiliency for critical workloads, it is recommended to have double connections per data center. Figure 20: Direct Connect HA Hosted Direct Connect connection supports 50Mbps, 500Mbps, up to 10Gbps, while Dedicated offers higher bandwidth. The main pricing parameter while using the Direct Connect connection is the Data Transfer Out (DTO) from AWS to the on-premises data center. DTO refers to the cumulative network traffic that is sent through AWS Direct Connect to destinations outside of AWS. This is charged per gigabyte (GB), and unlike capacity measurements, DTO refers to the amount of data transferred, not the speed. Transit Gateway \u00b6 This service aims to reduce complexity of interconnecting VPCs, it provides transitive peering between thousands of VPCs and on-premises, hub and spoke connection. It runs on the AWS global private network. Transit Gateway acts as a highly scalable cloud router\u2014each new connection is made only once. It is a regional resource but it can run cross-region. It is also possible to share it across accounts using Resource Access Manager. Control the connection vias Route Tables. This is also the only service supporting IP multicast. To increase the bandwidth of the connection to VPC, there is the site to site VPN ECMP (Equal-cost multi-path routing) feature, which is a routing strategy to forward packet over multiple best paths. The VPCs can be from different accounts. Figure 21: Transit Gateway between VPCs and On-premises VPN connection to the Transit gateway with ECMP double the troughtput as it used double tunnels. Elastic Load balancers \u00b6 Route traffic into the different EC2 instances. Elastic Load Balancing scales our load balancer capacity automatically in response to changes in incoming traffic. It is a managed service! And deployed per region. It also exposes a single point of access (DNS) to the deployed applications. In case of EC2 failure, it can route to a new instance, transparently and across multiple AZs. It uses health check (/health on the app called the ping path ) to assess instance availability. It also provides SSL termination. It is used to separate private (internal) to public (external) traffic. Figure 22: ELB Need to enable availability zone to be able to route traffic between target groups in different AZs. When We create a load balancer, we must choose whether to make it an internal load balancer or an internet-facing load balancer. Internet-facing load balancers have public IP addresses. The DNS name of an internet-facing load balancer is publicly resolvable to the public IP addresses of the nodes. Internal load balancers have only private IP addresses. Internal load balancers can only route requests from clients with access to the VPC of the load balancer. Figure 23: Public and private ELBs For certain needs, it also support stickness cookie to route to the same EC2 instance. ELB has security group defined for HTTP and HTTPS traffic coming from the internet, and the EC2 security group defines HTTP traffic to the ELB only. Four types of ELB supported: Classic load balancer: older generation. TCP and HTTP layer. For each instance created, update the load balancer configuration so it can route the traffic. Application load balancer : HTTP, HTTPS (layer 7), Web Socket. It specifies availability zones: it routes traffic to the targets in these Availability Zones. Each AZ has one subnet. To increase availability, we need at least two AZs. It uses target groups, to group applications. So it can target containers running in one EC2 instance. route on URL, hostname and query string. Get a fixed hostname in DNS. the application do not see the IP address of the client directly (ELB does a connection termination), but ELB puts client information in the header X-Forwarded-For (IP @), X-Forwarded-Port (port #) and X-Forwarded-Proto (protocol). Great for microservices or for container based apps (ECS). Support dynamic port mapping for ECS container. Support HTTP/2 and WebSocket. Info Target group: group EC2 instances by specifying Auto Scaling Group, but they also be task or containers in ECS, or lambda function. Health check is done at the target group level. Network load balancer : TCP, UDP (layer 4), TLS Handle millions request/s. Reach less than 100ms latency while ALB is at 400ms. Use to get one public static IP address per availability zone. Routes each individual TCP connection to a single target for the life of the connection. If you specify targets using an instance ID, traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance. If you specify targets using IP addresses, you can route traffic to an instance using any private IP address from one or more network interfaces. This enables multiple applications on an instance to use the same port Not free. Can reach target groups of EC2s, IP @, could be ALB. Health check is based on TCP, HTTP, and HTTPS. For NLB we need to add a rule in a the security group attached to the EC2 to get HTTP:80 to anywhere. Gateway LB : Used to analyze in traffic before routing to applications. Applies firewalls rules, intrusion detection, deep packet inspection. Works at layer 3: IP packet. Combine NLB and gateway service. Also use target groups. Use the Geneve protocol (support network virtualization use cases for data center ) on port 6081 To route traffic, first the DNS name of the load balancer is resolved. (They are part of the amazaonaws.com domain). 1 to many IP Addresses are sent back to the client. With NLBs, Elastic Load Balancing creates a network interface for each Availability Zone that we enable. Each load balancer node in the Availability Zone uses this network interface to get a static IP address. ELB scales our load balancer and updates the DNS entry. The time to live is set to 60s. To control that only the load balancer is sending traffic to the application, we need to set up an application security group on HTTP, and HTTPS with the source being the security group id of the ELB. LBs can scale but need to engage AWS operational team. HTTP 503 means LB is at capacity or target app is not registered. Verify security group in case of no communication between LB and app. Target group defines protocol to use, health check checking and what applications to reach (instance, IP or lambda). Below is an example of listener rule for an ALB: Figure 23: ALB rule ALB and Classic can use HTTP connection multiplexing to keep one connection with the backend application. Connection multiplexing improves latency and reduces the load on our applications. Load balancer stickiness \u00b6 Used when the same client needs to interact with the same backend instance. A cookie, with expiration date, is used to identify the client. The classical LB or ALB manages the routing. This could lead to unbalance traffic so overloading one instance. With ALB, stickness is configured in the target group properties. Two types of cookie: Application-based cookie : generated by the target app. The cookie name is specific to the target group. Duration-based cookie : generated by the Load Balancer. The following cookie names are reserved by the ELB (AWSALB, AWSALBAPP, AWSALBTG). Cross Zone Load Balancing \u00b6 Each load balancer instance distributes traffic evenly across all registered instances in all availability zones. If one AZ has 2 targets and another one has 8 targets, then with cross-zone, the LBs in each availability zone will route to any instance, so each will receive 10% of the traffic. Without that, the 2 targets zone will receive 25% traffic each, and the instance on the othe AZ only 6.25% of the traffic. This is the default setting for ALB and free of charge. It is disabled by default for NLB. TLS - Transport Layer Security, \u00b6 An SSL/TLS Certificate allows traffic between clients and load balancer to be encrypted in transit (in-flight encryption). Load balancer uses an X.509 certificate (SSL/TLS server certificate). Manage our own certificates using ACM (AWS Certificate Manager). When defining a HTTPS listener in a LB, we must specify a default certificate for the HTTPS protocol, while defining the routing rule to a given target group. Need multiple certs to support multiple domains. Clients can use SNI (Server Name Indication) to specify the hostname they want to reach. The ALB or NLB will get the certificates for each host to support the TLS handshake. Connection draining \u00b6 This is a setting to control connection timeout and reconnect when an instance is not responding. It is to set up the time to complete \u201cin-flight requests\u201d. When an instance is \"draining\", ELB stops sending new requests to the instance. The time out can be adjusted, depending of the application, from 1 to 3600 seconds, default is 300 seconds, or disabled (set value to 0). It is called Deregistration Delay in NLB & ALB. Global Accelerator \u00b6 AWS Global Accelerator is a network layer service that directs traffic to optimal endpoints over the AWS global network, this improves the availability and performance of Iour internet applications. It provides two static anycast IP addresses that act as a fixed entry point to our application endpoints in a single or multiple AWS Regions, such as our Application Load Balancers, Network Load Balancers, Elastic IP addresses or Amazon EC2 instances, in a single or in multiple AWS regions. The goal is to expose quickly an application to the WW. The problem is the number of internet hops done to access the target public ALB. The solution is to get as fast as possible to a AWS global network endpoint (Edge location), nearest region to the client. It is a global service. With Anycast IP a client is routed to the nearest server. All servers hold the same IP address. So for each application, we create 2 Anycast IP, and the traffic is sent to the edge locations. AWS Global Accelerator uses endpoint weights to determine the proportion of traffic that is directed to endpoints in an endpoint group, and traffic dials to control the percentage of traffic that is directed to an endpoint group Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP. VPN CloudHub \u00b6 If we have multiple AWS Site-to-Site VPN connections, we can provide secure communication between sites using the AWS VPN CloudHub. This enables our remote sites to communicate with each other, and not just with the VPC. Sites that use AWS Direct Connect connections to the virtual private gateway can also be part of the AWS VPN CloudHub. The VPN CloudHub operates on a simple hub-and-spoke model that we can use with or without a VPC. This design is suitable if we have multiple branch offices and existing internet connections and would like to implement a convenient, potentially low-cost hub-and-spoke model for primary or backup connectivity between these remote offices. FAQs \u00b6 What differentiates a public subnet from a private subnet? A public subnet is a subnet that's associated with a route table that has a route to an internet gateway","title":"Network"},{"location":"infra/networking/#networking","text":"All regions are interconnected via private AWS fiber links. This drives better availability, higher performance, lower jitter and reduced costs. Each region has redundant path to transit centers, which connect to private links to other AWS regions, and to AWS Direct Connect customers' data centers, internet via peering and paid transit. The connections between AZs is a metro area over DWDM (Dense wavelength division multiplexing) links. 82k fibers in a region. single digit milisecond latency. 25Tbps peak inter AZs traffic. IPv4 allows 3.7 billions of different addresses. Private IP @ is for private network connections. Internet gateway has public and private connections. Public IP can be geo-located. When connected to an EC2 the prompt lists the private IP ( ec2-user@ip-172-31-18-48 ). Private IP stays stable on instance restart, while public may change. With Elastic IP address, we can mask an EC2 instance failure by rapidly remapping the address to another instance. But better to use DNS. Elastic IP is a public IPv4 that we own as long as we want and we can attach it to one EC2 instance at a time. It is not free. In the context of High Performance Computing (HPC) and ML applications, or OS-bypass, we can use EFA.","title":"Networking"},{"location":"infra/networking/#virtual-private-cloud","text":"A virtual private cloud (VPC) is a virtual network dedicated to our AWS account. All new accounts have a default VPC. It is logically isolated from other virtual networks in the AWS Cloud. We can launch our AWS resources, such as Amazon EC2 instances, within our VPC. New EC2 instances are launched into the default VPC if no subnet is specified. When defining new VPC, we can specify CIDR, add subnets, associate security groups, ACL, and configure route tables. Figure 4: VPC By default, AWS creates a VPC with default public subnets, one per AZs, they are public because the main route table sends the subnet's traffic that is destined for the internet to the internet gateway. VPC Helps to: Assign static IP addresses, potentially multiple addresses for the same EC2 instance. Change security group membership for our instances while they're running. Control the outbound traffic from our instances (egress filtering) in addition to controlling the inbound traffic to them (ingress filtering). We can have multiple VPCs per region (max to 5 but this is a soft limit). 5 maximum CIDR per VPC. The IP range is min /28 and max /16. Default VPC includes an Internet Gateway . Internet gateway is a managed service and automatically scales, is redundant and highly available. Network Access Control List can be defined at the VPC level, so will be shared between subnets. The default network ACL is configured to allow all traffic to flow in and out of the subnets which it is associated. Each network ACL also includes a rule whose rule number is an asterisk. This rule ensures that if a packet doesn't match any of the other numbered rules, it's denied. The following diagram illustrates classical VPC, as defined years ago, with one vpc, two availability zones, two public subnets with EC2 instances within those subnets and two AZs. An internet gateway connected to a router. Subnet is defined within a VPC and in one availability zone. It defines an IP CIDR range: we should have less IP on public subnet as they are used for ELB and very few resources. Most of the services should be in private subnet. Figure 5: EC2s in public subnets A subnet is assigned a /24 CIDR block, which means 8 bits encoding (32-24), but AWS uses 5 IP addresses in each subnet for gateway, LB,... so the number of available addresses is 256 - 5 = 251. To identify a single 32 bit IPv4 address, we can use /32 CIDR convention Internet Gateway needs to be defined to any newly created VPC to get internet hosts being able to access EC2s. Route Tables need to be defined: one public associated with the public subnets and one private associated with private subnets. Non-default subnet has a private IPv4 address, but no public IPv4. We can make a default subnet into a private subnet by removing the route from the destination 0.0.0.0/0 to the Internet Gateway. The table in figure above defines such route, therefore the subnets are public. EC2 Instances should have either public IP or elastic IP and the subnet they belong to, must have a route to the internet gateway. The figure 5 above illustrates a route coming from any IP @ (0.0.0.0/0) goes to the internet gateway ( igw-id ). Any host in the private network 172.31.0.0/16 can communicate with other hosts in the local network. Route tables defines 172.31 as local with /20 CIDR address range, internal to the VPC. Default route to internet goes to the IGW, which has an elastic IP address assigned to it. Because the VPC is cross AZs, we need a router to route between subnets. (See TCP/IP summary )","title":"Virtual Private Cloud"},{"location":"infra/networking/#routing-tables","text":"As illustrated in the following diagram, the main routing table addresses internal to the VPC traffic, while custom tables define how inbound and outbound traffic can be structured within a subnet. Figure 6: Routing tables EC2 instances in subnet 2 are in private subnet. The corporate network, or on-premises network, can be connected to the VPC via customer gateway deployed on-premises, vitual private GTW in AWS region, and VPN connection between the two gateways.","title":"Routing Tables"},{"location":"infra/networking/#hands-on-work","text":"The CDK example in the ec2-vpc folder supports the following definitions: Figure 7: More classical VPC We can enable internet access for an EC2 instance launched into a non-default subnet by attaching an internet gateway to its VPC and configure routing tables for each subnets. The routes needs to define that traffic from internet goes to the IGW. The following route is associated to the public subnet-1: Figure 8: Route to Internet Gateway Alternatively, to allow an instance in our VPC to initiate outbound connections to the internet but prevents unsolicited inbound connections from the internet, we can use a Network Address Translation (NAT) service for IPv4 traffic. NAT maps multiple private IPv4 addresses to a single public IPv4 address, and does IP translation so host in the internet sees only public IP, original IP @ is in the header. In the private subnet, outbound to reach the internet has to go to the NAT Gateway, while traffic from machines in the subnets stays local, as illustrated in the following routing table: Figure 9: Route to NAT Gateway To use a Bastion Host , we attach a security group, to the EC2, to authorize inbound SSH and outbound HTTP traffic. CDK creates this SG automatically. So we can use Instance Connect to this instance, and within the terminal a ping to amazon.com will work. The bastion has a public IP address, and the VPC has a IGW with a route table. Figure 10: Security group for bastion host authorize public access to port 22 In the EC2 instance running in the private network, we need to add a Security Group with an inbound rule to specify SSH traffic from the SG of the Bastion. With this settings a SSH to the Bastion, then a copy of the pem file in the bastion host and a command like: ssh ec2-user@10.10.2.239 -i ec2.pem on the private IP @ of the EC2 instance (10.10.2.239) will make the connection from Bastion to EC2. Figure 11: Security group for EC2 to accept port 22 from Bastion only IPv6 uses Egress only Internet Gateway for outbound requests from a private Subnet. For IPv4 oubtound internet traffic from a private subnet, we use a NAT instance or NAT Gateway. NAT gateway is deployed inside a subnet and it can scale only inside that subnet. For fault tolerance, it is recommended that we deploy one NAT gateway per availability zone. As we have set up a NAT Gateway in each public subnet, and the route in the private network route all IP to the NAT gateway, we can ping from the EC2 running in the private subnet to the internet: Figure 12: Route to NAT to reach internet Here is the two NAT gateways, one in each subnet/AZ: Figure 13: Two NAT gateways for HA A NAT device has an Elastic IP address and is connected to the internet through an internet gateway.","title":"Hands-on work"},{"location":"infra/networking/#deeper-dive","text":"VPC FAQs . NAT gateway .","title":"Deeper Dive"},{"location":"infra/networking/#elastic-network-insterfaces","text":"ENI is a logical component in a VPC that represents a virtual network card. It has the following attributes: One primary private IPv4, one or more secondary IPv4. One Elastic IP (IPv4) per private IPv4. One Public IPv4. One or more security groups. A MAC address. We can create ENI independently and attach them on the fly (move them) on EC2 instances for failover purpose. Bound to a specific availability zone (AZ), We cannot attach ENI to an EC2 instance in a different AZ. New ENI doc.","title":"Elastic Network Insterfaces"},{"location":"infra/networking/#bastion-host","text":"The goal is to be able to access any EC2 instances running in the private subnets from outside of the VPC, using SSH. The bastion is running on public subnet, and then connected to the private subnets. The security group for the Bastion Host authorizies inbound on port 22 from restricted public CIDR. Security group of the EC2 instance allows the SG of the bastion host to accept connection on port 22.","title":"Bastion Host"},{"location":"infra/networking/#nat-gateway","text":"Use a NAT gateway so that instances in a private subnet can connect to services outside our VPC but external services cannot initiate a connection with those instances. Charged for each hour that our NAT gateway is available and each Gigabyte of data that it processes. It is created in a specified AZ and uses an Elastic IP and can only be used by EC2 in other subnets. The route is from the private subnet to the NATGW to the IGW. To get HA we need one NATG per AZ. The bandwidth is from 5 Gbps to automatic scale up 45Gbps.","title":"NAT Gateway"},{"location":"infra/networking/#network-acls","text":"Defines traffic rule at the subnet level. One NACL per subnet. A NACL specifies rules with number that defines evaluation priority. The last rule is an asterisk and denies a request in case of no rule conditions match. As soon as a rule matches traffic, it\u2019s applied immediately regardless of any higher-numbered rule that may contradict it Figure 14: Default NACL It is used to block a specific external IP address. The VPC automatically comes with a modifiable default network ACL. By default, it allows all inbound and outbound IPv4 traffic and, if applicable, IPv6 traffic. We can create a custom network ACL and associate it with a subnet. By default, each custom network ACL denies all inbound and outbound traffic until we add rules. Below is a complete figure to explain the process: A web server is initiating a connection to a DB on port 3306 and get response to an ephemeral port (allocated from 1024 to 65535): Figure 15: Connection flow between public to private Apps Each subnet in your VPC must be associated with a network ACL. If you don't explicitly associate a subnet with a network ACL, the subnet is automatically associated with the default network ACL. Security Group Network ACL Acts as firewall for EC2 Acts as firewall for subnet Controls inbound & outbound traffic at instance level Controls inbound & outbound traffic at subnet level Supports allow rules Supports deny and allow rules Evaluates all rules before deciding to allow traffic Evaluate rules in order Instances associated with a SG can't talk to each other unless we add a rule to allow it Each subnet must be associated with a NACL.","title":"Network ACLs"},{"location":"infra/networking/#vpc-peering","text":"The goal of VPC peering is to connect two VPCs using AWS network and let them behave as if they were in the same network. When defining the VPC, the CICDs should not overlap. It could be used to connect VPC cross Regions and even cross Accounts. Once the VPC peering connection is defined, we still need to specify the route to the CIDR to reach the VPC peering created. VPC sharing (part of Resource Access Manager) allows multiple AWS accounts to create their application resources such as EC2 instances, RDS databases, Redshift clusters, and Lambda functions, into shared and centrally-managed Amazon Virtual Private Clouds (VPCs). To set this up, the account that owns the VPC (owner) shares one or more subnets with other accounts (participants) that belong to the same organization from AWS Organizations. After a subnet is shared, the participants can view, create, modify, and delete their application resources in the subnets shared with them. Participants cannot view, modify, or delete resources that belong to other participants or to the VPC owner.","title":"VPC peering"},{"location":"infra/networking/#vpc-endpoint","text":"An interface VPC endpoint allows us to privately connect our Amazon VPC to supported AWS services. Interface VPC endpoints also connect to endpoint services hosted by other AWS customers and partners and AWS Marketplace partner services. VPC Endpoints remove the need of IGW, NATGW to access AWS Services. The service is redundant and scale horizontally. Two types of endpoint: Interface endpoints powered by PrivateLink: it provisions an ENI in our VPC, with a security group. Pay per hour and GB of data transferred. It can access AWS services such as Simple Queue Service (SQS), Simple Notification Service (SNS), Amazon Kinesis. Gateway endpoints : provision a GTW and setup routes in route tables. Used for S3 and DynamoDB only. Free.","title":"VPC Endpoint"},{"location":"infra/networking/#vpc-flow-logs","text":"Capture IP traffic at the ENI, VPC, subnet level. This is used to monitor and troubleshoot connectivity issues. Can be save in S3 and CloudWatch logs. It can work on managed service like ELB, RDS, ElastiCache, RedShift, NATGW, Transit Gateway. The log includes src and destination addresses, port number and action done (REJECT/ ACCEPT). When an inboud request is rejected it could be a NACL or SG issue, while if the inbound is accepted but outbound reject it is only a NACL issue. VPC Flog log is defined within the VPC: Figure 16: VPC flow logs The Flow can target S3, Firehouse or CloudWatch Figure 17: VPC flow definition to CloudWatch","title":"VPC Flow Logs"},{"location":"infra/networking/#extended-picture","text":"The following animation is presenting external integration from on-premises servers to AWS services and VPCs via site to site VPN, Private Gateway, Customer Gateway. Figure 18: Full VPC diagram We need to have VPC endpoint service to access the AWS services , like S3, privately as they are in our VPC. We need to ensure there is one interface endpoint for each availability zone. We need to pick a subnet in each AZ and add an interface endpoint to that subnet. TCP traffic is isolated. It is part of a larger offering called AWS PrivateLink to establish private connectivity between VPCs and services hosted on AWS or on-premises, without exposing data to the internet (No internet gateway, no NAT, no public IP @). CIDR Blocks should not overlap between VPCs for setting up a peering connection. Peering connection is allowed within a region, across regions, across different accounts. We can optionally connect our VPC to our own corporate data center using an IPsec AWS managed VPN connection, making the AWS Cloud an extension of our data center. A VPN connection consists of a virtual private gateway (VGW) attached to our VPC and a customer gateway located in the data center. A Virtual Private Gateway is the VPN concentrator on the Amazon side of the VPN connection. A customer gateway is a physical device or software appliance on the on-premise side of the VPN connection. We need to create a Site-to-site VPN connection between the CP GTW and Customer GTW. As seen in Figure 18 \"Full VPC diagram\", the VPC peering helps to connect between VPCs in different region, or within the same region. And Transit GTW is used to interconnect our virtual private clouds (VPCs) and on-premises networks. In fact Transit Gateway is a more modern and easier approach to link VPCs. Using Transit Gateway route tables, We can control the traffic flow between VPCs. The peering connection would work; however, it requires a lot of point-to-point connections. If we have multiple AWS Site-to-Site VPN connections, we can provide secure communication between sites using the AWS VPN CloudHub . This enables our remote sites to communicate with each other, and not just with the VPC. AWS VPN CloudHub operates on a simple hub-and-spoke model that we can use with or without a VPC.","title":"Extended picture"},{"location":"infra/networking/#direct-connect","text":"It provides a dedicated connection from a remote network to the VPC bypassing public internet. Need to setup a Virtual Private GTW. It is a private connection so it supports better bandwidth throughput at lower cost. It is not encrypted by default. Figure 19: Direct Connect between on-premise and VPC Get a direct connection setup can take more than a month. If we need to access two VPCs in two different regions from the corporate data center then we need a Direct Connect Gateway. To get reliability we can setup a VPN site to site connection in parallel to the Direct Connect link. For maximum resiliency for critical workloads, it is recommended to have double connections per data center. Figure 20: Direct Connect HA Hosted Direct Connect connection supports 50Mbps, 500Mbps, up to 10Gbps, while Dedicated offers higher bandwidth. The main pricing parameter while using the Direct Connect connection is the Data Transfer Out (DTO) from AWS to the on-premises data center. DTO refers to the cumulative network traffic that is sent through AWS Direct Connect to destinations outside of AWS. This is charged per gigabyte (GB), and unlike capacity measurements, DTO refers to the amount of data transferred, not the speed.","title":"Direct Connect"},{"location":"infra/networking/#transit-gateway","text":"This service aims to reduce complexity of interconnecting VPCs, it provides transitive peering between thousands of VPCs and on-premises, hub and spoke connection. It runs on the AWS global private network. Transit Gateway acts as a highly scalable cloud router\u2014each new connection is made only once. It is a regional resource but it can run cross-region. It is also possible to share it across accounts using Resource Access Manager. Control the connection vias Route Tables. This is also the only service supporting IP multicast. To increase the bandwidth of the connection to VPC, there is the site to site VPN ECMP (Equal-cost multi-path routing) feature, which is a routing strategy to forward packet over multiple best paths. The VPCs can be from different accounts. Figure 21: Transit Gateway between VPCs and On-premises VPN connection to the Transit gateway with ECMP double the troughtput as it used double tunnels.","title":"Transit Gateway"},{"location":"infra/networking/#elastic-load-balancers","text":"Route traffic into the different EC2 instances. Elastic Load Balancing scales our load balancer capacity automatically in response to changes in incoming traffic. It is a managed service! And deployed per region. It also exposes a single point of access (DNS) to the deployed applications. In case of EC2 failure, it can route to a new instance, transparently and across multiple AZs. It uses health check (/health on the app called the ping path ) to assess instance availability. It also provides SSL termination. It is used to separate private (internal) to public (external) traffic. Figure 22: ELB Need to enable availability zone to be able to route traffic between target groups in different AZs. When We create a load balancer, we must choose whether to make it an internal load balancer or an internet-facing load balancer. Internet-facing load balancers have public IP addresses. The DNS name of an internet-facing load balancer is publicly resolvable to the public IP addresses of the nodes. Internal load balancers have only private IP addresses. Internal load balancers can only route requests from clients with access to the VPC of the load balancer. Figure 23: Public and private ELBs For certain needs, it also support stickness cookie to route to the same EC2 instance. ELB has security group defined for HTTP and HTTPS traffic coming from the internet, and the EC2 security group defines HTTP traffic to the ELB only. Four types of ELB supported: Classic load balancer: older generation. TCP and HTTP layer. For each instance created, update the load balancer configuration so it can route the traffic. Application load balancer : HTTP, HTTPS (layer 7), Web Socket. It specifies availability zones: it routes traffic to the targets in these Availability Zones. Each AZ has one subnet. To increase availability, we need at least two AZs. It uses target groups, to group applications. So it can target containers running in one EC2 instance. route on URL, hostname and query string. Get a fixed hostname in DNS. the application do not see the IP address of the client directly (ELB does a connection termination), but ELB puts client information in the header X-Forwarded-For (IP @), X-Forwarded-Port (port #) and X-Forwarded-Proto (protocol). Great for microservices or for container based apps (ECS). Support dynamic port mapping for ECS container. Support HTTP/2 and WebSocket. Info Target group: group EC2 instances by specifying Auto Scaling Group, but they also be task or containers in ECS, or lambda function. Health check is done at the target group level. Network load balancer : TCP, UDP (layer 4), TLS Handle millions request/s. Reach less than 100ms latency while ALB is at 400ms. Use to get one public static IP address per availability zone. Routes each individual TCP connection to a single target for the life of the connection. If you specify targets using an instance ID, traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance. If you specify targets using IP addresses, you can route traffic to an instance using any private IP address from one or more network interfaces. This enables multiple applications on an instance to use the same port Not free. Can reach target groups of EC2s, IP @, could be ALB. Health check is based on TCP, HTTP, and HTTPS. For NLB we need to add a rule in a the security group attached to the EC2 to get HTTP:80 to anywhere. Gateway LB : Used to analyze in traffic before routing to applications. Applies firewalls rules, intrusion detection, deep packet inspection. Works at layer 3: IP packet. Combine NLB and gateway service. Also use target groups. Use the Geneve protocol (support network virtualization use cases for data center ) on port 6081 To route traffic, first the DNS name of the load balancer is resolved. (They are part of the amazaonaws.com domain). 1 to many IP Addresses are sent back to the client. With NLBs, Elastic Load Balancing creates a network interface for each Availability Zone that we enable. Each load balancer node in the Availability Zone uses this network interface to get a static IP address. ELB scales our load balancer and updates the DNS entry. The time to live is set to 60s. To control that only the load balancer is sending traffic to the application, we need to set up an application security group on HTTP, and HTTPS with the source being the security group id of the ELB. LBs can scale but need to engage AWS operational team. HTTP 503 means LB is at capacity or target app is not registered. Verify security group in case of no communication between LB and app. Target group defines protocol to use, health check checking and what applications to reach (instance, IP or lambda). Below is an example of listener rule for an ALB: Figure 23: ALB rule ALB and Classic can use HTTP connection multiplexing to keep one connection with the backend application. Connection multiplexing improves latency and reduces the load on our applications.","title":"Elastic Load balancers"},{"location":"infra/networking/#load-balancer-stickiness","text":"Used when the same client needs to interact with the same backend instance. A cookie, with expiration date, is used to identify the client. The classical LB or ALB manages the routing. This could lead to unbalance traffic so overloading one instance. With ALB, stickness is configured in the target group properties. Two types of cookie: Application-based cookie : generated by the target app. The cookie name is specific to the target group. Duration-based cookie : generated by the Load Balancer. The following cookie names are reserved by the ELB (AWSALB, AWSALBAPP, AWSALBTG).","title":"Load balancer stickiness"},{"location":"infra/networking/#cross-zone-load-balancing","text":"Each load balancer instance distributes traffic evenly across all registered instances in all availability zones. If one AZ has 2 targets and another one has 8 targets, then with cross-zone, the LBs in each availability zone will route to any instance, so each will receive 10% of the traffic. Without that, the 2 targets zone will receive 25% traffic each, and the instance on the othe AZ only 6.25% of the traffic. This is the default setting for ALB and free of charge. It is disabled by default for NLB.","title":"Cross Zone Load Balancing"},{"location":"infra/networking/#tls-transport-layer-security","text":"An SSL/TLS Certificate allows traffic between clients and load balancer to be encrypted in transit (in-flight encryption). Load balancer uses an X.509 certificate (SSL/TLS server certificate). Manage our own certificates using ACM (AWS Certificate Manager). When defining a HTTPS listener in a LB, we must specify a default certificate for the HTTPS protocol, while defining the routing rule to a given target group. Need multiple certs to support multiple domains. Clients can use SNI (Server Name Indication) to specify the hostname they want to reach. The ALB or NLB will get the certificates for each host to support the TLS handshake.","title":"TLS - Transport Layer Security,"},{"location":"infra/networking/#connection-draining","text":"This is a setting to control connection timeout and reconnect when an instance is not responding. It is to set up the time to complete \u201cin-flight requests\u201d. When an instance is \"draining\", ELB stops sending new requests to the instance. The time out can be adjusted, depending of the application, from 1 to 3600 seconds, default is 300 seconds, or disabled (set value to 0). It is called Deregistration Delay in NLB & ALB.","title":"Connection draining"},{"location":"infra/networking/#global-accelerator","text":"AWS Global Accelerator is a network layer service that directs traffic to optimal endpoints over the AWS global network, this improves the availability and performance of Iour internet applications. It provides two static anycast IP addresses that act as a fixed entry point to our application endpoints in a single or multiple AWS Regions, such as our Application Load Balancers, Network Load Balancers, Elastic IP addresses or Amazon EC2 instances, in a single or in multiple AWS regions. The goal is to expose quickly an application to the WW. The problem is the number of internet hops done to access the target public ALB. The solution is to get as fast as possible to a AWS global network endpoint (Edge location), nearest region to the client. It is a global service. With Anycast IP a client is routed to the nearest server. All servers hold the same IP address. So for each application, we create 2 Anycast IP, and the traffic is sent to the edge locations. AWS Global Accelerator uses endpoint weights to determine the proportion of traffic that is directed to endpoints in an endpoint group, and traffic dials to control the percentage of traffic that is directed to an endpoint group Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP.","title":"Global Accelerator"},{"location":"infra/networking/#vpn-cloudhub","text":"If we have multiple AWS Site-to-Site VPN connections, we can provide secure communication between sites using the AWS VPN CloudHub. This enables our remote sites to communicate with each other, and not just with the VPC. Sites that use AWS Direct Connect connections to the virtual private gateway can also be part of the AWS VPN CloudHub. The VPN CloudHub operates on a simple hub-and-spoke model that we can use with or without a VPC. This design is suitable if we have multiple branch offices and existing internet connections and would like to implement a convenient, potentially low-cost hub-and-spoke model for primary or backup connectivity between these remote offices.","title":"VPN CloudHub"},{"location":"infra/networking/#faqs","text":"What differentiates a public subnet from a private subnet? A public subnet is a subnet that's associated with a route table that has a route to an internet gateway","title":"FAQs"},{"location":"infra/route53/","text":"Route 53 \u00b6 A highly available, scalable, fully managed, and authoritative (we can update DNS records) DNS. It is also a Domain Registra. Supports a lot of routing types to respond to DNS query by taking into account % of traffic, latency, healthchecks... It uses the concept of hosted zone which is a \"container\" that holds information about how you want to route traffic for a domain or subdomain. The zone can be public (internet facing) or private (inside VPC). Need a DNS domain. A domain is at least 12$ a year and Route 53 fees is $0.5 per month per hosted zone. See Pricing page . DNS \u00b6 DNS is a collection of rules and records which helps client apps understand how to reach a server through URLs. Here is a quick figure to summarize the process, which in fact should also have the root server (for the .com ... resolution) and TLD server (for amazon or google ). The SLD server is the one presented in the figure. Records \u00b6 Record defines how to route traffic for a domain. Each record contains: a Domain name. record type A (IPv4) or AAAA(IPv6), CNAME, NS. value. routing policy. Time to Live (TTL): is set to get the web browser to keep the DNS resolution in cache. High TTL is around 24 hours, low TTL, at 60s, will make more DNS calls. TTL should be set to strike a balance between how long the value should be cached vs how much pressure should go on the DNS. Need to define the TTL for the app depending on the expected deployment model. A hosted zone is a container that holds information about how we want to route traffic for a domain. Two types are supported: public or private within a VPC. CNAME vs Alias \u00b6 CNAME is a DNS record to maps one domain name to another. CNAME should point to a ALB. Works on non root domain. Alias is used to point a hostname of an AWS resource and can work on root domain (domainname.com). The alias record target could be: ELB, CloudFront distributions, API gateway, Elastic Beanstalk environments, S3 Websites, VPC Interface endpoints, global accelerator, route 53 record (in the same hosted zone). EC2 DNS name could not be a target of alias. Use dig <hostname> to get the DNS resolution record from a Mac or for linux EC2 do the following. sudo yum install -y bind-utils nslookup domainname.com # or dig domainname.com Demonstrate \u00b6 Create three EC2 t2.micro instances, in different region, using security group to authorize SSH and HTTP from anywhere, with the following User data, so the returned HTML page includes the region name: #!/bin/bash yum update -y yum install -y httpd systemctl start httpd systemctl enable httpd EC2_AVAIL_ZONE=$(curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone) echo \"<h1>Hello World from $(hostname -f) in AZ $EC2_AVAIL_ZONE </h1>\" > /var/www/html/index.html Add at least one ALB to route to instances in an AZ Define a domain name (5$ to 12$ a year) Create a hosted zone in Route 53. Define DNS records in Route 53 for each ALB and EC2 IP @ based on a subdomain name, using one of the type as specified in next section. Routing policies \u00b6 It defines how Route 53 responds to DNS queries. It can be used to apply A/B testing, or looking query. Eight routing types: A simple routing policy to get an IP @ from a single resource (still can specify multiple IP@ to be returned in the response, and the client will pick one of the address randomly). There is no health check associated to this record. The weighted routing policy controls the % of the requests that go to specific endpoint. Can do blue-green traffic management. It can also help to split traffic between two regions. It can be associated with Health Checks The latency routing Policy redirects to the server that has the least latency close to the client. Latency is based on traffic between users to AWS Regions. Health check monitors the health and performance of the public resources and assesses DNS failure with automatic failover. We can have HTTP, TCP or HTTPS health checks. We can define from which region to run the health check. They are charged per HC / month. 15 Health checkers exist WW. Send every 30s. Need at least 18% health checkers reporting the endpoint is healthy. HTTP RC code 2xx or 3xx. It is recommended to have one HC per app deployment. It can also monitor latency. To assess private endpoint within a VPC, we need to add a CloudWatch metric and alarm, then create a Health Check to the alarm itself. The failover routing policy helps us to specify a record set to point to a primary and then a secondary instance for DR purpose. The Geo Location routing policy is based on user's location, and we may specify how the traffic from a given country should go to a specific IP@. Need to define a \u201cdefault\u201d policy in case there\u2019s no match on location. It is interesting for website localization, restrict content distribution, load balancing,... Geoproximity takes into account the user and AWS resources locations. It also supports shifting more traffic to resources based on the defined bias. It is part of Route 53 Traffic Flow . The Multi Value routing policy is used to access multiple resources. The record set, associates a Route 53 health checks with the records. The client on DNS request gets up to 8 healthy records returned for each Multi Value query. If one fails then the client can try one other IIP @ from the list. We can use Route 53 health checking to configure active-active and active-passive failover configurations. We configure active-active failover using any routing policy (or combination of routing policies) other than failover, and we configure active-passive failover using the failover routing policy. In active-active failover, all the records that have the same name, the same type (such as A or AAAA), and the same routing policy (such as weighted or latency) are active unless Route 53 considers them unhealthy. With Active-Active Failover, it uses all available resources all the time without a primary nor a secondary resource. If you used another domain registra, it is possible to get the list of AWS DNS server associated to the hosted public zone, and then configure your registra for the NS records to go to those DNS servers.","title":"Route 53"},{"location":"infra/route53/#route-53","text":"A highly available, scalable, fully managed, and authoritative (we can update DNS records) DNS. It is also a Domain Registra. Supports a lot of routing types to respond to DNS query by taking into account % of traffic, latency, healthchecks... It uses the concept of hosted zone which is a \"container\" that holds information about how you want to route traffic for a domain or subdomain. The zone can be public (internet facing) or private (inside VPC). Need a DNS domain. A domain is at least 12$ a year and Route 53 fees is $0.5 per month per hosted zone. See Pricing page .","title":"Route 53"},{"location":"infra/route53/#dns","text":"DNS is a collection of rules and records which helps client apps understand how to reach a server through URLs. Here is a quick figure to summarize the process, which in fact should also have the root server (for the .com ... resolution) and TLD server (for amazon or google ). The SLD server is the one presented in the figure.","title":"DNS"},{"location":"infra/route53/#records","text":"Record defines how to route traffic for a domain. Each record contains: a Domain name. record type A (IPv4) or AAAA(IPv6), CNAME, NS. value. routing policy. Time to Live (TTL): is set to get the web browser to keep the DNS resolution in cache. High TTL is around 24 hours, low TTL, at 60s, will make more DNS calls. TTL should be set to strike a balance between how long the value should be cached vs how much pressure should go on the DNS. Need to define the TTL for the app depending on the expected deployment model. A hosted zone is a container that holds information about how we want to route traffic for a domain. Two types are supported: public or private within a VPC.","title":"Records"},{"location":"infra/route53/#cname-vs-alias","text":"CNAME is a DNS record to maps one domain name to another. CNAME should point to a ALB. Works on non root domain. Alias is used to point a hostname of an AWS resource and can work on root domain (domainname.com). The alias record target could be: ELB, CloudFront distributions, API gateway, Elastic Beanstalk environments, S3 Websites, VPC Interface endpoints, global accelerator, route 53 record (in the same hosted zone). EC2 DNS name could not be a target of alias. Use dig <hostname> to get the DNS resolution record from a Mac or for linux EC2 do the following. sudo yum install -y bind-utils nslookup domainname.com # or dig domainname.com","title":"CNAME vs Alias"},{"location":"infra/route53/#demonstrate","text":"Create three EC2 t2.micro instances, in different region, using security group to authorize SSH and HTTP from anywhere, with the following User data, so the returned HTML page includes the region name: #!/bin/bash yum update -y yum install -y httpd systemctl start httpd systemctl enable httpd EC2_AVAIL_ZONE=$(curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone) echo \"<h1>Hello World from $(hostname -f) in AZ $EC2_AVAIL_ZONE </h1>\" > /var/www/html/index.html Add at least one ALB to route to instances in an AZ Define a domain name (5$ to 12$ a year) Create a hosted zone in Route 53. Define DNS records in Route 53 for each ALB and EC2 IP @ based on a subdomain name, using one of the type as specified in next section.","title":"Demonstrate"},{"location":"infra/route53/#routing-policies","text":"It defines how Route 53 responds to DNS queries. It can be used to apply A/B testing, or looking query. Eight routing types: A simple routing policy to get an IP @ from a single resource (still can specify multiple IP@ to be returned in the response, and the client will pick one of the address randomly). There is no health check associated to this record. The weighted routing policy controls the % of the requests that go to specific endpoint. Can do blue-green traffic management. It can also help to split traffic between two regions. It can be associated with Health Checks The latency routing Policy redirects to the server that has the least latency close to the client. Latency is based on traffic between users to AWS Regions. Health check monitors the health and performance of the public resources and assesses DNS failure with automatic failover. We can have HTTP, TCP or HTTPS health checks. We can define from which region to run the health check. They are charged per HC / month. 15 Health checkers exist WW. Send every 30s. Need at least 18% health checkers reporting the endpoint is healthy. HTTP RC code 2xx or 3xx. It is recommended to have one HC per app deployment. It can also monitor latency. To assess private endpoint within a VPC, we need to add a CloudWatch metric and alarm, then create a Health Check to the alarm itself. The failover routing policy helps us to specify a record set to point to a primary and then a secondary instance for DR purpose. The Geo Location routing policy is based on user's location, and we may specify how the traffic from a given country should go to a specific IP@. Need to define a \u201cdefault\u201d policy in case there\u2019s no match on location. It is interesting for website localization, restrict content distribution, load balancing,... Geoproximity takes into account the user and AWS resources locations. It also supports shifting more traffic to resources based on the defined bias. It is part of Route 53 Traffic Flow . The Multi Value routing policy is used to access multiple resources. The record set, associates a Route 53 health checks with the records. The client on DNS request gets up to 8 healthy records returned for each Multi Value query. If one fails then the client can try one other IIP @ from the list. We can use Route 53 health checking to configure active-active and active-passive failover configurations. We configure active-active failover using any routing policy (or combination of routing policies) other than failover, and we configure active-passive failover using the failover routing policy. In active-active failover, all the records that have the same name, the same type (such as A or AAAA), and the same routing policy (such as weighted or latency) are active unless Route 53 considers them unhealthy. With Active-Active Failover, it uses all available resources all the time without a primary nor a secondary resource. If you used another domain registra, it is possible to get the list of AWS DNS server associated to the hosted public zone, and then configure your registra for the NS records to go to those DNS servers.","title":"Routing policies"},{"location":"infra/security/","text":"Security \u00b6 Info Updated 1/09/2023 Introduction \u00b6 With the AWS Cloud, managing security and compliance is a shared responsibility between AWS and the customer: AWS is responsible of security of the cloud and offers the most flexible and secure cloud computing environment available today. AWS is responsible for patching their managed services and infrastructure security. Customers are responsible for the security in the cloud: secure workloads and applications that are deployed onto the cloud. When using EC2, we are responsible to patch OS for security (but AWS helps by providing patched AMIs, or tools such as Systems Manager , or Inspector for continuous vulnerability testing). AWS runs highly secured data centers. Multiple geographic regions and Availability Zones allow customers to remain resilient in the face of most failure modes, from system failure to natural disasters. For highly regulated industry, AWS helps by getting more than 50 certifications for the infrastructure, globally but also regionaly for specific countries. At global, these include the ISO 27001 , SOC 1 and 2 certifications. For regional in the USA, AWS aligns with FISMA and FedRAMP , in UK with Cyber Essentials, in Australia with IRA... The audits are done by a 3nd party and we can find reports in aws/artifact . AWS Compliance Center is a central location to research cloud-related regulatory requirements Fine-grain identity and access controls combined with continuous monitoring for near real-time security information ( CloudTrail ) ensures that the right resources have the right access at all times, wherever the information is stored Encryption \u00b6 Encryption is widely available through a lot of services and features on top of the platform. We will be able to develop application that can encrypt data at rest, or in transit as it flows over the network between services. S3 storage or EBS block attached storage, have a single click option to do encryption at rest with keys (using KMS). Figure 1: Encryption settings for EBS volume, using KMS The managed service, AWS Key Management Service , helps centrally managing our own keys. It is integrated into a lot of services and the keys never leave AWS FIPS 140-validated Hardware Security Modules unencrypted. User controls access and usage of the keys. With client side encryption, the data are encrypted by the client, and never decrypted by the Server. Only client with the data key can decrypt the data. KMS \u00b6 Managed service to help us create and control the cryptographic keys that are used to protect our data. It supports AWS, customer or custom keys: Figure 2: KMS & AWS keys Integrated with IAM and most AWS services (EBS,S3, RDS, SSM...). Audit KMS Key usage using CloudTrail. Two types of KMS Keys: Symmetric (AES 265 bits) is a single key used to encrypt and decrypt data. Must call KMS API to use it. Asymmetrics (RSA, ECC key pairs) - Public key to encrypt and private key to decrypt. Used to encrypt outside of AWS, with no KMS API access. For AWS managed keys, they are automatically rotated every year. KMS Keys are per region. But when doing snapshot of a EBS volume and moving it to another region, AWS will reencrypt the data with a KMS key from the target region automatically. KMS Key policies help to control who (users, roles ) can access the KMS keys. Used to do cross account access: the copy of a encrypted snapshot done from origin account to the target account will use this policy to access the key to decrypt the snapshot, and then encrypt the copy with a new private key within the target account. Figure 3: AWS key for S3 encryption, with Key policy To encrypt a local file using a symmetric Key in KMS, we can use the CLI like: aws kms encrypt --key-id alias/jb-key --plaintext fileb://ExampleSecretFile.txt --output text --query CiphertextBlob --region eu-west-2 > ExampleSecretFileEncrypted.base64 To share the encrypted file, we can do: cat ExampleSecretFileEncrypted.base64 | base64 --decode > ExampleSecretFileEncrypted Then to decrypt this file using KMS: aws kms decrypt --ciphertext-blob fileb://ExampleSecretFileEncrypted --output text --query Plaintext > ExampleFileDecrypted.base64 --region eu-west-2 # back to the text version cat ExampleFileDecrypted.base64 | base64 --decode > ExampleFileDecrypted.txt KMS supports also Multi-Region Keys, where primary key from one region is replicated to other regions. The Key ID stays the same. The idea is to be able to encrypt in one region and decrypt in another region. A use case will be to encrypt attribute of a DynamoDB Global Table with the Primary key, and let client who has access to the replicated key can decrypt the attribute with lower latency as it is done locally. Figure 4: DB table encrypted and replicated with Keys The attribute is decrypted only if the client has access to the KMS Key. S3 considerations \u00b6 By default with SSE-S3 objects encrypted or not are replicated. With SSE-C, we provide the encryption key, so the encrypted objects are not replicated. With SSE-KMS then we need to specify the KMS Key to encrypt the object in target bucket, adapt the KMS policy so the key is accessible in another region, and in the IAM role, be sure kms:Decrypt is enabled for the source key and kms:Encrypt for the target KMS Key. Encrypted AMI sharing process \u00b6 AMI in source account is encrypted with KMS Key from source account. AMI image should authorize the Launch permission for the target account. Define IAM role for the target account to use, and share KMS key accesses (DescribeKey, ReEncrypted, CreateGrant, Decrypt) via the role. When launching EC2 instance from this AMI in the target account, it is possible to use a new, local KMS key to re-encrypt the volumes. Organizations \u00b6 AWS Organizations helps to centraly manage multiple AWS accounts, group accounts, and simplify account creation. Using accounts helps to isolate AWS resources. It is a global service. Concepts \u00b6 Figure 5: Organization concepts The main account is the management account where other added accounts are members. An organization is a hierarchical structure (a tree) with a root and Organization Units (OU), and AWS accounts. The root user is a single sign-in identity that has complete access to all AWS services and resources in any accounts. Organization unit (OU) contains AWS Accounts or other OUs. It can have only one parent. Figure 6: Organization Services - manage accounts OU can be per team, per line of business. AWS Organizations uses IAM service-linked roles to enable trusted services to perform tasks on your behalf in your organization's member accounts. We can create service control policies (SCPs) cross AWS accounts to deny access to AWS services for individuals or group of accounts in an OU. AWS Organization exposes APIs to automate account management. It helps consolidating billing accross all the accounts and user can get pricing benefits from aggregate usage. Shared reserved instances and Saving Plans discounts apply across accounts. Can define Blocklist or Allowlist strategies. There is no cost to use AWS Organizations. We can invite an existing AWS account to our organization. But payment changes of ownership. Advantages \u00b6 Better isolation than VPC. Can use tagging for billing purpose. Enable CloudTrail for all accounts and get report in S3. Can define service control policies (SCP) as IAM policies applied to OU or Accounts to restric Users and Roles. Explicit allow. The root OU will have FullAWSAccess SCP. Figure 7: Organization policies Deeper Dive \u00b6 Presentation on organizations User guide Organization API S3 storage lens with Organization lab IAM Identity and Access Management \u00b6 Helps to control access to AWS services. This is a global service, defined at the account level, cross regions. IAM helps us to define users (physical person), groups and roles, and permissions (policies). * Each account has a root user. Root user access should be set up to use MFA and complex password. Do not delete root user. * Do not use root user, but create user and always use them when login. jerome , aws-jb and mathieu are users. * Administrator users are part of an admin group with admin priviledges, like AdministratorAccess . * Assign users to groups ( admin and developers ) and assign policies to groups and not to individual user. * Groups can only contain users, not other groups. * Users can belong to multiple groups. * A classical use case: Create the user accounts, create a group for each department, create and attach an appropriate policy to each group, and place each user account into their department\u2019s group. When new team members are onboarded, create their account and put them in the appropriate group. If an existing team member changes departments, move their account to their new IAM group. * AWS Account has a unique ID but can be set with an alias. The console URL includes the user alias. Security Policies \u00b6 Policies are written in JSON, to define permissions Allow , Deny for users to access AWS services, groups and roles... Policy applies to Principal : account/user/role, list the actions (what is allowed or denied) on the given resources . It must define an ID, a version and statement(s): { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"iam:List*\" ], \"Resource\" : \"*\" } ] } Another example to control access to IAM: Use the Least privilege permission approach: Give users the minimal amount of permissions they need to do their job. As soon as there is a deny in the chain of policy evaluation, then allows will not work. See the diagram below from the product documentation to review the decision flow to allow access to resource: Policy can define the password type > Account settings > Password policy , and when users are allowed to change the password. Inline policy can be defined at the user level, but it is recommended to use Group and Group level policies. As user can be part of multi groups, she/he will heritate to the different policies of those groups. IAM is not used for website authentication and authorization. For identity federation, use SAML standard. We can test Policies with the policy simulator . We can update a policy attached to a role and it takes immediate effect. Policies are not attached to AWS service, but to Roles. We can attach & detach roles to running EC2, without having to stop and restart it. Multi Factor Authentication - MFA \u00b6 Multi Factor Authentication is used to verify a human is the real user of a service Always protect root account with MFA. MFA = password + device we own. The device could be a universal 2 nd factor security key. (ubikey) Authy is a multi-device service with free mobile app. We can have multiple users on the same device. IAM Roles \u00b6 To get AWS services doing work on other service, we need to use IAM Role and policies. Roles are assigned per application, or per EC2 or lambda function... A lot of roles are predefined and can be reused, and we can define new role for any service intance we create. Maintaining roles, and policies within a role, is more efficient than maintaining policies for users. When a service assumes a role, IAM dynamically provides temporary credentials that expire after a defined period of time, between 15 minutes to 36 hours. IAM role does not create static access key, so no risk to have the key stolen. EC2 example \u00b6 When connected to an EC2 machine via ssh or using EC2 Instance Connect tool, we need to set the IAM roles for who can use the EC2. A command like aws iam list-users will not work until a role is attached. For example, the DemoEC2Role role is defined to let IAM access in read only: This role is then defined in the EC2 / Security > attach IAM role, and now read-only commands with aws iam will work. Lamba example \u00b6 We want to implement a lambda function, that will access S3 bucket to get file and another s3 bucket to put objects. We need a new role as illustrated in following figure, with permission to execute on lambda service, and trace qith XRay + a custom policy. See the Policies defined in labs/s3-lambda . AWSLambdaBasicExecutionRole is for logs, AWSXRayDaemonWriteAccess for the function to put traces into CloudWatch XRay. When defining the Lambda function we select the role we defined with the permissions we selected so the function can access other services. The role defines the permissions of our function. resource-based policy \u00b6 When user, application or service assumes a role, it takes the permissions assigned to the role, and loose its original permissions. While when we use resource-based policy, the principal doesn't have to give up his permissions. For example if a user in Account A need to scan DynamoDB table in account A and dumpt it in S3 bucket in account B, then it is important to use resource-based policy for S3 bucket, so user does not loose its access to dynamoDB. This is also used for EventBridge to access lambda, SNS, SQS, cloudWatch logs, API gateway... Permissions boundary \u00b6 Set a permissions boundary to control the maximum permissions a user can have. This is defined at user's or role level, and we define policy for example to authorize the user to do anything on EC2, CloudWatch or S3. The effective permission of a user is the join between Organization SCP, Permissions Boundary, and identity-based policies. Identity-based policies are attached to an IAM user, group, or role. These policies let you specify what that identity can do. This is used to: Delegate responsibilities to non-admin users within their permission boundaries to create specific resources, like IAM user. Allow developers to self-assign policies and manage their own permissions, while making sure they can increase their privileges. Restrict one user. Security tools \u00b6 In IAM, use > Credentials report to download account based report. In IAM, use > Users > select one user (aws-jb) and then Access Advisor tab : Access Advisor shows the services that the selected user can access and when those services were last accessed. Amazon GuardDuty is a security tool to continuously monitor your AWS accounts, instances, containers, users, and storage for potential threats. IAM Deeper dive \u00b6 Policy evaluation logic Amazon Cognito \u00b6 Amazon Cognito is a managed service which offers Authentication and Authorization features, it has User pools and Identity pools . It is scalable and highly available. Allows user to add user registration, sign in, and define access control. Supports standards based identity providers like OAuth 2.0, SAML, OIDC. User pools are user directories that provide sign-up and sign-in options for your app or mobile users. User pool is for sign-in functionality, and it integrates with API Gateway and ALB. Identity pools provide AWS credentials to grant your users access to your API, other AWS services via IAM permissions. Free tier of 50,000 MAUs for users who sign in directly to Cognito User Pools and 50 MAUs for users federated through SAML 2.0 based identity providers. Users can sign in through social identity providers like Google, Facebook, and Amazon. They can also sign in through enterprise providers like ADFS and Okta with SAML 2.0 and OpenID Connect authentication. Use Amazon Cognito's built-in UI and options for federating with multiple identity providers to add user sign-in, sign-up into an application. use AWS Amplify and the aws-amplify-vue module to provide basic user sign up, sign in, and sign out functionality. Examples \u00b6 Configure how user login: Then define if you want to use MFA, get an option to reset password.. Configure sign-up and message delivery. We can disable auto sign-up and sending email. System management parameter store \u00b6 Managed service, serverless, it is a secure storage for configuration and secrets. SSM Parameter Store has built-in version tracking capability. Each time we edit the value of a parameter, SSM Parameter Store creates a new version of the parameter and retains the previous versions. It stores data in a hierarchical tree. For example aws ssm get-parameters-by-path --path /an-app/dev/ Secrets Manager \u00b6 Keep Secret information, with automatic rotation enforced, with integration with RDS, Redshift, DocumentDB. Secrets can be replicated between regions. ACM - Certificate Manager \u00b6 Manage and deploy TLS certificates. Supports public and private certificates. Automatic renewal. Free of charge for public certificates. Integrated with ELBs, Cloudfront, APIs on API gateway. Cannot use ACM with EC2. WAF Web Application Firewall \u00b6 User to monitor the HTTP(S) requests that are forwarded to your protected (Layer 7) web application resources. It is deployed on ALB, API Gateway, CloudFront, AppSync GraphQL API, Cognito User Pool. Rules are defined in Web Access Control List: they can apply to IP@, HTTP headers, body, URI strings, Cross-site Scripting... We can define size constraints, geographic matches, or rate-based rules for DDoS protection. For example we can implement a rule capable of blocking all IPs that have more than 2,000 requests in the last 5 minute interval. WebACL is regional, except for CloudFront. Deeper dive \u00b6 Rate-based rule statement WAS FAQ Firewall manager \u00b6 Firewall manager simplifies your administration and maintenance tasks across multiple accounts (across AWS Organizations) and resources for a variety of protections, including AWS WAF, AWS Shield Advanced, Amazon VPC security groups, AWS Network Firewall, and Amazon Route 53 Resolver DNS Firewall. Shield \u00b6 To protect against DDoS attack. All AWS customers benefit from the automatic protections of AWS Shield Standard at no additional charge. When you use AWS Shield Standard with Amazon CloudFront and Amazon Route 53, you receive comprehensive availability protection against all known infrastructure (Layer 3 and 4) attacks. For more protection for EC2, ELB, CloudFront, Global Accelerator, Route 53... the advanced otion supports network and transport layer protections with near real-time visibility into attacks and 24/7 access to Shield Response Team. GuardDuty \u00b6 Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3. Security monitoring service that analyzes and processes data sources such as: AWS CloudTrail data events for Amazon S3 logs, CloudTrail management event logs, DNS logs, Amazon EBS volume data, Kubernetes audit logs, Amazon VPC flow logs RDS login activity It uses ML/AI model, anomaly detection, and integrated threat intelligence to identify and prioritize potential threats.. GuardDuty can detect compromised EC2 instances and container workloads serving malware, or mining bitcoin. By integrating with Amazon EventBridge Events, GuardDuty alerts are actionable, easy to aggregate across multiple accounts, and straightforward to push into existing event management and workflow systems. Amazon Inspector \u00b6 Vulnerability management service that continuously scans your AWS workloads for software vulnerabilities and unintended network exposure. Includes EC2 instance scan and container images in Amazon ECR. Amazon Macie \u00b6 Amazon Macie is a fully managed data security service that uses Machine Learning to discover and protect your sensitive data stored in S3 buckets. It automatically provides an inventory of S3 buckets including a list of unencrypted buckets, publicly accessible buckets, and buckets shared with other AWS accounts. It allows you to identify and alert you to sensitive data, such as Personally Identifiable Information (PII). All Macie findings are sent to Amazon EventBridge and can also be published to AWS Security Hub to initiate automated remediation such as blocking public access to your S3 storage. IAM Identity Center \u00b6 Single sign-on managed services for all AWS accounts within AWS Organizations, cloud applications, SAML2.0-enabled applications, EC2 Windows instances. The identity provider can be an identity store in IAM Identity center or an Active Directory, OneLogin, Okta... API reference AWS CloudHSM \u00b6 AWS CloudHSM (Hardware Security Module) helps you meet corporate, contractual, and regulatory compliance requirements for data security. A Hardware Security Module (HSM) provides secure key storage and cryptographic operations within a tamper-resistant hardware device. HSMs are designed to securely store cryptographic key material and use the key material without exposing it outside the cryptographic boundary of the hardware. We can create a cluster that has from 1 to 28 HSMs (the default limit is 6 HSMs per AWS account per AWS Region). Example of use case is when hosted on EC2, must encrypt the data before writing this data to storage. We can also use AWS KMS if you need to encrypt data before writing it to storage when an AWS managed hardware security module for the cryptographic keys is not required. AWS Audit Manager \u00b6 AWS Audit Manager is an automated service to continually audit your AWS usage to simplify how you manage risk and compliance with regulations and industry standards. It eifjcbfdjifhurnicntgjklgrihfkfcdfierkgkghcdi produces reports specific to auditors for PCI compliance, GDPR, and more. AWS Security Hub \u00b6 AWS Security Hub is a Cloud Security Posture Management service that performs security best practice checks, aggregates alerts, and enables automated remediation. AWS Security Hub is a single place to view all your security alerts from services like Amazon GuardDuty, Amazon Inspector, Amazon Macie, and AWS Firewall Manager. It is not used for producing audit reports. AWS Control Tower \u00b6 AWS Control Tower orchestration extends the capabilities of AWS Organizations to set up and govern multi-account environment. It leverages SCPs for preventative guardrails and AWS Config for detective guardrails. No additional charge exists for using AWS Control Tower. You only pay for the AWS services enabled by AWS Control Tower, and the services you use in your landing zone. Security FAQ \u00b6 Sensitive data in the cloud can be accessed by everyone AWS storage managed services are secured by default. You control who can access your data. For example, in S3 bucket, objects, by default are only available by the owner who created them. IAM policies define access to buckets, and in S3 itself you can define policies. Some policies can be defined at the Account level to forbid to define public bucket. From detection services like CloudTrail and CloudWatch Events you can route events to security team or respond in real-time to those events with code in a Lambda function. Integrate with existing Directory? IAM can integrate with your existing directory where you can map IAM groups to directory group membership. Running intrusion/penetration testing and vulnerability scan You can run any of those tests on your own workloads and for the following services: Amazon EC2 instances NAT Gateways Elastic Load Balancers Amazon RDS Amazon CloudFront Amazon Aurora Amazon API Gateways AWS Fargate AWS Lambda and Lambda Edge functions Amazon Lightsail resources Amazon Elastic Beanstalk environments You need to sign a \"Seek approval\" document so AWS can understand what you are doing and is not searching for malicious activities on your account or triggers security alarms. To integrate Penetration tests in your CI/CD Amazon Inspector is an automated vulnerability management service that continually scans AWS workloads for software vulnerabilities and unintended network exposure. How to ensure RDS database can only be accessed using profile credentials specific to EC2 instance? IAM has database authentication capabilities that would allow an RDS database to only be accessed using the profile credentials specific to your EC2 instances. Specifics business oriented accreditations The workload is certified by your 3nd party auditor. AWS provides the accreditation doc for infrastructure and hosting operations. You have full visibility into the logical infrastructure via API calls, so your auditors can get information on items like security group (firewall) rules or RDS configuration via API calls, or account access information through our API auditing service, CloudTrail . How does IDS/IPS works on AWS? Intrusion Detection and Prevention can be achieved in number of ways. There are tools/services like Guard Duty , 3 rd party solution (AWS marketplace) as well architecture best practices. Amazon VPC traffic mirroring is a \u201cvirtual fiber tap\u201d that gives you direct access to the network packets flowing through your VPC. Encrypted traffic between instances? Within the AWS network, you'll be hosted in a Virtual Private Cloud (VPC) and all traffic between your instances stay within your VPC. Other customers cannot see your traffic. Configuring inter-instance traffic encryption would be the same as on-premises. Does AWS provide DDoS protection? AWS Shield Standard is automically provided to all AWS customers and AWS Shield advanced service can be bought for additional features. AWS Shield is a managed Distributed Denial of Service events protection service that safeguards applications running on AWS. How to be sure to data is really deleted You own the data inside AWS. But on delete operation the data is destroyed and wiped out. When destroying physical hardware, AWS follows the NIST 888 requirements, and it is done in a secure zone in the data center. Audits are done by 3nd party vendor, and compliances are reported in Artifact . Is serverless secured? Roles are defined to define who can push code, access it, and fine grained control of the serverless execution. All serverless managed services inherit from the underlying platform security control. As an example, Lambda execution are authenticated, authorized and are made visible to the SREs via the commong logging mechanism. Solution Design \u00b6 The most important questions to ask and address when designing AWS solution with security consideration are: who can access the resources in the account. what can access the resources, and how access is allowed. Address how to secure, web tier, app tier and database access. What firewalls to use, and where. For each compute resources address access to monitoring and logging and network environment that contain those resources. Is there any Gateway needed? Finally consider data life cycle and access control over time. What are the compliance requirements? Blocking IP address \u00b6 At the VPC level, the best approach is to use Network ACL, when EC2 is exposed via public IP address, as illustrasted in the diagram below: Use a denial rule in NACL on specific IP address. Security group inbound rule specifies allowed only IP range, which should work, but then it blocks larger set of clients, so it will not be a solution for global application. Running a firewall on the EC2 will help, but it is more complex to administer. If an ALB is in the middle, then the EC2 security group will specify the SG of the ALB, and we are still using NACL. ALB does connection termination, so client will not be able to attack EC2. To add more control we can add a Web Application Firewall to add complex filtering on the IP @. Using a NLB, there will be no security group at the network load balancer, so traffic reaches EC2s. In this case only the NACL rules will help to protect. When using CloudFront, we need to add WAF, as NACL at the VPC level will not work, the ALB being connected to the cloudFront IP address, only (via security group). Geo restriction can be defined at the cloud front level, to deny a complete country for example. Learn more \u00b6 Data Safe cloud checklist Top security myth - dispelled","title":"Security"},{"location":"infra/security/#security","text":"Info Updated 1/09/2023","title":"Security"},{"location":"infra/security/#introduction","text":"With the AWS Cloud, managing security and compliance is a shared responsibility between AWS and the customer: AWS is responsible of security of the cloud and offers the most flexible and secure cloud computing environment available today. AWS is responsible for patching their managed services and infrastructure security. Customers are responsible for the security in the cloud: secure workloads and applications that are deployed onto the cloud. When using EC2, we are responsible to patch OS for security (but AWS helps by providing patched AMIs, or tools such as Systems Manager , or Inspector for continuous vulnerability testing). AWS runs highly secured data centers. Multiple geographic regions and Availability Zones allow customers to remain resilient in the face of most failure modes, from system failure to natural disasters. For highly regulated industry, AWS helps by getting more than 50 certifications for the infrastructure, globally but also regionaly for specific countries. At global, these include the ISO 27001 , SOC 1 and 2 certifications. For regional in the USA, AWS aligns with FISMA and FedRAMP , in UK with Cyber Essentials, in Australia with IRA... The audits are done by a 3nd party and we can find reports in aws/artifact . AWS Compliance Center is a central location to research cloud-related regulatory requirements Fine-grain identity and access controls combined with continuous monitoring for near real-time security information ( CloudTrail ) ensures that the right resources have the right access at all times, wherever the information is stored","title":"Introduction"},{"location":"infra/security/#encryption","text":"Encryption is widely available through a lot of services and features on top of the platform. We will be able to develop application that can encrypt data at rest, or in transit as it flows over the network between services. S3 storage or EBS block attached storage, have a single click option to do encryption at rest with keys (using KMS). Figure 1: Encryption settings for EBS volume, using KMS The managed service, AWS Key Management Service , helps centrally managing our own keys. It is integrated into a lot of services and the keys never leave AWS FIPS 140-validated Hardware Security Modules unencrypted. User controls access and usage of the keys. With client side encryption, the data are encrypted by the client, and never decrypted by the Server. Only client with the data key can decrypt the data.","title":"Encryption"},{"location":"infra/security/#kms","text":"Managed service to help us create and control the cryptographic keys that are used to protect our data. It supports AWS, customer or custom keys: Figure 2: KMS & AWS keys Integrated with IAM and most AWS services (EBS,S3, RDS, SSM...). Audit KMS Key usage using CloudTrail. Two types of KMS Keys: Symmetric (AES 265 bits) is a single key used to encrypt and decrypt data. Must call KMS API to use it. Asymmetrics (RSA, ECC key pairs) - Public key to encrypt and private key to decrypt. Used to encrypt outside of AWS, with no KMS API access. For AWS managed keys, they are automatically rotated every year. KMS Keys are per region. But when doing snapshot of a EBS volume and moving it to another region, AWS will reencrypt the data with a KMS key from the target region automatically. KMS Key policies help to control who (users, roles ) can access the KMS keys. Used to do cross account access: the copy of a encrypted snapshot done from origin account to the target account will use this policy to access the key to decrypt the snapshot, and then encrypt the copy with a new private key within the target account. Figure 3: AWS key for S3 encryption, with Key policy To encrypt a local file using a symmetric Key in KMS, we can use the CLI like: aws kms encrypt --key-id alias/jb-key --plaintext fileb://ExampleSecretFile.txt --output text --query CiphertextBlob --region eu-west-2 > ExampleSecretFileEncrypted.base64 To share the encrypted file, we can do: cat ExampleSecretFileEncrypted.base64 | base64 --decode > ExampleSecretFileEncrypted Then to decrypt this file using KMS: aws kms decrypt --ciphertext-blob fileb://ExampleSecretFileEncrypted --output text --query Plaintext > ExampleFileDecrypted.base64 --region eu-west-2 # back to the text version cat ExampleFileDecrypted.base64 | base64 --decode > ExampleFileDecrypted.txt KMS supports also Multi-Region Keys, where primary key from one region is replicated to other regions. The Key ID stays the same. The idea is to be able to encrypt in one region and decrypt in another region. A use case will be to encrypt attribute of a DynamoDB Global Table with the Primary key, and let client who has access to the replicated key can decrypt the attribute with lower latency as it is done locally. Figure 4: DB table encrypted and replicated with Keys The attribute is decrypted only if the client has access to the KMS Key.","title":"KMS"},{"location":"infra/security/#s3-considerations","text":"By default with SSE-S3 objects encrypted or not are replicated. With SSE-C, we provide the encryption key, so the encrypted objects are not replicated. With SSE-KMS then we need to specify the KMS Key to encrypt the object in target bucket, adapt the KMS policy so the key is accessible in another region, and in the IAM role, be sure kms:Decrypt is enabled for the source key and kms:Encrypt for the target KMS Key.","title":"S3 considerations"},{"location":"infra/security/#encrypted-ami-sharing-process","text":"AMI in source account is encrypted with KMS Key from source account. AMI image should authorize the Launch permission for the target account. Define IAM role for the target account to use, and share KMS key accesses (DescribeKey, ReEncrypted, CreateGrant, Decrypt) via the role. When launching EC2 instance from this AMI in the target account, it is possible to use a new, local KMS key to re-encrypt the volumes.","title":"Encrypted AMI sharing process"},{"location":"infra/security/#organizations","text":"AWS Organizations helps to centraly manage multiple AWS accounts, group accounts, and simplify account creation. Using accounts helps to isolate AWS resources. It is a global service.","title":"Organizations"},{"location":"infra/security/#concepts","text":"Figure 5: Organization concepts The main account is the management account where other added accounts are members. An organization is a hierarchical structure (a tree) with a root and Organization Units (OU), and AWS accounts. The root user is a single sign-in identity that has complete access to all AWS services and resources in any accounts. Organization unit (OU) contains AWS Accounts or other OUs. It can have only one parent. Figure 6: Organization Services - manage accounts OU can be per team, per line of business. AWS Organizations uses IAM service-linked roles to enable trusted services to perform tasks on your behalf in your organization's member accounts. We can create service control policies (SCPs) cross AWS accounts to deny access to AWS services for individuals or group of accounts in an OU. AWS Organization exposes APIs to automate account management. It helps consolidating billing accross all the accounts and user can get pricing benefits from aggregate usage. Shared reserved instances and Saving Plans discounts apply across accounts. Can define Blocklist or Allowlist strategies. There is no cost to use AWS Organizations. We can invite an existing AWS account to our organization. But payment changes of ownership.","title":"Concepts"},{"location":"infra/security/#advantages","text":"Better isolation than VPC. Can use tagging for billing purpose. Enable CloudTrail for all accounts and get report in S3. Can define service control policies (SCP) as IAM policies applied to OU or Accounts to restric Users and Roles. Explicit allow. The root OU will have FullAWSAccess SCP. Figure 7: Organization policies","title":"Advantages"},{"location":"infra/security/#deeper-dive","text":"Presentation on organizations User guide Organization API S3 storage lens with Organization lab","title":"Deeper Dive"},{"location":"infra/security/#iam-identity-and-access-management","text":"Helps to control access to AWS services. This is a global service, defined at the account level, cross regions. IAM helps us to define users (physical person), groups and roles, and permissions (policies). * Each account has a root user. Root user access should be set up to use MFA and complex password. Do not delete root user. * Do not use root user, but create user and always use them when login. jerome , aws-jb and mathieu are users. * Administrator users are part of an admin group with admin priviledges, like AdministratorAccess . * Assign users to groups ( admin and developers ) and assign policies to groups and not to individual user. * Groups can only contain users, not other groups. * Users can belong to multiple groups. * A classical use case: Create the user accounts, create a group for each department, create and attach an appropriate policy to each group, and place each user account into their department\u2019s group. When new team members are onboarded, create their account and put them in the appropriate group. If an existing team member changes departments, move their account to their new IAM group. * AWS Account has a unique ID but can be set with an alias. The console URL includes the user alias.","title":"IAM Identity and Access Management"},{"location":"infra/security/#security-policies","text":"Policies are written in JSON, to define permissions Allow , Deny for users to access AWS services, groups and roles... Policy applies to Principal : account/user/role, list the actions (what is allowed or denied) on the given resources . It must define an ID, a version and statement(s): { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"iam:List*\" ], \"Resource\" : \"*\" } ] } Another example to control access to IAM: Use the Least privilege permission approach: Give users the minimal amount of permissions they need to do their job. As soon as there is a deny in the chain of policy evaluation, then allows will not work. See the diagram below from the product documentation to review the decision flow to allow access to resource: Policy can define the password type > Account settings > Password policy , and when users are allowed to change the password. Inline policy can be defined at the user level, but it is recommended to use Group and Group level policies. As user can be part of multi groups, she/he will heritate to the different policies of those groups. IAM is not used for website authentication and authorization. For identity federation, use SAML standard. We can test Policies with the policy simulator . We can update a policy attached to a role and it takes immediate effect. Policies are not attached to AWS service, but to Roles. We can attach & detach roles to running EC2, without having to stop and restart it.","title":"Security Policies"},{"location":"infra/security/#multi-factor-authentication-mfa","text":"Multi Factor Authentication is used to verify a human is the real user of a service Always protect root account with MFA. MFA = password + device we own. The device could be a universal 2 nd factor security key. (ubikey) Authy is a multi-device service with free mobile app. We can have multiple users on the same device.","title":"Multi Factor Authentication - MFA"},{"location":"infra/security/#iam-roles","text":"To get AWS services doing work on other service, we need to use IAM Role and policies. Roles are assigned per application, or per EC2 or lambda function... A lot of roles are predefined and can be reused, and we can define new role for any service intance we create. Maintaining roles, and policies within a role, is more efficient than maintaining policies for users. When a service assumes a role, IAM dynamically provides temporary credentials that expire after a defined period of time, between 15 minutes to 36 hours. IAM role does not create static access key, so no risk to have the key stolen.","title":"IAM Roles"},{"location":"infra/security/#permissions-boundary","text":"Set a permissions boundary to control the maximum permissions a user can have. This is defined at user's or role level, and we define policy for example to authorize the user to do anything on EC2, CloudWatch or S3. The effective permission of a user is the join between Organization SCP, Permissions Boundary, and identity-based policies. Identity-based policies are attached to an IAM user, group, or role. These policies let you specify what that identity can do. This is used to: Delegate responsibilities to non-admin users within their permission boundaries to create specific resources, like IAM user. Allow developers to self-assign policies and manage their own permissions, while making sure they can increase their privileges. Restrict one user.","title":"Permissions boundary"},{"location":"infra/security/#security-tools","text":"In IAM, use > Credentials report to download account based report. In IAM, use > Users > select one user (aws-jb) and then Access Advisor tab : Access Advisor shows the services that the selected user can access and when those services were last accessed. Amazon GuardDuty is a security tool to continuously monitor your AWS accounts, instances, containers, users, and storage for potential threats.","title":"Security tools"},{"location":"infra/security/#iam-deeper-dive","text":"Policy evaluation logic","title":"IAM Deeper dive"},{"location":"infra/security/#amazon-cognito","text":"Amazon Cognito is a managed service which offers Authentication and Authorization features, it has User pools and Identity pools . It is scalable and highly available. Allows user to add user registration, sign in, and define access control. Supports standards based identity providers like OAuth 2.0, SAML, OIDC. User pools are user directories that provide sign-up and sign-in options for your app or mobile users. User pool is for sign-in functionality, and it integrates with API Gateway and ALB. Identity pools provide AWS credentials to grant your users access to your API, other AWS services via IAM permissions. Free tier of 50,000 MAUs for users who sign in directly to Cognito User Pools and 50 MAUs for users federated through SAML 2.0 based identity providers. Users can sign in through social identity providers like Google, Facebook, and Amazon. They can also sign in through enterprise providers like ADFS and Okta with SAML 2.0 and OpenID Connect authentication. Use Amazon Cognito's built-in UI and options for federating with multiple identity providers to add user sign-in, sign-up into an application. use AWS Amplify and the aws-amplify-vue module to provide basic user sign up, sign in, and sign out functionality.","title":"Amazon Cognito"},{"location":"infra/security/#examples","text":"Configure how user login: Then define if you want to use MFA, get an option to reset password.. Configure sign-up and message delivery. We can disable auto sign-up and sending email.","title":"Examples"},{"location":"infra/security/#system-management-parameter-store","text":"Managed service, serverless, it is a secure storage for configuration and secrets. SSM Parameter Store has built-in version tracking capability. Each time we edit the value of a parameter, SSM Parameter Store creates a new version of the parameter and retains the previous versions. It stores data in a hierarchical tree. For example aws ssm get-parameters-by-path --path /an-app/dev/","title":"System management parameter store"},{"location":"infra/security/#secrets-manager","text":"Keep Secret information, with automatic rotation enforced, with integration with RDS, Redshift, DocumentDB. Secrets can be replicated between regions.","title":"Secrets Manager"},{"location":"infra/security/#acm-certificate-manager","text":"Manage and deploy TLS certificates. Supports public and private certificates. Automatic renewal. Free of charge for public certificates. Integrated with ELBs, Cloudfront, APIs on API gateway. Cannot use ACM with EC2.","title":"ACM - Certificate Manager"},{"location":"infra/security/#waf-web-application-firewall","text":"User to monitor the HTTP(S) requests that are forwarded to your protected (Layer 7) web application resources. It is deployed on ALB, API Gateway, CloudFront, AppSync GraphQL API, Cognito User Pool. Rules are defined in Web Access Control List: they can apply to IP@, HTTP headers, body, URI strings, Cross-site Scripting... We can define size constraints, geographic matches, or rate-based rules for DDoS protection. For example we can implement a rule capable of blocking all IPs that have more than 2,000 requests in the last 5 minute interval. WebACL is regional, except for CloudFront.","title":"WAF Web Application Firewall"},{"location":"infra/security/#deeper-dive_1","text":"Rate-based rule statement WAS FAQ","title":"Deeper dive"},{"location":"infra/security/#firewall-manager","text":"Firewall manager simplifies your administration and maintenance tasks across multiple accounts (across AWS Organizations) and resources for a variety of protections, including AWS WAF, AWS Shield Advanced, Amazon VPC security groups, AWS Network Firewall, and Amazon Route 53 Resolver DNS Firewall.","title":"Firewall manager"},{"location":"infra/security/#shield","text":"To protect against DDoS attack. All AWS customers benefit from the automatic protections of AWS Shield Standard at no additional charge. When you use AWS Shield Standard with Amazon CloudFront and Amazon Route 53, you receive comprehensive availability protection against all known infrastructure (Layer 3 and 4) attacks. For more protection for EC2, ELB, CloudFront, Global Accelerator, Route 53... the advanced otion supports network and transport layer protections with near real-time visibility into attacks and 24/7 access to Shield Response Team.","title":"Shield"},{"location":"infra/security/#guardduty","text":"Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3. Security monitoring service that analyzes and processes data sources such as: AWS CloudTrail data events for Amazon S3 logs, CloudTrail management event logs, DNS logs, Amazon EBS volume data, Kubernetes audit logs, Amazon VPC flow logs RDS login activity It uses ML/AI model, anomaly detection, and integrated threat intelligence to identify and prioritize potential threats.. GuardDuty can detect compromised EC2 instances and container workloads serving malware, or mining bitcoin. By integrating with Amazon EventBridge Events, GuardDuty alerts are actionable, easy to aggregate across multiple accounts, and straightforward to push into existing event management and workflow systems.","title":"GuardDuty"},{"location":"infra/security/#amazon-inspector","text":"Vulnerability management service that continuously scans your AWS workloads for software vulnerabilities and unintended network exposure. Includes EC2 instance scan and container images in Amazon ECR.","title":"Amazon Inspector"},{"location":"infra/security/#amazon-macie","text":"Amazon Macie is a fully managed data security service that uses Machine Learning to discover and protect your sensitive data stored in S3 buckets. It automatically provides an inventory of S3 buckets including a list of unencrypted buckets, publicly accessible buckets, and buckets shared with other AWS accounts. It allows you to identify and alert you to sensitive data, such as Personally Identifiable Information (PII). All Macie findings are sent to Amazon EventBridge and can also be published to AWS Security Hub to initiate automated remediation such as blocking public access to your S3 storage.","title":"Amazon Macie"},{"location":"infra/security/#iam-identity-center","text":"Single sign-on managed services for all AWS accounts within AWS Organizations, cloud applications, SAML2.0-enabled applications, EC2 Windows instances. The identity provider can be an identity store in IAM Identity center or an Active Directory, OneLogin, Okta... API reference","title":"IAM Identity Center"},{"location":"infra/security/#aws-cloudhsm","text":"AWS CloudHSM (Hardware Security Module) helps you meet corporate, contractual, and regulatory compliance requirements for data security. A Hardware Security Module (HSM) provides secure key storage and cryptographic operations within a tamper-resistant hardware device. HSMs are designed to securely store cryptographic key material and use the key material without exposing it outside the cryptographic boundary of the hardware. We can create a cluster that has from 1 to 28 HSMs (the default limit is 6 HSMs per AWS account per AWS Region). Example of use case is when hosted on EC2, must encrypt the data before writing this data to storage. We can also use AWS KMS if you need to encrypt data before writing it to storage when an AWS managed hardware security module for the cryptographic keys is not required.","title":"AWS CloudHSM"},{"location":"infra/security/#aws-audit-manager","text":"AWS Audit Manager is an automated service to continually audit your AWS usage to simplify how you manage risk and compliance with regulations and industry standards. It eifjcbfdjifhurnicntgjklgrihfkfcdfierkgkghcdi produces reports specific to auditors for PCI compliance, GDPR, and more.","title":"AWS Audit Manager"},{"location":"infra/security/#aws-security-hub","text":"AWS Security Hub is a Cloud Security Posture Management service that performs security best practice checks, aggregates alerts, and enables automated remediation. AWS Security Hub is a single place to view all your security alerts from services like Amazon GuardDuty, Amazon Inspector, Amazon Macie, and AWS Firewall Manager. It is not used for producing audit reports.","title":"AWS Security Hub"},{"location":"infra/security/#aws-control-tower","text":"AWS Control Tower orchestration extends the capabilities of AWS Organizations to set up and govern multi-account environment. It leverages SCPs for preventative guardrails and AWS Config for detective guardrails. No additional charge exists for using AWS Control Tower. You only pay for the AWS services enabled by AWS Control Tower, and the services you use in your landing zone.","title":"AWS Control Tower"},{"location":"infra/security/#security-faq","text":"Sensitive data in the cloud can be accessed by everyone AWS storage managed services are secured by default. You control who can access your data. For example, in S3 bucket, objects, by default are only available by the owner who created them. IAM policies define access to buckets, and in S3 itself you can define policies. Some policies can be defined at the Account level to forbid to define public bucket. From detection services like CloudTrail and CloudWatch Events you can route events to security team or respond in real-time to those events with code in a Lambda function. Integrate with existing Directory? IAM can integrate with your existing directory where you can map IAM groups to directory group membership. Running intrusion/penetration testing and vulnerability scan You can run any of those tests on your own workloads and for the following services: Amazon EC2 instances NAT Gateways Elastic Load Balancers Amazon RDS Amazon CloudFront Amazon Aurora Amazon API Gateways AWS Fargate AWS Lambda and Lambda Edge functions Amazon Lightsail resources Amazon Elastic Beanstalk environments You need to sign a \"Seek approval\" document so AWS can understand what you are doing and is not searching for malicious activities on your account or triggers security alarms. To integrate Penetration tests in your CI/CD Amazon Inspector is an automated vulnerability management service that continually scans AWS workloads for software vulnerabilities and unintended network exposure. How to ensure RDS database can only be accessed using profile credentials specific to EC2 instance? IAM has database authentication capabilities that would allow an RDS database to only be accessed using the profile credentials specific to your EC2 instances. Specifics business oriented accreditations The workload is certified by your 3nd party auditor. AWS provides the accreditation doc for infrastructure and hosting operations. You have full visibility into the logical infrastructure via API calls, so your auditors can get information on items like security group (firewall) rules or RDS configuration via API calls, or account access information through our API auditing service, CloudTrail . How does IDS/IPS works on AWS? Intrusion Detection and Prevention can be achieved in number of ways. There are tools/services like Guard Duty , 3 rd party solution (AWS marketplace) as well architecture best practices. Amazon VPC traffic mirroring is a \u201cvirtual fiber tap\u201d that gives you direct access to the network packets flowing through your VPC. Encrypted traffic between instances? Within the AWS network, you'll be hosted in a Virtual Private Cloud (VPC) and all traffic between your instances stay within your VPC. Other customers cannot see your traffic. Configuring inter-instance traffic encryption would be the same as on-premises. Does AWS provide DDoS protection? AWS Shield Standard is automically provided to all AWS customers and AWS Shield advanced service can be bought for additional features. AWS Shield is a managed Distributed Denial of Service events protection service that safeguards applications running on AWS. How to be sure to data is really deleted You own the data inside AWS. But on delete operation the data is destroyed and wiped out. When destroying physical hardware, AWS follows the NIST 888 requirements, and it is done in a secure zone in the data center. Audits are done by 3nd party vendor, and compliances are reported in Artifact . Is serverless secured? Roles are defined to define who can push code, access it, and fine grained control of the serverless execution. All serverless managed services inherit from the underlying platform security control. As an example, Lambda execution are authenticated, authorized and are made visible to the SREs via the commong logging mechanism.","title":"Security FAQ"},{"location":"infra/security/#solution-design","text":"The most important questions to ask and address when designing AWS solution with security consideration are: who can access the resources in the account. what can access the resources, and how access is allowed. Address how to secure, web tier, app tier and database access. What firewalls to use, and where. For each compute resources address access to monitoring and logging and network environment that contain those resources. Is there any Gateway needed? Finally consider data life cycle and access control over time. What are the compliance requirements?","title":"Solution Design"},{"location":"infra/security/#blocking-ip-address","text":"At the VPC level, the best approach is to use Network ACL, when EC2 is exposed via public IP address, as illustrasted in the diagram below: Use a denial rule in NACL on specific IP address. Security group inbound rule specifies allowed only IP range, which should work, but then it blocks larger set of clients, so it will not be a solution for global application. Running a firewall on the EC2 will help, but it is more complex to administer. If an ALB is in the middle, then the EC2 security group will specify the SG of the ALB, and we are still using NACL. ALB does connection termination, so client will not be able to attack EC2. To add more control we can add a Web Application Firewall to add complex filtering on the IP @. Using a NLB, there will be no security group at the network load balancer, so traffic reaches EC2s. In this case only the NACL rules will help to protect. When using CloudFront, we need to add WAF, as NACL at the VPC level will not work, the ALB being connected to the cloudFront IP address, only (via security group). Geo restriction can be defined at the cloud front level, to deny a complete country for example.","title":"Blocking IP address"},{"location":"infra/security/#learn-more","text":"Data Safe cloud checklist Top security myth - dispelled","title":"Learn more"},{"location":"infra/storage/","text":"Storage \u00b6 AWS storage services are grouped into three categories \u2013 block storage, file storage, and object storage. File Storage \u00b6 File storage is ideal when you require centralized access to files that need to be easily shared and managed by multiple host computers. Typically, this storage is mounted onto multiple hosts, and requires file locking and integration with existing file system communication protocols. File storage systems are often supported with a network attached storage (NAS) servers. Block Storage \u00b6 Block storage splits files into fixed-size chunks of data called blocks that have their own addresses, which improve read access. Outside of the address, no additional metadata is associated with each block. Block storage in the cloud is analogous to direct-attached storage (DAS) or a storage area network (SAN). Amazon EC2 instance store provides temporary block-level storage for an instance. The storage is located on disks that are physically attached to the host computer. It is deleted when the EC2 instance is deleted. It has a high throughput and low latency I/O. Instance store is ideal if you host applications that replicate data to other EC2 instances such as Kafka or Hadoop. Object Storage \u00b6 Objects are stored in a flat structure instead of a hierarchy. We can store almost any type of data, and there is no limit to the number of objects stored, which makes it readily scalable. Amazon Elastic Block Storage EBS \u00b6 Elastic Block Store Volume is a network drive attached to the EC2 instance. It is locked to an AZ, and uses provisioned capacity in GBs and IOPS. It is HA, and can be backed up. Create a EBS while creating the EC2 instance and keep it. It is not deleted on EC2 shutdown. EBS volume can be attached to a new EC2 instance, normally there is a 1 to 1 relation between volume and EC2 instance. Except for multi-attach EBS. The maximum amount of storage you can have is 16 TB. Once logged, add a filesystem, mount to a folder and modify boot so the volume is mounted at start time. Which looks like: # List existing block storage, verify our created storage is present lsblk # Verify file system type sudo file -s /dev/xdvf # Create a ext4 file system on the device sudo mkfs -t ext4 /dev/xvdb # make a mount point sudo mkdir /data sudo mount /dev/xvdb /data # Add entry in /etc/fstab with line like: /dev/xvdb /data ext4 default,nofail 0 2 EBS is already a redundant storage, replicated within an AZ. EC2 instance has a logical volume that can be attached to two or more EBS RAID 0 volumes, where write operations are distributed among them. It is used to increate IOPS without any fault tolerance. If one fails, we lost data. It could be used for database with built-in replication mechanism or Kafka node. RAID 1 is for better fault tolerance: a write operation is going to all attached volumes. Volume types \u00b6 When creating EC2 instances, you can only use the following EBS volume types as boot volumes: gp2, gp3, io1, io2, and Magnetic (Standard) gp2 or gp3 : SSD, used for most workload up to 16 TB at 16000 IOPS max (3 IOPS per GB brustable to 3000). io 1 or io 2 : critical app with large database workloads. max ratio 50:1 IOPS/GB. Min 100 iops and 4G to 16T. 99.9% durability and even 99.999% for io2. EBS Provisioned IOPS SSD (io2 Block Express) is the highest-performance SSD volume designed for business-critical latency-sensitive transactional workloads. st 1 : HDD. Streaming workloads requiring consistent, fast throughput at a low price. For Big data, Data warehouses, Log processing. Up to 16 TiB. 99.9% durability. sc 1 : throughput oriented storage. 500G- 16T, 500MiB/s. Max IOPs at 250. Used for cold HDD, and infrequently accessed data. 99.9% durability. Encryption has a minimum impact on latency. It encrypts data at rest and during snapshots. * Provisioned IOPS (PIOPS) SSD: used for critical apps with sustained IOPS performance, even more than 16k IOPS. Instance store is a volume attached to the instance, used for root folder. It is a ephemeral storage but has millions read per s and 700k write IOPS. It provides the best disk performance and can be used to have high performance cache for our applications. If we need to run a high-performance database that requires an IOPS of 210,000 for its underlying filesystem, we need instance store and DB replication in place. Example of use cases App requires up to 400 GB of storage for temporary data that is discarded after usage. The application requires approximately 40,000 random IOPS to perform the work on file. => Prefer a SSD-Backed Storage Optimized (i2) EC2 instances to get more than 365,000 random IOPS. The instance store has no additional cost, compared with the regular hourly cost of the instance. Provisioned IOPS SSD (io1 or io2) EBS volumes can deliver more than the 40,000 IOPS that are required in the scenario. However, this solution is not as cost-effective as an instance store because Amazon EBS adds cost to the hourly instance rate. This solution provides persistence of data beyond the lifecycle of the instance, but persistence is not required in this use case. A database must provide at least 40 GiB of storage capacity and 1,000 IOPS. The most effective storage is gp2 with 334 GB storage: Baseline I/O performance for General Purpose SSD storage is 3 IOPS for each GiB. For 334 GiB of storage, the baseline performance would be 1,002 IOPS. Additionally, General Purpose SSD storage is more cost-effective than Provisioned IOPS storage. Snapshots \u00b6 EBS snapshots are incremental backups that only save the blocks on the volume that have changed after your most recent snapshot. Used to backup disk at any point of time of a volume and stores it on S3. Snapshot Lifecycle policy helps to create snapshot with scheduling it by defining policies. To move a volume to another AZ or data center we can create a volume from a snapshot. EBS snapshots can be used to create multiple new volumes, whether they\u2019re in the same Availability Zone or a different one For a consistent snapshot of an EBS Volume, you need to ensure the application flushes any cached data to disk and no other write I/O is performed by the file system on that volume. Once that is taken care of, you can issue a snapshot command. The snapshot command needs only a couple of seconds to capture a point-in-time. You can start using the volume after this. The actual data backup happens in the background, and you don\u2019t have to wait for the data copy to complete. EBS Multi-attach \u00b6 Only for io1 or io2 EBS type, a volume can be attached to multiple EC2 instances (up to 16) running in the same AZ. Each instance has full R/W permission. The file system must be cluster aware. S3 - Simple Storage Service \u00b6 Amazon S3 allows people to store objects (files) in buckets (directories), which must have a globally unique name (cross users!). They are defined at the region level. Object in a bucket, is referenced as a key which can be seen as a file path in a file system. The max size for an object is 5 TB but big file needs to be uploaded in multi-part using 5GB max size. S3 has 11 9's high durability of objects across multiple AZ (At least 3). Availability varies with storage class, S3 standard is 99.99%. S3 supports strong consistency for all operations with a read-after-write consistency. S3 supports versioning at the bucket level. So file can be restored from previous version, and even deleted file can be retrieved from a previous version. Within the S3 console we will see all buckets in one view (its is a global service). But the buckets are created within a region and are local to the region. Use cases \u00b6 Backup and restore Disaster Recovery Archive Data lakes and big data analytics Hybrid cloud storage: seamless connection between on-premises applications and S3 with AWS Storage Gateway. Cloud-native application data Media hosting Software delivery Static website GETTING started Security control \u00b6 By default a bucket access is not public, see the Block Public Access setting. Can be enforced at the account level and need to be disable at the account level, before doing it at the bucket level (amazon S3 > block public access settings for this account > edit block public access settings for this account). To control access with policies we need to disable this, and then define Bucket policy. S3 Bucket Policy : is security policy defined in S3 console, and also allows cross-account access control. Can be set at the bucket or object level. Policies can be attached to users, groups, or Amazon S3 buckets, enabling centralized management of permissions. With bucket policies, you can grant users within your AWS Account or other AWS Accounts access to your Amazon S3 resources. Explicit DENY in an IAM policy will take precedence over a bucket policy permission . Define policies from examples or using policy generator tool . And copy paste the generated policy By default, when another AWS account uploads an object to your S3 bucket, that account (the object writer) owns the object, has access to it, and can grant other users access to it through ACLs. Bucket owner can take ownership of all objects. It is recommended to disable ACL and use identity and bucket policies. Objects can also be encrypted, and different mechanisms are available: SSE-S3 : server-side encrypted S3 objects using keys handled & managed and own by AWS using AES-256 protocol must set x-amz-server-side-encryption: \"AES256\" header in the POST request to upload the file. SSE-KMS : leverage AWS Key Management Service to manage encryption keys. Use x-amz-server-side-encryption: \"aws:kms\" header in POST request. Server side encrypted. It gives user control of the key rotation policy and audit trail with CloudTrail . SSE-C : when we want to manage our own encryption keys. Server-side encrypted. Encryption key must be provided in HTTPS headers, for every HTTPS request made. HTTPS is mandatory. Client Side Encryption : encrypt before sending objects to S3. Encryption can be done at the bucket level, or using bucker policies to refuse any PUT calls on S3 object without encryption header. An IAM principal can access an S3 object if the user IAM permissions allow it or the resource policy allows it and there is no explicit deny. S3 Website hosting \u00b6 We can have static website on S3. Once html pages are uploaded, setting the properties as static web site from the bucket. The bucket needs to be public, and have a security policy to allow any user to GetObject action. The URL may look like: <bucket-name>.s3-website.<AWS-region>.amazonaws.com Example of pushing a mkdocs site to s3 after enabling public access mkdocs build aws s3 sync ./site s3://jbcodeforce-aws-studies # The url is at the bottom of the bucket in the website under the Bucket website endpoint for example: http://jbcodeforce-aws-studies.s3-website-us-west-2.amazonaws.com Cross Origin Resource Sharing CORS : The web browser requests won\u2019t be fulfilled unless the other origin allows for the requests, using CORS Headers Access-Control-Allow-Origin . If a client does a cross-origin request on our S3 bucket, we need to enable the correct CORS headers: this is done by adding a security policy with CORS configuration like: <CORSConfiguration> <CORSRule> <AllowedOrigin> enter-bucket-url-here </AllowedOrigin> <AllowedMethod> GET </AllowedMethod> <MaxAgeSeconds> 3000 </MaxAgeSeconds> <AllowedHeader> Authorization </AllowedHeader> </CORSRule> </CORSConfiguration> S3 replication \u00b6 Once versioning enabled on source and target, a bucket can be replicated in the same region (SRR) or cross regions (CRR). S3 replication is done on at least 3 AZs. One DC down does not impact S3 availability. The replication is done asynchronously. SRR is for log aggregation for example or live replication between production and test, while CRR is used for compliance and DR or replication across AWS accounts. Delete operations are not replicated. Must give proper IAM permissions to S3. When replication is set, only new objects are replicated. To replicate exiting objects use S3 Batch Replication. S3 Batch Replication provides you a way to replicate objects that existed before a replication configuration was in place, objects that have previously been replicated, and objects that have failed replication By default delete markers are not replicated by with the advanced options we can enable it. The AWS S3 sync command uses the CopyObject APIs to copy objects between S3 buckets in same region. The sync command lists the source and target buckets to identify objects that are in the source bucket but that aren't in the target bucket. The command also identifies objects in the source bucket that have different LastModified dates than the objects that are in the target bucket. The sync command on a versioned bucket copies only the current version of the object\u2014previous versions aren't copied S3 Storage classes \u00b6 When uploading a document into an existing bucket, we can specify the storage class to keep data over time. Different levels are offered with different cost and SLA. With Intelligent Tiering , S3 automates the process of moving objects to the most cost-effective access tier based on access frequency. With One Zone IA , there is a risk of data loss in the event of availability zone destruction, and some objects may be unavailable when an AZ goes down. Standard is the most expensive storage option. Standard IA has a separate retrieval fee. Amazon Glacier is for archiving, like writing to tapes. The pricing includes storage and object retrieval cost. Glacier Deep Archive (also named Bulk) is the lowest cost storage option for long-term archival and digital preservation. Deep Archive can take several hours (from 12 to 48 hours) depending on the retrieval tier. We can transition objects between storage classes. For infrequently accessed object, move them to STANDARD_IA. For archive objects, that we don\u2019t need in real-time, use GLACIER or DEEP_ARCHIVE. Moving objects can be automated using a lifecycle configuration. At the bucket level, a user can define lifecycle rules for when to transition an object to another storage class. and To prevent accidental file deletions, we can setup MFA Delete to use MFA tokens before deleting objects. To improve performance, a big file can be split and then uploaded with local connection to the closed edge access and then use AWS private network to copy between buckets in different region. In case of infinished parts use S3 Lifecycle policy to automate old/unfinished parts deletion. S3 to Kafka lab Storage Class Analysis can continuously monitor your bucket and track how your objects are accessed over time. This tool generates detailed reports on the percentage of data retrieved and by age groups. You can use this report to manage lifecycle policies. Storage Lens provides a dashboard on all S3 activities and is automatically enabled. Other features \u00b6 Secure FTP : server to let you send file via SFTP. Requester Pay : The requester (AWS authenticated) of the data pay for the cost of the request and the data download from the bucket, not the owner. Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Transfer Acceleration takes advantage of Amazon CloudFront\u2019s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path. Pre-signed URL : share object with URL with temporary access. Can be done with the command: aws s3 presign . Up to 168 hours valid. S3 Select and Glacier Select : to retrieve a smaller set of data from an object using server-side SQL. Can filter by rows and columns. 80% cheaper and 400% faster as it uses less network transfer and less CPU on client side. Event Notifications : on actions like S3:ObjectCreated, S3:ObjectRemoved, S3:ObjectRestore. Can be combined with name filtering. Events may be sent to SNS, SQS, Lambda function, and EventBridge. Amazon Macie : is a machine learning security service to discover, classify and protect sensitive data stored in S3. S3 Object lock : to meet regulatory requirements of write once read many storage. Use Legal Hold to prevent an object or its versions from being overwritten or deleted indefinitely and gives you the ability to remove it manually. S3 Byte-Range Fetches : parallelize GET by requesting specific byte ranges. Used to speed up download or to download partial data. S3 Batch operations : perform bulk operations on existing S3 objects with a single request. To get the list of object, use S3 Inventory . Could not integrate custom code. Server Access Logs : used for audit purpose to track any request made to S3 in the same region, from any account. Logs are saved in another bucket. S3 Glacier Vault Lock : Adopt a Write Once Read Many model, by creating a Vault Lock Policy. Data will never be deleted. S3 Access points: Access points are named network endpoints that are attached to buckets that you can use to perform S3 object operations. An Access Point alias provides the same functionality as an Access Point ARN and can be substituted for use anywhere an S3 bucket name is normally used for data access. FAQ \u00b6 The last one MB of each file in a bucket contains summary information that you want to expose in a search what to use? Byte-Range fetch allows you to read only a portion of data from the object. Since the summary is a small part of each object, it is efficient to directly read the summary rather than downloading an entire object from S3. Pricing factors Frequency of access, storage cost, retrieval cost and retrieval time. The S3 Intelligent Tiering automatically changes storage class depending on usage to optimize cost. S3 lifecycle is based on age and can be defined with rules. Expected performance? S3 automatically scales to high request rates and latency around 100 to 200ms. 5500 GET/HEAD requests per s per prefix in a bucket. 3500 PUT/COPY/POST/DELETE. When uploading files from internet host, it is recommended to upload to AWS edge location and then use AWS private backbone to move file to S3 bucket in target region. This will limit internet traffic and cost. How to be informed if an object is restored to S3 from Glacier? The Amazon S3 notification feature enables you to receive notifications when certain events happen in your bucket. To enable notifications, you must first add a notification configuration that identifies the events you want Amazon S3 to publish and the destinations where you want Amazon S3 to send the notifications. How to upload local file to s3 bucket using CLI? We can use python and the boto3 library or aws s3 CLI. Be sure to have a IAM user (e.g. s3admin) with AmazonS3FullAccess managed policy. The aws config may have added access Ky and secret, may be in a dedicated profile. aws s3 cp $PWD /companies.csv s3://jb-data-set/ --profile s3admin For boto3 example see code under big-data-tenant-analytics project How to retrieve in minutes up to 250MB of archive from Glacier? Expedited retrievals allow you to quickly access () 1 to 5 minutes) your data when occasional urgent requests for a subset of archives are required. It provides up to 150 MB/s of retrieval throughput. If you require access to Expedited retrievals under all circumstances, you must purchase provisioned retrieval capacity . What is provisioned capacity in Glacier? Provisioned capacity ensures that your retrieval capacity for expedited retrievals is available when you need it. Each unit of capacity provides that at least three expedited retrievals can be performed every five minutes and provides up to 150 MB/s of retrieval throughput. Amazon AppFlow \u00b6 AppFlow offers a fully managed service for easily automating the bidirectional exchange of data to SaaS vendors from AWS services like Amazon S3. This helps avoid resource constraints. Elastic File System (EFS) \u00b6 Fully managed NFS file system. FAQ for multi AZs. (3x gp2 cost), controlled by using security group. This security group needs to add in bound rule of type NFS connected / linked to the SG of the EC2. Only Linux based AMI. POSIX filesystem. Encryption is supported using KMS. 1000 concurrent clients. 10GB+/s throughput, bursting or provisioned, grow to petabyte. Support different performance mode, like max I/O or general purpose Select regional (standard or default) or one zone availabiltiy and durability. Billed for what you use. Support storage tiers to move files after n days. Used on infrequent access, so lifecycle management move file to EFS-IA. Use amazon EFS util tool in each EC2 instance to mount the EFS to a target mount point. Is defined in a subnet, so the EC2 needs to specify in which subnet it runs. Snowball \u00b6 Move TB to PB of data in and out AWS using physical device to ship data as doing over network will take a lot of time, and may fail. The Snowball Edge device has 100TB and compute power to do some local processing on data. With Compute Optimized version there are 52 vCPUs, 200GB of RAM, optional GPU, 42TB. And for Storage Optimized version, 40 vCPUs,, 80 GB RAM, and object storage clustering. You can use Snowball Edge Storage Optimized if you have a large backlog of data to transfer or if you frequently collect data that needs to be transferred to AWS and your storage is in an area where high-bandwidth internet connections are not available or cost-prohibitive. Snowcone : smaller portable, secured, rugged, for harsh environments. Limited to 8TB. We can use AWS DataSync to sned data. 2 CPUs, 4GB of mem. SnowMobile is a truck with 100 PB capacity. Once on site, it is transferred to S3. Can be used for Edge Computing when there is no internet access. All can run EC2 instances and AWS lambda function using AWS IoT Greengrass. for local configuration of the snowball, there is a the AWS OpsHub app. FSx \u00b6 A managed service for file system technology from 3nd party vendors like Lustre, NetApp ONTAP, Windows File Server, OpenZFS... For Windows FS, supports SMB protocol and NTFS. Can be mounted to Linux EC2. 10s GB/s IOPS and 100s PB of data. Can be configured on multi AZ. Data is backed-up on S3 Lustre is a linux clustered FS and supports High Performance Computing, POSIX... sub ms latency, 100s GB/s IOPS. NetApp ONTAP: supports NFS, SMB, iSCSI protocols. Supports storage auto scaling, and point in time instantaneous cloning. OpenZFS compatibles with NFS, scale to 1 million IOPS with < 0,5ms latency, and point in time instantaneous cloning. Deployment options: Scratch FS is used for temporary storage with no replication. Supports High burst Persistent FS: long-term storage, replicaed in same AZ Hybrid cloud storage \u00b6 AWS Storage Gateway exposes an API in front of S3 to provide on-premises applications with access to virtually unlimited cloud storage. Three gateway types: file : FSx or S3 . S3 buckets are accessible using NFS or SMB protocols. Controlled access via IAM roles. File gateway is installed on-premise and communicate with AWS via HTTPS. FSx, storage gateways brings cache of last accessed files. volume : this is a block storage using iSCSI protocol. On-premise and visible as a local volume backed by S3. Two volume types: Cached volumes: Your primary data is stored in Amazon S3 while your frequently acccessed data is retained locally in the cache for low-latency access. Stored volumes: Your entire dataset is stored locally while also being asynchronously backed up to Amazon S3. tape : same approach but with virtual tape library. Can go to S3 and Glacier. Works with existing tape software. Hardware appliance to run a Storage gateway in the on-premises data center. Transfer Family \u00b6 To transfer data with FTP, FTPS, SFTP protocols to AWS Storage services like S3, EFS DataSync \u00b6 AWS DataSync is an online data transfer service that simplifies, automates, and accelerates copying large amounts of data between on-premises storage systems and AWS Storage services, as well as between AWS Storage services. Move a large amount of data to and from on-premises (using agent) to AWS, or to AWS to AWS different storage services. Can be used for: Data Migration with automatic encryption and data integrity validation. Archive cold data. Data protection. Data movement for timely in-cloud processing. Replication tasks can be scheduled. It keeps the metadata and permissions about the file. One agent task can get 10 GB/s As an example to support the above architecture, we can configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private Virtual InterFace (VIF). Set up a DataSync scheduled task to send the video files to the EFS file system every 24 hours. The DataSync agent is deployed as a virtual machine that should be deployed on-premises in the same LAN as your source storage to minimize the distance traveled. Transferring files from on-premises to AWS and back without leaving your VPC using AWS DataSync . DatSynch FAQ . Public and Private interface . Storage comparison \u00b6 S3: Object Storage. Glacier: Object Archival. EFS: When we need distributed, highly resilient storage, using Network File System for Linux instances, POSIX filesystem. Across multiple AZs. FSx for Windows: Network File System for Windows servers. Central storage for Windows based applications. FSx for Lustre: High Performance Computing (HPC) Linux file system. It can store data directly to S3 too. FSx for NetApp: High OS compatibility. FSx for OpenZFS: for ZFS compatibility. EBS volumes: Network storage for one EC2 instance at a time. Instance Storage: Physical storage for your EC2 instance (high IOPS). But Ephemeral. Storage Gateway: File Gateway, Volume Gateway (cache & stored), Tape Gateway. Snowcone, Snowball / Snowmobile: to move large amount of data to the cloud, physically. Database: for specific workloads, usually with indexing and querying. DataSync: schedule data sync from on-premises to AWS or AWS to AWS services.","title":"S3-Storage"},{"location":"infra/storage/#storage","text":"AWS storage services are grouped into three categories \u2013 block storage, file storage, and object storage.","title":"Storage"},{"location":"infra/storage/#file-storage","text":"File storage is ideal when you require centralized access to files that need to be easily shared and managed by multiple host computers. Typically, this storage is mounted onto multiple hosts, and requires file locking and integration with existing file system communication protocols. File storage systems are often supported with a network attached storage (NAS) servers.","title":"File Storage"},{"location":"infra/storage/#block-storage","text":"Block storage splits files into fixed-size chunks of data called blocks that have their own addresses, which improve read access. Outside of the address, no additional metadata is associated with each block. Block storage in the cloud is analogous to direct-attached storage (DAS) or a storage area network (SAN). Amazon EC2 instance store provides temporary block-level storage for an instance. The storage is located on disks that are physically attached to the host computer. It is deleted when the EC2 instance is deleted. It has a high throughput and low latency I/O. Instance store is ideal if you host applications that replicate data to other EC2 instances such as Kafka or Hadoop.","title":"Block Storage"},{"location":"infra/storage/#object-storage","text":"Objects are stored in a flat structure instead of a hierarchy. We can store almost any type of data, and there is no limit to the number of objects stored, which makes it readily scalable.","title":"Object Storage"},{"location":"infra/storage/#amazon-elastic-block-storage-ebs","text":"Elastic Block Store Volume is a network drive attached to the EC2 instance. It is locked to an AZ, and uses provisioned capacity in GBs and IOPS. It is HA, and can be backed up. Create a EBS while creating the EC2 instance and keep it. It is not deleted on EC2 shutdown. EBS volume can be attached to a new EC2 instance, normally there is a 1 to 1 relation between volume and EC2 instance. Except for multi-attach EBS. The maximum amount of storage you can have is 16 TB. Once logged, add a filesystem, mount to a folder and modify boot so the volume is mounted at start time. Which looks like: # List existing block storage, verify our created storage is present lsblk # Verify file system type sudo file -s /dev/xdvf # Create a ext4 file system on the device sudo mkfs -t ext4 /dev/xvdb # make a mount point sudo mkdir /data sudo mount /dev/xvdb /data # Add entry in /etc/fstab with line like: /dev/xvdb /data ext4 default,nofail 0 2 EBS is already a redundant storage, replicated within an AZ. EC2 instance has a logical volume that can be attached to two or more EBS RAID 0 volumes, where write operations are distributed among them. It is used to increate IOPS without any fault tolerance. If one fails, we lost data. It could be used for database with built-in replication mechanism or Kafka node. RAID 1 is for better fault tolerance: a write operation is going to all attached volumes.","title":"Amazon Elastic Block Storage EBS"},{"location":"infra/storage/#volume-types","text":"When creating EC2 instances, you can only use the following EBS volume types as boot volumes: gp2, gp3, io1, io2, and Magnetic (Standard) gp2 or gp3 : SSD, used for most workload up to 16 TB at 16000 IOPS max (3 IOPS per GB brustable to 3000). io 1 or io 2 : critical app with large database workloads. max ratio 50:1 IOPS/GB. Min 100 iops and 4G to 16T. 99.9% durability and even 99.999% for io2. EBS Provisioned IOPS SSD (io2 Block Express) is the highest-performance SSD volume designed for business-critical latency-sensitive transactional workloads. st 1 : HDD. Streaming workloads requiring consistent, fast throughput at a low price. For Big data, Data warehouses, Log processing. Up to 16 TiB. 99.9% durability. sc 1 : throughput oriented storage. 500G- 16T, 500MiB/s. Max IOPs at 250. Used for cold HDD, and infrequently accessed data. 99.9% durability. Encryption has a minimum impact on latency. It encrypts data at rest and during snapshots. * Provisioned IOPS (PIOPS) SSD: used for critical apps with sustained IOPS performance, even more than 16k IOPS. Instance store is a volume attached to the instance, used for root folder. It is a ephemeral storage but has millions read per s and 700k write IOPS. It provides the best disk performance and can be used to have high performance cache for our applications. If we need to run a high-performance database that requires an IOPS of 210,000 for its underlying filesystem, we need instance store and DB replication in place. Example of use cases App requires up to 400 GB of storage for temporary data that is discarded after usage. The application requires approximately 40,000 random IOPS to perform the work on file. => Prefer a SSD-Backed Storage Optimized (i2) EC2 instances to get more than 365,000 random IOPS. The instance store has no additional cost, compared with the regular hourly cost of the instance. Provisioned IOPS SSD (io1 or io2) EBS volumes can deliver more than the 40,000 IOPS that are required in the scenario. However, this solution is not as cost-effective as an instance store because Amazon EBS adds cost to the hourly instance rate. This solution provides persistence of data beyond the lifecycle of the instance, but persistence is not required in this use case. A database must provide at least 40 GiB of storage capacity and 1,000 IOPS. The most effective storage is gp2 with 334 GB storage: Baseline I/O performance for General Purpose SSD storage is 3 IOPS for each GiB. For 334 GiB of storage, the baseline performance would be 1,002 IOPS. Additionally, General Purpose SSD storage is more cost-effective than Provisioned IOPS storage.","title":"Volume types"},{"location":"infra/storage/#snapshots","text":"EBS snapshots are incremental backups that only save the blocks on the volume that have changed after your most recent snapshot. Used to backup disk at any point of time of a volume and stores it on S3. Snapshot Lifecycle policy helps to create snapshot with scheduling it by defining policies. To move a volume to another AZ or data center we can create a volume from a snapshot. EBS snapshots can be used to create multiple new volumes, whether they\u2019re in the same Availability Zone or a different one For a consistent snapshot of an EBS Volume, you need to ensure the application flushes any cached data to disk and no other write I/O is performed by the file system on that volume. Once that is taken care of, you can issue a snapshot command. The snapshot command needs only a couple of seconds to capture a point-in-time. You can start using the volume after this. The actual data backup happens in the background, and you don\u2019t have to wait for the data copy to complete.","title":"Snapshots"},{"location":"infra/storage/#ebs-multi-attach","text":"Only for io1 or io2 EBS type, a volume can be attached to multiple EC2 instances (up to 16) running in the same AZ. Each instance has full R/W permission. The file system must be cluster aware.","title":"EBS Multi-attach"},{"location":"infra/storage/#s3-simple-storage-service","text":"Amazon S3 allows people to store objects (files) in buckets (directories), which must have a globally unique name (cross users!). They are defined at the region level. Object in a bucket, is referenced as a key which can be seen as a file path in a file system. The max size for an object is 5 TB but big file needs to be uploaded in multi-part using 5GB max size. S3 has 11 9's high durability of objects across multiple AZ (At least 3). Availability varies with storage class, S3 standard is 99.99%. S3 supports strong consistency for all operations with a read-after-write consistency. S3 supports versioning at the bucket level. So file can be restored from previous version, and even deleted file can be retrieved from a previous version. Within the S3 console we will see all buckets in one view (its is a global service). But the buckets are created within a region and are local to the region.","title":"S3 - Simple Storage Service"},{"location":"infra/storage/#use-cases","text":"Backup and restore Disaster Recovery Archive Data lakes and big data analytics Hybrid cloud storage: seamless connection between on-premises applications and S3 with AWS Storage Gateway. Cloud-native application data Media hosting Software delivery Static website GETTING started","title":"Use cases"},{"location":"infra/storage/#security-control","text":"By default a bucket access is not public, see the Block Public Access setting. Can be enforced at the account level and need to be disable at the account level, before doing it at the bucket level (amazon S3 > block public access settings for this account > edit block public access settings for this account). To control access with policies we need to disable this, and then define Bucket policy. S3 Bucket Policy : is security policy defined in S3 console, and also allows cross-account access control. Can be set at the bucket or object level. Policies can be attached to users, groups, or Amazon S3 buckets, enabling centralized management of permissions. With bucket policies, you can grant users within your AWS Account or other AWS Accounts access to your Amazon S3 resources. Explicit DENY in an IAM policy will take precedence over a bucket policy permission . Define policies from examples or using policy generator tool . And copy paste the generated policy By default, when another AWS account uploads an object to your S3 bucket, that account (the object writer) owns the object, has access to it, and can grant other users access to it through ACLs. Bucket owner can take ownership of all objects. It is recommended to disable ACL and use identity and bucket policies. Objects can also be encrypted, and different mechanisms are available: SSE-S3 : server-side encrypted S3 objects using keys handled & managed and own by AWS using AES-256 protocol must set x-amz-server-side-encryption: \"AES256\" header in the POST request to upload the file. SSE-KMS : leverage AWS Key Management Service to manage encryption keys. Use x-amz-server-side-encryption: \"aws:kms\" header in POST request. Server side encrypted. It gives user control of the key rotation policy and audit trail with CloudTrail . SSE-C : when we want to manage our own encryption keys. Server-side encrypted. Encryption key must be provided in HTTPS headers, for every HTTPS request made. HTTPS is mandatory. Client Side Encryption : encrypt before sending objects to S3. Encryption can be done at the bucket level, or using bucker policies to refuse any PUT calls on S3 object without encryption header. An IAM principal can access an S3 object if the user IAM permissions allow it or the resource policy allows it and there is no explicit deny.","title":"Security control"},{"location":"infra/storage/#s3-website-hosting","text":"We can have static website on S3. Once html pages are uploaded, setting the properties as static web site from the bucket. The bucket needs to be public, and have a security policy to allow any user to GetObject action. The URL may look like: <bucket-name>.s3-website.<AWS-region>.amazonaws.com Example of pushing a mkdocs site to s3 after enabling public access mkdocs build aws s3 sync ./site s3://jbcodeforce-aws-studies # The url is at the bottom of the bucket in the website under the Bucket website endpoint for example: http://jbcodeforce-aws-studies.s3-website-us-west-2.amazonaws.com Cross Origin Resource Sharing CORS : The web browser requests won\u2019t be fulfilled unless the other origin allows for the requests, using CORS Headers Access-Control-Allow-Origin . If a client does a cross-origin request on our S3 bucket, we need to enable the correct CORS headers: this is done by adding a security policy with CORS configuration like: <CORSConfiguration> <CORSRule> <AllowedOrigin> enter-bucket-url-here </AllowedOrigin> <AllowedMethod> GET </AllowedMethod> <MaxAgeSeconds> 3000 </MaxAgeSeconds> <AllowedHeader> Authorization </AllowedHeader> </CORSRule> </CORSConfiguration>","title":"S3 Website hosting"},{"location":"infra/storage/#s3-replication","text":"Once versioning enabled on source and target, a bucket can be replicated in the same region (SRR) or cross regions (CRR). S3 replication is done on at least 3 AZs. One DC down does not impact S3 availability. The replication is done asynchronously. SRR is for log aggregation for example or live replication between production and test, while CRR is used for compliance and DR or replication across AWS accounts. Delete operations are not replicated. Must give proper IAM permissions to S3. When replication is set, only new objects are replicated. To replicate exiting objects use S3 Batch Replication. S3 Batch Replication provides you a way to replicate objects that existed before a replication configuration was in place, objects that have previously been replicated, and objects that have failed replication By default delete markers are not replicated by with the advanced options we can enable it. The AWS S3 sync command uses the CopyObject APIs to copy objects between S3 buckets in same region. The sync command lists the source and target buckets to identify objects that are in the source bucket but that aren't in the target bucket. The command also identifies objects in the source bucket that have different LastModified dates than the objects that are in the target bucket. The sync command on a versioned bucket copies only the current version of the object\u2014previous versions aren't copied","title":"S3 replication"},{"location":"infra/storage/#s3-storage-classes","text":"When uploading a document into an existing bucket, we can specify the storage class to keep data over time. Different levels are offered with different cost and SLA. With Intelligent Tiering , S3 automates the process of moving objects to the most cost-effective access tier based on access frequency. With One Zone IA , there is a risk of data loss in the event of availability zone destruction, and some objects may be unavailable when an AZ goes down. Standard is the most expensive storage option. Standard IA has a separate retrieval fee. Amazon Glacier is for archiving, like writing to tapes. The pricing includes storage and object retrieval cost. Glacier Deep Archive (also named Bulk) is the lowest cost storage option for long-term archival and digital preservation. Deep Archive can take several hours (from 12 to 48 hours) depending on the retrieval tier. We can transition objects between storage classes. For infrequently accessed object, move them to STANDARD_IA. For archive objects, that we don\u2019t need in real-time, use GLACIER or DEEP_ARCHIVE. Moving objects can be automated using a lifecycle configuration. At the bucket level, a user can define lifecycle rules for when to transition an object to another storage class. and To prevent accidental file deletions, we can setup MFA Delete to use MFA tokens before deleting objects. To improve performance, a big file can be split and then uploaded with local connection to the closed edge access and then use AWS private network to copy between buckets in different region. In case of infinished parts use S3 Lifecycle policy to automate old/unfinished parts deletion. S3 to Kafka lab Storage Class Analysis can continuously monitor your bucket and track how your objects are accessed over time. This tool generates detailed reports on the percentage of data retrieved and by age groups. You can use this report to manage lifecycle policies. Storage Lens provides a dashboard on all S3 activities and is automatically enabled.","title":"S3 Storage classes"},{"location":"infra/storage/#other-features","text":"Secure FTP : server to let you send file via SFTP. Requester Pay : The requester (AWS authenticated) of the data pay for the cost of the request and the data download from the bucket, not the owner. Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Transfer Acceleration takes advantage of Amazon CloudFront\u2019s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path. Pre-signed URL : share object with URL with temporary access. Can be done with the command: aws s3 presign . Up to 168 hours valid. S3 Select and Glacier Select : to retrieve a smaller set of data from an object using server-side SQL. Can filter by rows and columns. 80% cheaper and 400% faster as it uses less network transfer and less CPU on client side. Event Notifications : on actions like S3:ObjectCreated, S3:ObjectRemoved, S3:ObjectRestore. Can be combined with name filtering. Events may be sent to SNS, SQS, Lambda function, and EventBridge. Amazon Macie : is a machine learning security service to discover, classify and protect sensitive data stored in S3. S3 Object lock : to meet regulatory requirements of write once read many storage. Use Legal Hold to prevent an object or its versions from being overwritten or deleted indefinitely and gives you the ability to remove it manually. S3 Byte-Range Fetches : parallelize GET by requesting specific byte ranges. Used to speed up download or to download partial data. S3 Batch operations : perform bulk operations on existing S3 objects with a single request. To get the list of object, use S3 Inventory . Could not integrate custom code. Server Access Logs : used for audit purpose to track any request made to S3 in the same region, from any account. Logs are saved in another bucket. S3 Glacier Vault Lock : Adopt a Write Once Read Many model, by creating a Vault Lock Policy. Data will never be deleted. S3 Access points: Access points are named network endpoints that are attached to buckets that you can use to perform S3 object operations. An Access Point alias provides the same functionality as an Access Point ARN and can be substituted for use anywhere an S3 bucket name is normally used for data access.","title":"Other features"},{"location":"infra/storage/#faq","text":"The last one MB of each file in a bucket contains summary information that you want to expose in a search what to use? Byte-Range fetch allows you to read only a portion of data from the object. Since the summary is a small part of each object, it is efficient to directly read the summary rather than downloading an entire object from S3. Pricing factors Frequency of access, storage cost, retrieval cost and retrieval time. The S3 Intelligent Tiering automatically changes storage class depending on usage to optimize cost. S3 lifecycle is based on age and can be defined with rules. Expected performance? S3 automatically scales to high request rates and latency around 100 to 200ms. 5500 GET/HEAD requests per s per prefix in a bucket. 3500 PUT/COPY/POST/DELETE. When uploading files from internet host, it is recommended to upload to AWS edge location and then use AWS private backbone to move file to S3 bucket in target region. This will limit internet traffic and cost. How to be informed if an object is restored to S3 from Glacier? The Amazon S3 notification feature enables you to receive notifications when certain events happen in your bucket. To enable notifications, you must first add a notification configuration that identifies the events you want Amazon S3 to publish and the destinations where you want Amazon S3 to send the notifications. How to upload local file to s3 bucket using CLI? We can use python and the boto3 library or aws s3 CLI. Be sure to have a IAM user (e.g. s3admin) with AmazonS3FullAccess managed policy. The aws config may have added access Ky and secret, may be in a dedicated profile. aws s3 cp $PWD /companies.csv s3://jb-data-set/ --profile s3admin For boto3 example see code under big-data-tenant-analytics project How to retrieve in minutes up to 250MB of archive from Glacier? Expedited retrievals allow you to quickly access () 1 to 5 minutes) your data when occasional urgent requests for a subset of archives are required. It provides up to 150 MB/s of retrieval throughput. If you require access to Expedited retrievals under all circumstances, you must purchase provisioned retrieval capacity . What is provisioned capacity in Glacier? Provisioned capacity ensures that your retrieval capacity for expedited retrievals is available when you need it. Each unit of capacity provides that at least three expedited retrievals can be performed every five minutes and provides up to 150 MB/s of retrieval throughput.","title":"FAQ"},{"location":"infra/storage/#amazon-appflow","text":"AppFlow offers a fully managed service for easily automating the bidirectional exchange of data to SaaS vendors from AWS services like Amazon S3. This helps avoid resource constraints.","title":"Amazon AppFlow"},{"location":"infra/storage/#elastic-file-system-efs","text":"Fully managed NFS file system. FAQ for multi AZs. (3x gp2 cost), controlled by using security group. This security group needs to add in bound rule of type NFS connected / linked to the SG of the EC2. Only Linux based AMI. POSIX filesystem. Encryption is supported using KMS. 1000 concurrent clients. 10GB+/s throughput, bursting or provisioned, grow to petabyte. Support different performance mode, like max I/O or general purpose Select regional (standard or default) or one zone availabiltiy and durability. Billed for what you use. Support storage tiers to move files after n days. Used on infrequent access, so lifecycle management move file to EFS-IA. Use amazon EFS util tool in each EC2 instance to mount the EFS to a target mount point. Is defined in a subnet, so the EC2 needs to specify in which subnet it runs.","title":"Elastic File System (EFS)"},{"location":"infra/storage/#snowball","text":"Move TB to PB of data in and out AWS using physical device to ship data as doing over network will take a lot of time, and may fail. The Snowball Edge device has 100TB and compute power to do some local processing on data. With Compute Optimized version there are 52 vCPUs, 200GB of RAM, optional GPU, 42TB. And for Storage Optimized version, 40 vCPUs,, 80 GB RAM, and object storage clustering. You can use Snowball Edge Storage Optimized if you have a large backlog of data to transfer or if you frequently collect data that needs to be transferred to AWS and your storage is in an area where high-bandwidth internet connections are not available or cost-prohibitive. Snowcone : smaller portable, secured, rugged, for harsh environments. Limited to 8TB. We can use AWS DataSync to sned data. 2 CPUs, 4GB of mem. SnowMobile is a truck with 100 PB capacity. Once on site, it is transferred to S3. Can be used for Edge Computing when there is no internet access. All can run EC2 instances and AWS lambda function using AWS IoT Greengrass. for local configuration of the snowball, there is a the AWS OpsHub app.","title":"Snowball"},{"location":"infra/storage/#fsx","text":"A managed service for file system technology from 3nd party vendors like Lustre, NetApp ONTAP, Windows File Server, OpenZFS... For Windows FS, supports SMB protocol and NTFS. Can be mounted to Linux EC2. 10s GB/s IOPS and 100s PB of data. Can be configured on multi AZ. Data is backed-up on S3 Lustre is a linux clustered FS and supports High Performance Computing, POSIX... sub ms latency, 100s GB/s IOPS. NetApp ONTAP: supports NFS, SMB, iSCSI protocols. Supports storage auto scaling, and point in time instantaneous cloning. OpenZFS compatibles with NFS, scale to 1 million IOPS with < 0,5ms latency, and point in time instantaneous cloning. Deployment options: Scratch FS is used for temporary storage with no replication. Supports High burst Persistent FS: long-term storage, replicaed in same AZ","title":"FSx"},{"location":"infra/storage/#hybrid-cloud-storage","text":"AWS Storage Gateway exposes an API in front of S3 to provide on-premises applications with access to virtually unlimited cloud storage. Three gateway types: file : FSx or S3 . S3 buckets are accessible using NFS or SMB protocols. Controlled access via IAM roles. File gateway is installed on-premise and communicate with AWS via HTTPS. FSx, storage gateways brings cache of last accessed files. volume : this is a block storage using iSCSI protocol. On-premise and visible as a local volume backed by S3. Two volume types: Cached volumes: Your primary data is stored in Amazon S3 while your frequently acccessed data is retained locally in the cache for low-latency access. Stored volumes: Your entire dataset is stored locally while also being asynchronously backed up to Amazon S3. tape : same approach but with virtual tape library. Can go to S3 and Glacier. Works with existing tape software. Hardware appliance to run a Storage gateway in the on-premises data center.","title":"Hybrid cloud storage"},{"location":"infra/storage/#transfer-family","text":"To transfer data with FTP, FTPS, SFTP protocols to AWS Storage services like S3, EFS","title":"Transfer Family"},{"location":"infra/storage/#datasync","text":"AWS DataSync is an online data transfer service that simplifies, automates, and accelerates copying large amounts of data between on-premises storage systems and AWS Storage services, as well as between AWS Storage services. Move a large amount of data to and from on-premises (using agent) to AWS, or to AWS to AWS different storage services. Can be used for: Data Migration with automatic encryption and data integrity validation. Archive cold data. Data protection. Data movement for timely in-cloud processing. Replication tasks can be scheduled. It keeps the metadata and permissions about the file. One agent task can get 10 GB/s As an example to support the above architecture, we can configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private Virtual InterFace (VIF). Set up a DataSync scheduled task to send the video files to the EFS file system every 24 hours. The DataSync agent is deployed as a virtual machine that should be deployed on-premises in the same LAN as your source storage to minimize the distance traveled. Transferring files from on-premises to AWS and back without leaving your VPC using AWS DataSync . DatSynch FAQ . Public and Private interface .","title":"DataSync"},{"location":"infra/storage/#storage-comparison","text":"S3: Object Storage. Glacier: Object Archival. EFS: When we need distributed, highly resilient storage, using Network File System for Linux instances, POSIX filesystem. Across multiple AZs. FSx for Windows: Network File System for Windows servers. Central storage for Windows based applications. FSx for Lustre: High Performance Computing (HPC) Linux file system. It can store data directly to S3 too. FSx for NetApp: High OS compatibility. FSx for OpenZFS: for ZFS compatibility. EBS volumes: Network storage for one EC2 instance at a time. Instance Storage: Physical storage for your EC2 instance (high IOPS). But Ephemeral. Storage Gateway: File Gateway, Volume Gateway (cache & stored), Tape Gateway. Snowcone, Snowball / Snowmobile: to move large amount of data to the cloud, physically. Database: for specific workloads, usually with indexing and querying. DataSync: schedule data sync from on-premises to AWS or AWS to AWS services.","title":"Storage comparison"},{"location":"kinesis/","text":"Kinesis services \u00b6 Designed to process real-time streaming data. Three main different components are: Kinesis Streams : low latency streaming ingest at scale. They offer patterns for data stream processing. It looks similar to Kafka, but MKS is the Kafka deployment. Kinesis Analytics : perform real-time analytics on streams using SQL. This Apache Flink as managed service. Kinesis Firehose : load streams into S3, Redshift, ElasticSearch. No administration, auto scaling, serverless. Kinesis Data Streams \u00b6 It is a distributed data stream into Shards for parallel processing. /it uses a public end point and applications can authenticate using IAM role. Kinesis Data Streams is using a throughput provisining model, a shard can inject 1 Mb/s or 1000 msg /s with an egress of 2Mb/s. Adding shards help to scale the throughput. A single shard supports up to 5 messages per second, so a unique consumer gets records every 200ms. Adding more consumers on the same shard, the propagation delay increases and throughput per consumer decreases. With 5 consumers, each receives 400kB max every second. Producer sends message with Partition Key . A sequence number is added to the message to note where the message is in the Shard. Retention from 1 to 365 days. Capable to replay the messages. Immutable records, not deleted by applications. Message in a shard, can share partition key, and keep ordering. Producer can use SDK, or Kinesis Producer Library (KPL) or being a Kinesis agent. Consumer may use SDK and Kinesis Client Library (KCL), or being one of the managed services like: Lambda, Kinesis Data Firehose, Kinesis Data Analytics. For consuming side, each Shard gets 2MB/s out. It uses enhanced fan-out if we have multiple consumers retrieving data from a stream in parallel. This throughput automatically scales with the number of shards in a stream. Pricing is per Shard provisioned per hour. The capacity limits of a Kinesis data stream are defined by the number of shards within the data stream. The limits can be exceeded by either data throughput or the number of reading data calls. Each shard allows for 1 MB/s incoming data and 2 MB/s outgoing data. You should increase the number of shards within your data stream to provide enough capacity. There is an On-demand mode, pay as you go, with a default capacity of 4MB/s or 4000mg/s. Pricing per stream, per hour and data in/out per GB. Captured Metrics are: # of incoming/outgoing bytes, # incoming/outgoing records, Write / read provisioned throughput exceeded, and iterator age ms. Deployment \u00b6 Using CDK, see example in cdk/kinesis , but can be summarized as: from aws_cdk import ( aws_kinesis as kinesis ) kinesis . Stream ( self , \"SaaSdemoStream\" , stream_name = \"bg-jobs\" , shard_count = 1 , retention_period = Duration . hours ( 24 ) ) Using CLI: aws kinesis create-stream --stream-name ExampleInputStream --shard-count 1 --region us-west-2 --profile adminuser Producer \u00b6 Producer applications are done using Kinesis Producer Library (KPL) and they can batch events, and perform retries. Internally KPL uses queue to bufferize messages. Example of python code using boto3 and KPL: STREAM_NAME = \"companies\" my_session = boto3 . session . Session () my_region = my_session . region_name kinesis_client = boto3 . client ( 'kinesis' , region_name = my_region ) def sendCompanyJson (): company = { \"companyID\" : \"comp_4\" , \"industry\" : \"retail\" , \"revenu\" : 29080 , \"employees\" : 14540 , \"job30\" : 4 , \"job90\" : 13 , \"monthlyFee\" : 460.00 , \"totalFee\" : 1172.00 } companyAsString = json . dumps ( company ) print ( companyAsString ) kinesis_client . put_record ( StreamName = STREAM_NAME , Data = companyAsString , PartitionKey = \"partitionkey\" ) AWS CLI \u00b6 Produce: aws kinesis put-record --stream-name test --partition-key user1 --data \"user signup\" --cli-binary-format raw-in-base64-out Consumer \u00b6 AWS CLI \u00b6 Consume: # Describe the stream aws kinesis describe-stream --stream-name test # Get some data aws kinesis get-shard-iterator --stream-name test --shard-id shardId--00000000 --shard-iterator-type TRIM_HORIZON # The returned message gave the next message iterator that should be used in the next call. aws kinesis get-records --shard-iterator <the-iterator-id> Kinesis Data Firehose \u00b6 Firehose is a fully managed service for delivering real-time streaming data to various supported destinations. It can delegates the record transformation processing to a custom Lambda function, but it supports different format already. It outputs batch files to the target destinations. Batch is based on 60s (or more) window or 1 MB of data. Therefore it is a near real-time service. Failed records can go to a S3 bucket. As a managed services it also support auto scaling. IAM role need to be referenced to write to S3. Kinesis Data Analytics \u00b6 This is a managed service to transform and analyze streaming data in real time using Apache Flink, an open-source framework and engine for processing data streams. It can consume records from different source, and in this demonstration we use Kinesis Data Streams. The underlying architecture consists of a Job Manager and n Task Managers . The JobManager controls the execution of a single application. It receives an application for execution and builds a Task Execution Graph from the defined Job Graph. It manages job submission and the job lifecycle then allocates work to Task Managers The Resource Manager manages Task Slots and leverages underlying orchestrator, like Kubernetes or Yarn. A Task slot is the unit of work executed on CPU. The Task Managers execute the actual stream processing logic. There are multiple task managers running in a cluster. The number of slots limits the number of tasks a TaskManager can execute. After it has been started, a TaskManager registers its slots to the ResourceManager When to choose what \u00b6 As Apache Flink is an open-source project, it is possible to deploy it in a Kubernetes cluster, using Flink operator. This will bring you with the most flexible solution as you can select the underlying EC2 instances needed, to optimize your cost. Also you will have fine-grained control over cluster settings, debugging tools and monitoring. While Kinesis Data Analytics helps you to focus on the application logic, which is not simple programming experience, as stateful processing is challenging, there is no management of infrastructure, monitoring, auto scaling and high availability integrated in the service. In addition to the AWS integrations, the Kinesis Data Analytics libraries include more than 10 Apache Flink connectors and the ability to build custom integrations. Considerations \u00b6 When connecting to Kinesis Data Streams, we need to consider the number of shards and the constraint on the throughput to desing the Flink application to avoid getting throttled. As introduced previously, with one Flink Application, we may need to pause around 200ms before doing the next GetRecords. Deployment Flink App to Kinesis Data Analytics \u00b6 Deeper dive \u00b6 Amazon Kinesis Data Analytics for SQL Applications Developer Guide Getting started with example on how to create application with CLI.s AWS Kafka and DynamoDB for real time fraud detection Real Time Streaming with Amazon Kinesis","title":"Kinesis*"},{"location":"kinesis/#kinesis-services","text":"Designed to process real-time streaming data. Three main different components are: Kinesis Streams : low latency streaming ingest at scale. They offer patterns for data stream processing. It looks similar to Kafka, but MKS is the Kafka deployment. Kinesis Analytics : perform real-time analytics on streams using SQL. This Apache Flink as managed service. Kinesis Firehose : load streams into S3, Redshift, ElasticSearch. No administration, auto scaling, serverless.","title":"Kinesis services"},{"location":"kinesis/#kinesis-data-streams","text":"It is a distributed data stream into Shards for parallel processing. /it uses a public end point and applications can authenticate using IAM role. Kinesis Data Streams is using a throughput provisining model, a shard can inject 1 Mb/s or 1000 msg /s with an egress of 2Mb/s. Adding shards help to scale the throughput. A single shard supports up to 5 messages per second, so a unique consumer gets records every 200ms. Adding more consumers on the same shard, the propagation delay increases and throughput per consumer decreases. With 5 consumers, each receives 400kB max every second. Producer sends message with Partition Key . A sequence number is added to the message to note where the message is in the Shard. Retention from 1 to 365 days. Capable to replay the messages. Immutable records, not deleted by applications. Message in a shard, can share partition key, and keep ordering. Producer can use SDK, or Kinesis Producer Library (KPL) or being a Kinesis agent. Consumer may use SDK and Kinesis Client Library (KCL), or being one of the managed services like: Lambda, Kinesis Data Firehose, Kinesis Data Analytics. For consuming side, each Shard gets 2MB/s out. It uses enhanced fan-out if we have multiple consumers retrieving data from a stream in parallel. This throughput automatically scales with the number of shards in a stream. Pricing is per Shard provisioned per hour. The capacity limits of a Kinesis data stream are defined by the number of shards within the data stream. The limits can be exceeded by either data throughput or the number of reading data calls. Each shard allows for 1 MB/s incoming data and 2 MB/s outgoing data. You should increase the number of shards within your data stream to provide enough capacity. There is an On-demand mode, pay as you go, with a default capacity of 4MB/s or 4000mg/s. Pricing per stream, per hour and data in/out per GB. Captured Metrics are: # of incoming/outgoing bytes, # incoming/outgoing records, Write / read provisioned throughput exceeded, and iterator age ms.","title":"Kinesis Data Streams"},{"location":"kinesis/#deployment","text":"Using CDK, see example in cdk/kinesis , but can be summarized as: from aws_cdk import ( aws_kinesis as kinesis ) kinesis . Stream ( self , \"SaaSdemoStream\" , stream_name = \"bg-jobs\" , shard_count = 1 , retention_period = Duration . hours ( 24 ) ) Using CLI: aws kinesis create-stream --stream-name ExampleInputStream --shard-count 1 --region us-west-2 --profile adminuser","title":"Deployment"},{"location":"kinesis/#producer","text":"Producer applications are done using Kinesis Producer Library (KPL) and they can batch events, and perform retries. Internally KPL uses queue to bufferize messages. Example of python code using boto3 and KPL: STREAM_NAME = \"companies\" my_session = boto3 . session . Session () my_region = my_session . region_name kinesis_client = boto3 . client ( 'kinesis' , region_name = my_region ) def sendCompanyJson (): company = { \"companyID\" : \"comp_4\" , \"industry\" : \"retail\" , \"revenu\" : 29080 , \"employees\" : 14540 , \"job30\" : 4 , \"job90\" : 13 , \"monthlyFee\" : 460.00 , \"totalFee\" : 1172.00 } companyAsString = json . dumps ( company ) print ( companyAsString ) kinesis_client . put_record ( StreamName = STREAM_NAME , Data = companyAsString , PartitionKey = \"partitionkey\" )","title":"Producer"},{"location":"kinesis/#consumer","text":"","title":"Consumer"},{"location":"kinesis/#kinesis-data-firehose","text":"Firehose is a fully managed service for delivering real-time streaming data to various supported destinations. It can delegates the record transformation processing to a custom Lambda function, but it supports different format already. It outputs batch files to the target destinations. Batch is based on 60s (or more) window or 1 MB of data. Therefore it is a near real-time service. Failed records can go to a S3 bucket. As a managed services it also support auto scaling. IAM role need to be referenced to write to S3.","title":"Kinesis Data Firehose"},{"location":"kinesis/#kinesis-data-analytics","text":"This is a managed service to transform and analyze streaming data in real time using Apache Flink, an open-source framework and engine for processing data streams. It can consume records from different source, and in this demonstration we use Kinesis Data Streams. The underlying architecture consists of a Job Manager and n Task Managers . The JobManager controls the execution of a single application. It receives an application for execution and builds a Task Execution Graph from the defined Job Graph. It manages job submission and the job lifecycle then allocates work to Task Managers The Resource Manager manages Task Slots and leverages underlying orchestrator, like Kubernetes or Yarn. A Task slot is the unit of work executed on CPU. The Task Managers execute the actual stream processing logic. There are multiple task managers running in a cluster. The number of slots limits the number of tasks a TaskManager can execute. After it has been started, a TaskManager registers its slots to the ResourceManager","title":"Kinesis Data Analytics"},{"location":"kinesis/#when-to-choose-what","text":"As Apache Flink is an open-source project, it is possible to deploy it in a Kubernetes cluster, using Flink operator. This will bring you with the most flexible solution as you can select the underlying EC2 instances needed, to optimize your cost. Also you will have fine-grained control over cluster settings, debugging tools and monitoring. While Kinesis Data Analytics helps you to focus on the application logic, which is not simple programming experience, as stateful processing is challenging, there is no management of infrastructure, monitoring, auto scaling and high availability integrated in the service. In addition to the AWS integrations, the Kinesis Data Analytics libraries include more than 10 Apache Flink connectors and the ability to build custom integrations.","title":"When to choose what"},{"location":"kinesis/#considerations","text":"When connecting to Kinesis Data Streams, we need to consider the number of shards and the constraint on the throughput to desing the Flink application to avoid getting throttled. As introduced previously, with one Flink Application, we may need to pause around 200ms before doing the next GetRecords.","title":"Considerations"},{"location":"kinesis/#deployment-flink-app-to-kinesis-data-analytics","text":"","title":"Deployment Flink App to Kinesis Data Analytics"},{"location":"kinesis/#deeper-dive","text":"Amazon Kinesis Data Analytics for SQL Applications Developer Guide Getting started with example on how to create application with CLI.s AWS Kafka and DynamoDB for real time fraud detection Real Time Streaming with Amazon Kinesis","title":"Deeper dive"},{"location":"monitoring/monitoring/","text":"Monitoring and Audit \u00b6 CloudWatch \u00b6 CloudWatch collects monitoring and operational data in the form of logs, metrics, and events, and visualizes it using automated dashboards so you can get a unified view of your AWS resources, applications, and services that run in AWS and on-premises. The basic features set: correlate your metrics and logs to better understand the health and performance of your resources. create alarms based on metric value thresholds or anomalous metric behavior based on machine learning algorithms. CloudWatch Metrics \u00b6 CloudWatch provides metrics for every services in AWS. Metric represents a variable to measure like CPU utilization, Network inbound traffic... Metrics are within a VPC so belong to namespaces. They have timestamps and heve up to 10 attributes or dimensions. To monitor our EC2 instance memory usage, we need to use a Unified CloudWatch Agent to push memory usage as a custom metric to CW. Alarm is associated with one metric. So, we need one alarm per metric. You can also combine outcome of two alarms using the CloudWatch Composite Alarm. CloudWatch Logs \u00b6 Concepts: Log groups to groups logs, representing an application. Log streams: instances within an application / log files or containers. Priced for retention period, so expiration policies can be defined. CloudWatch can send logs to S3, Kinesis Firehose, Kinesis Data Streams, Lambda,... Can define filters to reduce logs or trigger CloudWatch alarms, or add insights to query logs and for Dashboards. Here are simple filter: Use Subscription Filter to get near real-time logs to targeted sink: Logs Insights helps to define query to search within the logs. CloudWatch Agent \u00b6 By default EC2 instances do not send logs to CloudWatch. We need to run agent on EC2 to push log files we want. We need to use an IAM role that let the EC2 instance be able to send logs to CW. The new Unified Agent send logs and system-level metrics. CloudWatch Alarms \u00b6 Alarms are used to trigger notification from any metrics. The states of an alarm are: OK, INSUFFICIENT_DATA, ALARM. A period specifies the lenght of time in seconds to evaluate the metric. The target of the alarm may be to stop, reboot, terminate, recover of an EC2 instance, trigger an Auto Scaling Action for EC2, or send notification to SNS. You can use the stop or terminate actions to help you save money when you no longer need an instance to be running. You can create an Amazon CloudWatch alarm that monitors an Amazon EC2 instance and automatically reboots the instance. The reboot alarm action is recommended for Instance Health Check failures. CloudWatch Event \u00b6 Is now EventBridge , a more generic event-driven, serverless managed service. CloudWatch Insight \u00b6 CloudWatch Container Insights collects, aggregates, and summarizes metrics and logs from your containerized applications and microservices. Available for ECS, EKS, K8S on EC2s. CloudWatch Lambda Insights simplifies the collection, visualization, and investigation of detailed compute performance metrics, errors, and logs to isolate performance problems and optimize your Lambda environment. Application Insight is to set up monitoring and gain insights to your application health so you can quickly detect and diagnose problems and reduce the mean time to resolution Contributor Insights allows you to create realtime Top N time series reports by analyzing CloudWatch Logs based on rules you define. The rule matches log events and reports the top Contributors, where a \"Contributor\" is a unique combination of the fields defined in the rule. It can be used to identify the heaviest network users, find the URLs that generate the most erroes. EKS workshop with CloudWatch container insight CloudTrail \u00b6 A managed service to provides governance, audit capabilities for all activities (API calls) and events within an Account. Can be across regions and accounts on a single, centrally controlled platform. We can use CloudTrail to detect unusual activity in our AWS accounts. By default, trails are configured to log management events (operations performed on AWS resources). Data events are not logged. This is a usage-based paid service. CloudTrail Insight is used to detect unusual activity in AWS account. Config \u00b6 Record and evaluate configurations against compliance rules of your AWS resources. For example can be used to continuously monitor our EC2 instances to assess if they have a specific port exposed. AWS Config allows you to remediate noncompliant resources that are evaluated by AWS Config Rules. AWS Config applies remediation using AWS Systems Manager Automation documents. This is done at the Action level of a config rule. AWS Config provides AWS-managed rules, which are predefined, customizable rules that AWS Config uses to evaluate whether the AWS resources comply with common best practices. We can leverage an AWS Config managed rule to check if any ACM certificates in our account are marked for expiration within the specified number of days. Putting them together \u00b6 If we define an Elastic Load Balancer then, CloudWatch will help us to monitor incoming connections metric, visualize error codes as % over time, and supports dashbaord to get performance monitoring. Config will help us to track security group rules, configuration changes done on the load balancer, as well as defining compliance rules to ensure SSL certificates are always assigned to LB. CloudTrail tracks who made any changes to the configuration with API calls. Deeper Dive \u00b6 CDK sample: CDK Python Backup & Cloudwatch event to illustrate a cloudwatch event rule to stop instances at UTC 15pm everyday CloudWatch Container Insights for EKS cluster workshop","title":"Monitoring"},{"location":"monitoring/monitoring/#monitoring-and-audit","text":"","title":"Monitoring and Audit"},{"location":"monitoring/monitoring/#cloudwatch","text":"CloudWatch collects monitoring and operational data in the form of logs, metrics, and events, and visualizes it using automated dashboards so you can get a unified view of your AWS resources, applications, and services that run in AWS and on-premises. The basic features set: correlate your metrics and logs to better understand the health and performance of your resources. create alarms based on metric value thresholds or anomalous metric behavior based on machine learning algorithms.","title":"CloudWatch"},{"location":"monitoring/monitoring/#cloudwatch-metrics","text":"CloudWatch provides metrics for every services in AWS. Metric represents a variable to measure like CPU utilization, Network inbound traffic... Metrics are within a VPC so belong to namespaces. They have timestamps and heve up to 10 attributes or dimensions. To monitor our EC2 instance memory usage, we need to use a Unified CloudWatch Agent to push memory usage as a custom metric to CW. Alarm is associated with one metric. So, we need one alarm per metric. You can also combine outcome of two alarms using the CloudWatch Composite Alarm.","title":"CloudWatch Metrics"},{"location":"monitoring/monitoring/#cloudwatch-logs","text":"Concepts: Log groups to groups logs, representing an application. Log streams: instances within an application / log files or containers. Priced for retention period, so expiration policies can be defined. CloudWatch can send logs to S3, Kinesis Firehose, Kinesis Data Streams, Lambda,... Can define filters to reduce logs or trigger CloudWatch alarms, or add insights to query logs and for Dashboards. Here are simple filter: Use Subscription Filter to get near real-time logs to targeted sink: Logs Insights helps to define query to search within the logs.","title":"CloudWatch Logs"},{"location":"monitoring/monitoring/#cloudwatch-agent","text":"By default EC2 instances do not send logs to CloudWatch. We need to run agent on EC2 to push log files we want. We need to use an IAM role that let the EC2 instance be able to send logs to CW. The new Unified Agent send logs and system-level metrics.","title":"CloudWatch Agent"},{"location":"monitoring/monitoring/#cloudwatch-alarms","text":"Alarms are used to trigger notification from any metrics. The states of an alarm are: OK, INSUFFICIENT_DATA, ALARM. A period specifies the lenght of time in seconds to evaluate the metric. The target of the alarm may be to stop, reboot, terminate, recover of an EC2 instance, trigger an Auto Scaling Action for EC2, or send notification to SNS. You can use the stop or terminate actions to help you save money when you no longer need an instance to be running. You can create an Amazon CloudWatch alarm that monitors an Amazon EC2 instance and automatically reboots the instance. The reboot alarm action is recommended for Instance Health Check failures.","title":"CloudWatch Alarms"},{"location":"monitoring/monitoring/#cloudwatch-event","text":"Is now EventBridge , a more generic event-driven, serverless managed service.","title":"CloudWatch Event"},{"location":"monitoring/monitoring/#cloudwatch-insight","text":"CloudWatch Container Insights collects, aggregates, and summarizes metrics and logs from your containerized applications and microservices. Available for ECS, EKS, K8S on EC2s. CloudWatch Lambda Insights simplifies the collection, visualization, and investigation of detailed compute performance metrics, errors, and logs to isolate performance problems and optimize your Lambda environment. Application Insight is to set up monitoring and gain insights to your application health so you can quickly detect and diagnose problems and reduce the mean time to resolution Contributor Insights allows you to create realtime Top N time series reports by analyzing CloudWatch Logs based on rules you define. The rule matches log events and reports the top Contributors, where a \"Contributor\" is a unique combination of the fields defined in the rule. It can be used to identify the heaviest network users, find the URLs that generate the most erroes. EKS workshop with CloudWatch container insight","title":"CloudWatch Insight"},{"location":"monitoring/monitoring/#cloudtrail","text":"A managed service to provides governance, audit capabilities for all activities (API calls) and events within an Account. Can be across regions and accounts on a single, centrally controlled platform. We can use CloudTrail to detect unusual activity in our AWS accounts. By default, trails are configured to log management events (operations performed on AWS resources). Data events are not logged. This is a usage-based paid service. CloudTrail Insight is used to detect unusual activity in AWS account.","title":"CloudTrail"},{"location":"monitoring/monitoring/#config","text":"Record and evaluate configurations against compliance rules of your AWS resources. For example can be used to continuously monitor our EC2 instances to assess if they have a specific port exposed. AWS Config allows you to remediate noncompliant resources that are evaluated by AWS Config Rules. AWS Config applies remediation using AWS Systems Manager Automation documents. This is done at the Action level of a config rule. AWS Config provides AWS-managed rules, which are predefined, customizable rules that AWS Config uses to evaluate whether the AWS resources comply with common best practices. We can leverage an AWS Config managed rule to check if any ACM certificates in our account are marked for expiration within the specified number of days.","title":"Config"},{"location":"monitoring/monitoring/#putting-them-together","text":"If we define an Elastic Load Balancer then, CloudWatch will help us to monitor incoming connections metric, visualize error codes as % over time, and supports dashbaord to get performance monitoring. Config will help us to track security group rules, configuration changes done on the load balancer, as well as defining compliance rules to ensure SSL certificates are always assigned to LB. CloudTrail tracks who made any changes to the configuration with API calls.","title":"Putting them together"},{"location":"monitoring/monitoring/#deeper-dive","text":"CDK sample: CDK Python Backup & Cloudwatch event to illustrate a cloudwatch event rule to stop instances at UTC 15pm everyday CloudWatch Container Insights for EKS cluster workshop","title":"Deeper Dive"},{"location":"playground/ecs/","text":"ECS playground \u00b6 Deploy NGinx docker image on Fargate \u00b6 Create an ECS cluster if you do not have one Add a task definition to use NGInx demo hello docker image There is no need to specify environment variables in this demonstration. Specify the environment like EC2 or Fargate and then the CPU needs There is not need to add a Task role as the container is not accessing AWS services via API. No Storage neither. Use log collection to CloudWatch, but not needed to send trace to AWS X-Ray and metrics to AWS Distro. As this is a web app, we need to create a service: Select the ECS cluster as target environment Then the service with one replica to start with: The deployment paraneters control the % allocated during deployment: Be sure to select the VPC where the ECS cluster is defined ( not sure about that ) then the subnets, and a specific security group on port 80 coming from the ALB () As we want to facade with an ALB, we do not want to setup a public IP address with a ENI. Get DNS name of the application or from the ALB If you created a public IP address then an ENI was created, and going to the ENI definition will let you access to the DNS name, something like: ec2-34-216-228-13.us-west-2.compute.amazonaws.com","title":"ECS"},{"location":"playground/ecs/#ecs-playground","text":"","title":"ECS playground"},{"location":"playground/ecs/#deploy-nginx-docker-image-on-fargate","text":"Create an ECS cluster if you do not have one Add a task definition to use NGInx demo hello docker image There is no need to specify environment variables in this demonstration. Specify the environment like EC2 or Fargate and then the CPU needs There is not need to add a Task role as the container is not accessing AWS services via API. No Storage neither. Use log collection to CloudWatch, but not needed to send trace to AWS X-Ray and metrics to AWS Distro. As this is a web app, we need to create a service: Select the ECS cluster as target environment Then the service with one replica to start with: The deployment paraneters control the % allocated during deployment: Be sure to select the VPC where the ECS cluster is defined ( not sure about that ) then the subnets, and a specific security group on port 80 coming from the ALB () As we want to facade with an ALB, we do not want to setup a public IP address with a ENI. Get DNS name of the application or from the ALB If you created a public IP address then an ENI was created, and going to the ENI definition will let you access to the DNS name, something like: ec2-34-216-228-13.us-west-2.compute.amazonaws.com","title":"Deploy NGinx docker image on Fargate"},{"location":"playground/gettingstarted/","text":"Getting started \u00b6 Defined users and groups with IAM \u00b6 See summary on IAM Access the AWS console from which we can login as root user or as an IAM user: aws-jb for AWS account. (Or using personal account login to the account https://jbcodeforce.signin.aws.amazon.com/console with admin user jerome . In the IAM service, create groups (Developers, Adminstrators), define basic policies. Define security policies \u00b6 Attached to the group level. AWS CLI common commands \u00b6 We can access AWS using the CLI or the SDK which both user access keys generated from the console (> Users > jerome > Security credentials > Access Keys). The keys are saved in ~/.aws/credentials in different profile * A named profile is a collection of settings and credentials that you can apply to a AWS CLI command. When you specify a profile to run a command, the settings and credentials are used to run that command. * Installation: aws cli * The cli needs to be configured: aws configure with the credential, key and region to access. Use IAM user to get a new credentials key. The credentials and API key are in ~/.aws/credentials in default profile Test with some commands: aws --version # get your users aws iam list-users # For a given profile aws iam list-users --profile hackaton VPC scenario with CLI - Tutorial Use CloudShell in west-2 region for using aws cli aws-shell is also available to facilitate the user experience in your laptop terminal console. When using CLI in a EC2 instance always use an IAM role to control security credentials. This role may come with a policy authorizing exactly what the EC2 instance should be able to do. Also within a EC2 instance, it is possible to use the URL http://169.254.169.254/latest/meta-data to get information about the EC2. We can retrieve the IAM Role name from that metadata. Deploying Apache HTTP on EC2 \u00b6 Create a EC2 t2.micro instance with AWS Linux, a public IP address, a security group with SSH enabled from anywhere and HTTP on port 80 accessible from the internet. Associate the EC2 with a Key Pair so we can do SSH on the instance (and download the .pem file). The free tier is elligible to 30 GB of disk. Under the Advanced details section, add the following bash script in the User Data field: #!/bin/bash # update OS yum update -y # Get Apache HTTPd yum install -y httpd # Start the service systemctl start httpd # Enable it cross restart systemctl enable httpd > Created symlink from /etc/systemd/system/multi-user.target.wants/httpd.service to /usr/lib/systemd/system/httpd.service # Get the availability zone EC2-AZ = $( curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone/ ) # Change the home page by changing /var/www/html/index.html echo \"<h1>Hello from $( hostname -f ) </h1>\" > /var/www/html/index.html # or use the following echo \"<h3>Hello World from $( hostname -f ) in AZ= $EC2_AZ </h3>\" > /var/www/html/index.html Once launched, from the console, get the DNS name or the public IP address and try a curl or use your web browser to that HTTP address (not https). Troubleshooting \u00b6 Connection timeout: Any timeout (not just for SSH) is related to security groups or a firewall rule. This may also mean a corporate firewall or a personal firewall is blocking the connection. Go to the security group and look at inbound rules. Permission denied (publickey,gssapi-keyex,gssapi-with-mic) : You are using the wrong security key or not using a security key. \"Able to connect yesterday, but not today\": When you restart a EC2 instance, the public IP of your EC2 instance will change, so prefer to use the DNS name and not the IP @. SSH to EC2 \u00b6 Get public IP address of the EC2 instance Get pem certificate for the CA while you created the EC2 via the Key pair (e.g. my-key-pair.pem ) Issue the following command where the certificate is to open a SSH session ssh -i my-key-pair.pem ec2-user@35.91.239.193 you should get the prompt: Last login: Wed Nov 2 17 :24:30 2022 from ec2-18-237-140-165.us-west-2.compute.amazonaws.com __ | __ | _ ) _ | ( / Amazon Linux 2 AMI ___ | \\_ __ | ___ | https://aws.amazon.com/amazon-linux-2/ EC2 Instance Connect \u00b6 Access the EC2 terminal inside the web browser using SSH. Select the instance and then Connect button at the top. It comes with the aws cli . Never enter any account id inside aws configure inside an EC2 instance, use IAM role instead. For example to access another service (like IAM), we need an IAM Role added to the EC2 instance: go to the EC2 instance, Action > Security > Modify IAM Roles add DemoEC2Role for example. We should be able to do aws iam list-users command. Access to service within the EC2 \u00b6 To access to external AWS service we need to use IAM role. So define a Role in IAM with the Permission Policy linked to the resource you try to access, for example select on S3 policies to be able to access S3 bucket. On an existing EC2 we can use the menu Actions > Security > Modify IAM Roles . A High availability WebApp deployment summary \u00b6 Based on the AWS essential training, here is a quick summary of the things to do for a classical HA webapp deployment. Create a VPC with private and public subnets, using at least 2 AZs. This is simplified with the new console which creates all those elements in one click: The results, with all the networking objects created, look like below: Verify routing table for public and private subnets. Add security group to the VPC using HTTP and HTTPS to the internet gateway. Start EC2 to one of the public subnet and define user data to start your app. Here is an example #!/bin/bash -ex yum -y update curl -sL https://rpm.nodesource.com/setup_15.x | bash - yum -y install nodejs mkdir -p /var/app wget https://aws-tc-largeobjects.s3-us-west-2.amazonaws.com/ILT-TF-100-TECESS-5/app/app.zip unzip app.zip -d /var/app/ cd /var/app npm install npm start Get the security key with .pem file for the public certificate Be sure the inbound rules include HTTP and HTTPS on all IPv4 addresses defined in the security group. Create a EC2 instance with Terraform \u00b6 Build a main.tf labs/terraform-vpc , which uses the aws provider to provision a micro EC2 instance: terraf orm { required_providers { aws = { source = \"hashicorp/aws\" versio n = \"~> 3.27\" } } required_versio n = \">= 0.14.9\" } provider \"aws\" { pro f ile = \"default\" regio n = \"us-west-2\" } resource \"aws_instance\" \"app_server\" { ami = \"ami-830c94e3\" i nstan ce_ t ype = \"t2.micro\" ta gs = { Name = \"ExampleAppServerInstance\" } } Resource blocks contain arguments which you use to configure the resource. Arguments can include things like machine sizes, disk image names, or VPC IDs. terraform apply # inspect state terraform show Install nginx inside a EC2 t2.micro. \u00b6 Be sure to have a policy to authorize HTTP inbound traffic on port 80 for 0.0.0.0/0. Define load balancer \u00b6 AWS Cloud9 \u00b6 AWS Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code within a web browser. Go to the AWS Management Console, select Services then select Cloud9 under Developer Tools. Select Create environment. Enter Development into Name and optionally provide a Description. You may leave Environment settings at their defaults of launching a new t2.micro EC2 instance which will be paused after 30 minutes of inactivity. Once ready, your IDE will open to a welcome screen. Verify the environment with aws sts get-caller-identity ECR for Container Registry \u00b6 Amazon Elastic Container Registry (Amazon ECR) is an AWS managed container image registry service that is secure, scalable, and reliable. See Getting started guide which can be summarized as Your client must authenticate to Amazon ECR registries as an AWS user before you can push and pull images. You can control access to your repositories and the images within them with repository policies. ECR public gallery includes docker images to be reusable. Deploy a Web App on AWS Elastic Beanstalk \u00b6 Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS Guide","title":"Getting Started IAM-EC2"},{"location":"playground/gettingstarted/#getting-started","text":"","title":"Getting started"},{"location":"playground/gettingstarted/#defined-users-and-groups-with-iam","text":"See summary on IAM Access the AWS console from which we can login as root user or as an IAM user: aws-jb for AWS account. (Or using personal account login to the account https://jbcodeforce.signin.aws.amazon.com/console with admin user jerome . In the IAM service, create groups (Developers, Adminstrators), define basic policies.","title":"Defined users and groups with IAM"},{"location":"playground/gettingstarted/#define-security-policies","text":"Attached to the group level.","title":"Define security policies"},{"location":"playground/gettingstarted/#aws-cli-common-commands","text":"We can access AWS using the CLI or the SDK which both user access keys generated from the console (> Users > jerome > Security credentials > Access Keys). The keys are saved in ~/.aws/credentials in different profile * A named profile is a collection of settings and credentials that you can apply to a AWS CLI command. When you specify a profile to run a command, the settings and credentials are used to run that command. * Installation: aws cli * The cli needs to be configured: aws configure with the credential, key and region to access. Use IAM user to get a new credentials key. The credentials and API key are in ~/.aws/credentials in default profile Test with some commands: aws --version # get your users aws iam list-users # For a given profile aws iam list-users --profile hackaton VPC scenario with CLI - Tutorial Use CloudShell in west-2 region for using aws cli aws-shell is also available to facilitate the user experience in your laptop terminal console. When using CLI in a EC2 instance always use an IAM role to control security credentials. This role may come with a policy authorizing exactly what the EC2 instance should be able to do. Also within a EC2 instance, it is possible to use the URL http://169.254.169.254/latest/meta-data to get information about the EC2. We can retrieve the IAM Role name from that metadata.","title":"AWS CLI common commands"},{"location":"playground/gettingstarted/#deploying-apache-http-on-ec2","text":"Create a EC2 t2.micro instance with AWS Linux, a public IP address, a security group with SSH enabled from anywhere and HTTP on port 80 accessible from the internet. Associate the EC2 with a Key Pair so we can do SSH on the instance (and download the .pem file). The free tier is elligible to 30 GB of disk. Under the Advanced details section, add the following bash script in the User Data field: #!/bin/bash # update OS yum update -y # Get Apache HTTPd yum install -y httpd # Start the service systemctl start httpd # Enable it cross restart systemctl enable httpd > Created symlink from /etc/systemd/system/multi-user.target.wants/httpd.service to /usr/lib/systemd/system/httpd.service # Get the availability zone EC2-AZ = $( curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone/ ) # Change the home page by changing /var/www/html/index.html echo \"<h1>Hello from $( hostname -f ) </h1>\" > /var/www/html/index.html # or use the following echo \"<h3>Hello World from $( hostname -f ) in AZ= $EC2_AZ </h3>\" > /var/www/html/index.html Once launched, from the console, get the DNS name or the public IP address and try a curl or use your web browser to that HTTP address (not https).","title":"Deploying Apache HTTP on EC2"},{"location":"playground/gettingstarted/#troubleshooting","text":"Connection timeout: Any timeout (not just for SSH) is related to security groups or a firewall rule. This may also mean a corporate firewall or a personal firewall is blocking the connection. Go to the security group and look at inbound rules. Permission denied (publickey,gssapi-keyex,gssapi-with-mic) : You are using the wrong security key or not using a security key. \"Able to connect yesterday, but not today\": When you restart a EC2 instance, the public IP of your EC2 instance will change, so prefer to use the DNS name and not the IP @.","title":"Troubleshooting"},{"location":"playground/gettingstarted/#ssh-to-ec2","text":"Get public IP address of the EC2 instance Get pem certificate for the CA while you created the EC2 via the Key pair (e.g. my-key-pair.pem ) Issue the following command where the certificate is to open a SSH session ssh -i my-key-pair.pem ec2-user@35.91.239.193 you should get the prompt: Last login: Wed Nov 2 17 :24:30 2022 from ec2-18-237-140-165.us-west-2.compute.amazonaws.com __ | __ | _ ) _ | ( / Amazon Linux 2 AMI ___ | \\_ __ | ___ | https://aws.amazon.com/amazon-linux-2/","title":"SSH to EC2"},{"location":"playground/gettingstarted/#ec2-instance-connect","text":"Access the EC2 terminal inside the web browser using SSH. Select the instance and then Connect button at the top. It comes with the aws cli . Never enter any account id inside aws configure inside an EC2 instance, use IAM role instead. For example to access another service (like IAM), we need an IAM Role added to the EC2 instance: go to the EC2 instance, Action > Security > Modify IAM Roles add DemoEC2Role for example. We should be able to do aws iam list-users command.","title":"EC2 Instance Connect"},{"location":"playground/gettingstarted/#access-to-service-within-the-ec2","text":"To access to external AWS service we need to use IAM role. So define a Role in IAM with the Permission Policy linked to the resource you try to access, for example select on S3 policies to be able to access S3 bucket. On an existing EC2 we can use the menu Actions > Security > Modify IAM Roles .","title":"Access to service within the EC2"},{"location":"playground/gettingstarted/#a-high-availability-webapp-deployment-summary","text":"Based on the AWS essential training, here is a quick summary of the things to do for a classical HA webapp deployment. Create a VPC with private and public subnets, using at least 2 AZs. This is simplified with the new console which creates all those elements in one click: The results, with all the networking objects created, look like below: Verify routing table for public and private subnets. Add security group to the VPC using HTTP and HTTPS to the internet gateway. Start EC2 to one of the public subnet and define user data to start your app. Here is an example #!/bin/bash -ex yum -y update curl -sL https://rpm.nodesource.com/setup_15.x | bash - yum -y install nodejs mkdir -p /var/app wget https://aws-tc-largeobjects.s3-us-west-2.amazonaws.com/ILT-TF-100-TECESS-5/app/app.zip unzip app.zip -d /var/app/ cd /var/app npm install npm start Get the security key with .pem file for the public certificate Be sure the inbound rules include HTTP and HTTPS on all IPv4 addresses defined in the security group.","title":"A High availability WebApp deployment summary"},{"location":"playground/gettingstarted/#create-a-ec2-instance-with-terraform","text":"Build a main.tf labs/terraform-vpc , which uses the aws provider to provision a micro EC2 instance: terraf orm { required_providers { aws = { source = \"hashicorp/aws\" versio n = \"~> 3.27\" } } required_versio n = \">= 0.14.9\" } provider \"aws\" { pro f ile = \"default\" regio n = \"us-west-2\" } resource \"aws_instance\" \"app_server\" { ami = \"ami-830c94e3\" i nstan ce_ t ype = \"t2.micro\" ta gs = { Name = \"ExampleAppServerInstance\" } } Resource blocks contain arguments which you use to configure the resource. Arguments can include things like machine sizes, disk image names, or VPC IDs. terraform apply # inspect state terraform show","title":"Create a EC2 instance with Terraform"},{"location":"playground/gettingstarted/#install-nginx-inside-a-ec2-t2micro","text":"Be sure to have a policy to authorize HTTP inbound traffic on port 80 for 0.0.0.0/0.","title":"Install nginx inside a EC2 t2.micro."},{"location":"playground/gettingstarted/#define-load-balancer","text":"","title":"Define load balancer"},{"location":"playground/gettingstarted/#aws-cloud9","text":"AWS Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code within a web browser. Go to the AWS Management Console, select Services then select Cloud9 under Developer Tools. Select Create environment. Enter Development into Name and optionally provide a Description. You may leave Environment settings at their defaults of launching a new t2.micro EC2 instance which will be paused after 30 minutes of inactivity. Once ready, your IDE will open to a welcome screen. Verify the environment with aws sts get-caller-identity","title":"AWS Cloud9"},{"location":"playground/gettingstarted/#ecr-for-container-registry","text":"Amazon Elastic Container Registry (Amazon ECR) is an AWS managed container image registry service that is secure, scalable, and reliable. See Getting started guide which can be summarized as Your client must authenticate to Amazon ECR registries as an AWS user before you can push and pull images. You can control access to your repositories and the images within them with repository policies. ECR public gallery includes docker images to be reusable.","title":"ECR for Container Registry"},{"location":"playground/gettingstarted/#deploy-a-web-app-on-aws-elastic-beanstalk","text":"Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS Guide","title":"Deploy a Web App on AWS Elastic Beanstalk"},{"location":"playground/rds/","text":"Hands-on RDS \u00b6 Create RDS Postgresql with Console \u00b6 Be sure to define a security group in your VPC with a Inbound rules for the expected port number and MyIP address so you can run your app locally on your laptop and access RDS PostgreSQL DB. Using PostgreSQL instance: See also the tutorial of creating Postgresql to get the different steps and default practices. Some remarks from the AWS tutorial: In Templates changes the options in the form, for a production deployment with multi-AZ DB instance with primary and standby DB instances. Be sure to keep the username and password Specify EC2 instance type and Storage class, and if you need storage autoscaling or not. Assign to a VPC, choose network IP type and subnets, public access (not for production), and exposed port: Be sure the DB instance is associated with a security group that provides access to it (port 5432). If your DB instance is publicly accessible, make sure its associated security group has inbound rules for the IP addresses that you want to access it. Info Access to the created security group via the RDS instance panel: Verify the Monitoring panel to see the traffic you may have generated (see next sections) To delete the DB using the list of DB panel and Actions > Delete . In case of modify the RDS database configuration to authorize deletion. Modify an existing instance to be public \u00b6 Choose Databases, and then select the Aurora DB instance in the Aurora Cluster that you want to modify. Choose Modify. From the Modify DB instance page, under Connectivity, expand the Additional Configuration section. Set Public access to Yes or No. Choose Continue, and check the summary of modifications. To apply the changes immediately, select Apply immediately. Changing this setting on the existing DB instance in the cluster affects the network connectivity. See this note for more detail. Create an Aurora Serverless instance \u00b6 The difference with previous step is the Aurora database, with a postgresql driver, and then serverless option. This will be a better solution for demonstration and to have access to an SQL editor inside AWS RDS console webapp, if not we need an external tool like pgadmin remotely connected. Quarkus App Client \u00b6 See the repository Autonomous Car Ride which uses Quarkus, Panache, and CarRide entity. The repository includes a docker file to start local postgresql and pgadmin containers. When changing to the URL properties to the one of the RDS service, then the local execution with quarkus dev connect to the RDS database. quarkus.datasource.jdbc.url = jdbc:postgresql://<>.us-west-2.rds.amazonaws.com:5432/postgres Adding a new server definition in the PGADMIN tool, with the RDS URL, user and password, we can see the DB and records from the RDS DB as illustrated below: The URL for the quarkus app can be overwritten by environment variable: QUARKUS_DATASOURCE_JDBC_URL Python Client \u00b6 Need to install boto3 wiht pip. Bot3 RDS API documentation. import boto3 client = boto3 . client ( 'rds' ) response = client . describe_db_instances ( DBInstanceIdentifier = 'customer-action-instance-1' , ) print ( response )","title":"RDS"},{"location":"playground/rds/#hands-on-rds","text":"","title":"Hands-on RDS"},{"location":"playground/rds/#create-rds-postgresql-with-console","text":"Be sure to define a security group in your VPC with a Inbound rules for the expected port number and MyIP address so you can run your app locally on your laptop and access RDS PostgreSQL DB. Using PostgreSQL instance: See also the tutorial of creating Postgresql to get the different steps and default practices. Some remarks from the AWS tutorial: In Templates changes the options in the form, for a production deployment with multi-AZ DB instance with primary and standby DB instances. Be sure to keep the username and password Specify EC2 instance type and Storage class, and if you need storage autoscaling or not. Assign to a VPC, choose network IP type and subnets, public access (not for production), and exposed port: Be sure the DB instance is associated with a security group that provides access to it (port 5432). If your DB instance is publicly accessible, make sure its associated security group has inbound rules for the IP addresses that you want to access it. Info Access to the created security group via the RDS instance panel: Verify the Monitoring panel to see the traffic you may have generated (see next sections) To delete the DB using the list of DB panel and Actions > Delete . In case of modify the RDS database configuration to authorize deletion.","title":"Create RDS Postgresql with Console"},{"location":"playground/rds/#modify-an-existing-instance-to-be-public","text":"Choose Databases, and then select the Aurora DB instance in the Aurora Cluster that you want to modify. Choose Modify. From the Modify DB instance page, under Connectivity, expand the Additional Configuration section. Set Public access to Yes or No. Choose Continue, and check the summary of modifications. To apply the changes immediately, select Apply immediately. Changing this setting on the existing DB instance in the cluster affects the network connectivity. See this note for more detail.","title":"Modify an existing instance to be public"},{"location":"playground/rds/#create-an-aurora-serverless-instance","text":"The difference with previous step is the Aurora database, with a postgresql driver, and then serverless option. This will be a better solution for demonstration and to have access to an SQL editor inside AWS RDS console webapp, if not we need an external tool like pgadmin remotely connected.","title":"Create an Aurora Serverless instance"},{"location":"playground/rds/#quarkus-app-client","text":"See the repository Autonomous Car Ride which uses Quarkus, Panache, and CarRide entity. The repository includes a docker file to start local postgresql and pgadmin containers. When changing to the URL properties to the one of the RDS service, then the local execution with quarkus dev connect to the RDS database. quarkus.datasource.jdbc.url = jdbc:postgresql://<>.us-west-2.rds.amazonaws.com:5432/postgres Adding a new server definition in the PGADMIN tool, with the RDS URL, user and password, we can see the DB and records from the RDS DB as illustrated below: The URL for the quarkus app can be overwritten by environment variable: QUARKUS_DATASOURCE_JDBC_URL","title":"Quarkus App Client"},{"location":"playground/rds/#python-client","text":"Need to install boto3 wiht pip. Bot3 RDS API documentation. import boto3 client = boto3 . client ( 'rds' ) response = client . describe_db_instances ( DBInstanceIdentifier = 'customer-action-instance-1' , ) print ( response )","title":"Python Client"},{"location":"playground/rt-data-processing/","text":"Build a Serverless Real-Time Data Processing App \u00b6 From this hands-on lab Some changes from the lab itself. Another Demo \u00b6 Functional architecture \u00b6 We have 10000 refrigerator containers in the field, which send sensor telemetry messages using MQTT over HTTPS to a data ingestion and real-time event backbone as landing zone. This layer keeps data for 7 days, so on the left side we are moving records to Data Lake From Data lake data engineers and data scientists will query data at rest, encrypted, and even build dashboards On the right side we have the cold chain monitor component, and the best action processing in case there is a refrigeration container or reefer with problem. As part of the action processing we can notify a field engineer to work on container, persist data in a DB for triggering a downstream business process Hands-on architecture \u00b6 Now on the scope of the demonstration we build for you, using AWS managed services, this is the architecture we are proposing We do not have refrigerator at our hand so we get the data structure from your engineers, and develop a small simulator to send records to the data ingestion layer. The AWS Kinesis Data Stream is the service to manage pub/sub processing from streams (like a topic). we have telemetries and faulty-reefers The stateful logic processing is done inside Kinesis analytics: The logic is to consume messages from data stream, process them with time window constraint and logic then write messages to the faulty-reefers data stream On the right side we have the act part, where you can have different consumers which are getting message asynchronously to process the faulty message For data lake, Firehose is the service to do data movement and transformation, like ETL. The target is distributed storage, called S3. The persistence looks like a file system folder, and is named bucker. For dashboard we use QuickSight Kinesis SDK for python Create Kinesis Elements \u00b6 Data Streams \u00b6 Streams Analytics \u00b6 CREATE OR REPLACE PUMP \"STREAM_PUMP\" AS INSERT INTO \"FAULTY_REEFERS\" SELECT STREAM \"container_id\" , \"measurement_time\" , \"product_id\" , \"temperature\" , \"kilowatts\" , \"oxygen_level\" , \"carbon_dioxide_level\" , \"fan_1\" , \"latitude\" , \"longitude\" , TUMBLE_START ( ts , INTERVAL '1' MINUTE ) as window_start , TUMBLE_STOP ( ts , INTERVAL '1' MINUTE ) as window_stop , COUNT ( temperature ) FROM \"SOURCE_SQL_STREAM_001\" GROUP BY TUMBLE ( ts , INTERVAL '1' MINUTE ), temperature WHERE \"temperature\" > 18 ;","title":"Data processing"},{"location":"playground/rt-data-processing/#build-a-serverless-real-time-data-processing-app","text":"From this hands-on lab Some changes from the lab itself.","title":"Build a Serverless Real-Time Data Processing App"},{"location":"playground/rt-data-processing/#another-demo","text":"","title":"Another Demo"},{"location":"playground/rt-data-processing/#functional-architecture","text":"We have 10000 refrigerator containers in the field, which send sensor telemetry messages using MQTT over HTTPS to a data ingestion and real-time event backbone as landing zone. This layer keeps data for 7 days, so on the left side we are moving records to Data Lake From Data lake data engineers and data scientists will query data at rest, encrypted, and even build dashboards On the right side we have the cold chain monitor component, and the best action processing in case there is a refrigeration container or reefer with problem. As part of the action processing we can notify a field engineer to work on container, persist data in a DB for triggering a downstream business process","title":"Functional architecture"},{"location":"playground/rt-data-processing/#hands-on-architecture","text":"Now on the scope of the demonstration we build for you, using AWS managed services, this is the architecture we are proposing We do not have refrigerator at our hand so we get the data structure from your engineers, and develop a small simulator to send records to the data ingestion layer. The AWS Kinesis Data Stream is the service to manage pub/sub processing from streams (like a topic). we have telemetries and faulty-reefers The stateful logic processing is done inside Kinesis analytics: The logic is to consume messages from data stream, process them with time window constraint and logic then write messages to the faulty-reefers data stream On the right side we have the act part, where you can have different consumers which are getting message asynchronously to process the faulty message For data lake, Firehose is the service to do data movement and transformation, like ETL. The target is distributed storage, called S3. The persistence looks like a file system folder, and is named bucker. For dashboard we use QuickSight Kinesis SDK for python","title":"Hands-on architecture"},{"location":"playground/rt-data-processing/#create-kinesis-elements","text":"","title":"Create Kinesis Elements"},{"location":"playground/s3-org-billing/","text":"S3 storage aggregation with Storage Lens and Organization \u00b6 Info Created 1/11/2023 The goal of this lab is to demonstrate single view in S3 Storage Lens of S3 buckets created in multiple regions in multiple accounts. Requirements \u00b6 Get one organization to centralize 2 child accounts. Get S3 bucket for the 2 account each in 2 regions Create a Storage Lens dashboard to see consolidated metrics AWS Organizations \u00b6 AWS Organizations is an AWS service that helps you aggregate all of your AWS accounts under one organization hierarchy. Invite or create an account from Organization Enable trusted access from S3 Storage Lens to Organization, to aggregate storage metrics and usage data for all member accounts in your organization. S3 Storage Lens \u00b6 A service that aggregates your usage and activity metrics and displays the information in an interactive dashboard on the Amazon S3 console or through a metrics data export that can be downloaded in CSV or Parquet format. [Optional] Add account to be S3 Storage Lens administrator Create a S3 Storage Lens dashboard with a region to keep dashboard state: Still in the same form, you should see the organization linked to the service, and include all accounts and all regions: Can select metrics and defined new metrics, here we use the free ones: Metrics will help to find out how much storage you have across the entire organization . Which are the fastest - growing buckets and prefixes ... We can specify advanced metrics to get prefix - level aggregation . Free metrics are available for queries for a 14 - day period , and advanced metrics are available for queries for a 15 - month period . We export metrics generated every day, in CSV or Parquet format, and save result in the parent account's s3 bucket: Finally the data should be encrypted at rest, so using S3 managed keys will be the default: The dashboard is created and will get data in the next 24h to 48h. Lens pricing can be found in the Management and Analytics tab in S3 pricing page. Add files in buckets \u00b6 Account 1 (boyerje): add buckets in regions us-west-2 and us-east-1 aws w3 cp ... s3://jb-data-set/ Account 2 (jerome boyer): add buckets in regions us-west-1 Resources \u00b6 S3 Storage Lens user guide","title":"S3 storage lens"},{"location":"playground/s3-org-billing/#s3-storage-aggregation-with-storage-lens-and-organization","text":"Info Created 1/11/2023 The goal of this lab is to demonstrate single view in S3 Storage Lens of S3 buckets created in multiple regions in multiple accounts.","title":"S3 storage aggregation with Storage Lens and Organization"},{"location":"playground/s3-org-billing/#requirements","text":"Get one organization to centralize 2 child accounts. Get S3 bucket for the 2 account each in 2 regions Create a Storage Lens dashboard to see consolidated metrics","title":"Requirements"},{"location":"playground/s3-org-billing/#aws-organizations","text":"AWS Organizations is an AWS service that helps you aggregate all of your AWS accounts under one organization hierarchy. Invite or create an account from Organization Enable trusted access from S3 Storage Lens to Organization, to aggregate storage metrics and usage data for all member accounts in your organization.","title":"AWS Organizations"},{"location":"playground/s3-org-billing/#s3-storage-lens","text":"A service that aggregates your usage and activity metrics and displays the information in an interactive dashboard on the Amazon S3 console or through a metrics data export that can be downloaded in CSV or Parquet format. [Optional] Add account to be S3 Storage Lens administrator Create a S3 Storage Lens dashboard with a region to keep dashboard state: Still in the same form, you should see the organization linked to the service, and include all accounts and all regions: Can select metrics and defined new metrics, here we use the free ones: Metrics will help to find out how much storage you have across the entire organization . Which are the fastest - growing buckets and prefixes ... We can specify advanced metrics to get prefix - level aggregation . Free metrics are available for queries for a 14 - day period , and advanced metrics are available for queries for a 15 - month period . We export metrics generated every day, in CSV or Parquet format, and save result in the parent account's s3 bucket: Finally the data should be encrypted at rest, so using S3 managed keys will be the default: The dashboard is created and will get data in the next 24h to 48h. Lens pricing can be found in the Management and Analytics tab in S3 pricing page.","title":"S3 Storage Lens"},{"location":"playground/s3-org-billing/#add-files-in-buckets","text":"Account 1 (boyerje): add buckets in regions us-west-2 and us-east-1 aws w3 cp ... s3://jb-data-set/ Account 2 (jerome boyer): add buckets in regions us-west-1","title":"Add files in buckets"},{"location":"playground/s3-org-billing/#resources","text":"S3 Storage Lens user guide","title":"Resources"},{"location":"playground/spark-emr/","text":"Data analytics with Spark and EMR \u00b6 Gender - Age Analytics \u00b6 This is based on the tutorial from A Cloud Guru on Data Analytics with Spark and EMR, with some new adaptation. The problem statement The marketing department has personal information in thousand of CSV files and wants to run analytics by counting the gender per age groups... Hundred CSV files are in S3 bucket. Using s3-dist-cp to move them into the EMR cluster. Use EMR to run spark job. Define Spark processing in python to read all CSV files from HDFS then group records by age and gender then count records and order the results in descending mode. Result in HDFS. Copy result back to S3. Error 11/29 Still issue in S3 to HDFS step Manual Steps \u00b6 Create a EMR cluster last version (6.9) with Spark runtime 3.3, Hadoop, Hue, Hive and Fig Do not change the networking or cluster config... Change the EC2 to m4.large , if it is available within the AWS Region you selected and use only one instance for the core node: Set a cluster name: Do not use key pair as we do not need to SSH on core node Once the cluster is started, get the URL of the HDFS Name Node console and keep note of the associated port number Update the Security Groups for Master : to authorize access to the port number from anywhere 0.0.0.0/0. Be sure to use Custom TCP protocol for the Inbound rule: Open the HDFS Name Node URL in a web browser to get visibility into the files in our Hadoop cluster. Upload data files (folder upload) to S3 bucket. (The data files are in user-data-acg.zip) Use s3-dist-cp as a Step (type: Customer jar) to copy data from S3 to HDFS. (This command is defined in command-runner.jar ). Here is the arguments to use: s3-dist-cp --src = s3://jb-data-set/user-data-acg/ --dest = hdfs:/// Running the task... Create a Step to run the python code Create a S3 bucket as target for the output: s3://jb-data-set/gender-age-output Define a new Step with s3-disp-cp from HDFS to S3. s3-dist-cp --src = hdfs:///results --dest = s3://jb-data-set/gender-age-output spark-summit hdfs://pyspark-script/gender-age-count.py s3://emr-scripts-403993201276/scripts/gender-age-count.py AWS CLI based deployment \u00b6 Under the labs/analytics/ folder. Create the EMR cluster with Spark, Hadoop and Hive (version 6.9) ./emr-starting/create-cluster.sh Under gender-age/data folder unzip the user-data-acg.zip then upload the user-data-acg folder to a s3 bucket Upload data to s3 bucket aws s3 cp gender-age/data/user-data-acg s3://jb-data-set/ * Deploy the S3 to HDFS step cd gender-age ./s3-to-hdfs.sh * Start the python * Copy the HDFS result to S3 Clean up \u00b6 Delete cluster: ./emr-starting/delete-cluster.sh Delete S3 bucket content and buckets Remove emr roles in IAM","title":"Analytics on EMR"},{"location":"playground/spark-emr/#data-analytics-with-spark-and-emr","text":"","title":"Data analytics with Spark and EMR"},{"location":"playground/spark-emr/#gender-age-analytics","text":"This is based on the tutorial from A Cloud Guru on Data Analytics with Spark and EMR, with some new adaptation. The problem statement The marketing department has personal information in thousand of CSV files and wants to run analytics by counting the gender per age groups... Hundred CSV files are in S3 bucket. Using s3-dist-cp to move them into the EMR cluster. Use EMR to run spark job. Define Spark processing in python to read all CSV files from HDFS then group records by age and gender then count records and order the results in descending mode. Result in HDFS. Copy result back to S3. Error 11/29 Still issue in S3 to HDFS step","title":"Gender - Age Analytics"},{"location":"playground/spark-emr/#manual-steps","text":"Create a EMR cluster last version (6.9) with Spark runtime 3.3, Hadoop, Hue, Hive and Fig Do not change the networking or cluster config... Change the EC2 to m4.large , if it is available within the AWS Region you selected and use only one instance for the core node: Set a cluster name: Do not use key pair as we do not need to SSH on core node Once the cluster is started, get the URL of the HDFS Name Node console and keep note of the associated port number Update the Security Groups for Master : to authorize access to the port number from anywhere 0.0.0.0/0. Be sure to use Custom TCP protocol for the Inbound rule: Open the HDFS Name Node URL in a web browser to get visibility into the files in our Hadoop cluster. Upload data files (folder upload) to S3 bucket. (The data files are in user-data-acg.zip) Use s3-dist-cp as a Step (type: Customer jar) to copy data from S3 to HDFS. (This command is defined in command-runner.jar ). Here is the arguments to use: s3-dist-cp --src = s3://jb-data-set/user-data-acg/ --dest = hdfs:/// Running the task... Create a Step to run the python code Create a S3 bucket as target for the output: s3://jb-data-set/gender-age-output Define a new Step with s3-disp-cp from HDFS to S3. s3-dist-cp --src = hdfs:///results --dest = s3://jb-data-set/gender-age-output spark-summit hdfs://pyspark-script/gender-age-count.py s3://emr-scripts-403993201276/scripts/gender-age-count.py","title":"Manual Steps"},{"location":"playground/spark-emr/#aws-cli-based-deployment","text":"Under the labs/analytics/ folder. Create the EMR cluster with Spark, Hadoop and Hive (version 6.9) ./emr-starting/create-cluster.sh Under gender-age/data folder unzip the user-data-acg.zip then upload the user-data-acg folder to a s3 bucket Upload data to s3 bucket aws s3 cp gender-age/data/user-data-acg s3://jb-data-set/ * Deploy the S3 to HDFS step cd gender-age ./s3-to-hdfs.sh * Start the python * Copy the HDFS result to S3","title":"AWS CLI based deployment"},{"location":"playground/spark-emr/#clean-up","text":"Delete cluster: ./emr-starting/delete-cluster.sh Delete S3 bucket content and buckets Remove emr roles in IAM","title":"Clean up"},{"location":"sa/psa-role/","text":"Principal Solution Architect \u00b6 Partner to navigate the 200+ services, deploy your solution to the cloud Technologist Evangelist and educator Role \u00b6 technical leader and a strategic influencer like a CTO architect solutions to significantly complex problems, high ambiguity, leverage technical, industry, and business context expertise (e.g., outcome priorities, customer experience, shared goals, business case) to influence the direction of longer-term business and technology strategies identify both immediate and future risks and constraints advise customers on how to make the right trade-offs in solutions design (e.g., extensibility, flexibility, scalability, maintainability) able to dive deeply into technical details own the design and delivery of a program of customer solutions including the overall strategy and end-to-end architecture. Apply the design principles of security, reliability, cost optimization, operational excellence, and performance efficiency. proactively look for opportunities to scale the solution to benefit other customers with similar problems or requirements lead the curation of thought leadership content and ensure delivered content is relevant to customer needs. Attitude \u00b6 relentlessly simplify and are able to deconstruct extraordinarily complex problems into their constituent building blocks, accelerating customer adoption and/or enabling teams across the organization to work in parallel on the problem. recognize problems both inside and outside your area, build consensus around a vision, and drive resolution leveraging the right resources Associate certification study \u00b6","title":"SA"},{"location":"sa/psa-role/#principal-solution-architect","text":"Partner to navigate the 200+ services, deploy your solution to the cloud Technologist Evangelist and educator","title":"Principal Solution Architect"},{"location":"sa/psa-role/#role","text":"technical leader and a strategic influencer like a CTO architect solutions to significantly complex problems, high ambiguity, leverage technical, industry, and business context expertise (e.g., outcome priorities, customer experience, shared goals, business case) to influence the direction of longer-term business and technology strategies identify both immediate and future risks and constraints advise customers on how to make the right trade-offs in solutions design (e.g., extensibility, flexibility, scalability, maintainability) able to dive deeply into technical details own the design and delivery of a program of customer solutions including the overall strategy and end-to-end architecture. Apply the design principles of security, reliability, cost optimization, operational excellence, and performance efficiency. proactively look for opportunities to scale the solution to benefit other customers with similar problems or requirements lead the curation of thought leadership content and ensure delivered content is relevant to customer needs.","title":"Role"},{"location":"sa/psa-role/#attitude","text":"relentlessly simplify and are able to deconstruct extraordinarily complex problems into their constituent building blocks, accelerating customer adoption and/or enabling teams across the organization to work in parallel on the problem. recognize problems both inside and outside your area, build consensus around a vision, and drive resolution leveraging the right resources","title":"Attitude"},{"location":"sa/psa-role/#associate-certification-study","text":"","title":"Associate certification study"},{"location":"sa/resilience/","text":"Resilience \u00b6 Resiliency is the ability of a workload to recover from infrastructure or service disruptions, dynamically acquire computing resources to meet demand, and mitigate disruptions, such as misconfigurations or transient network issues. Apply the sharing responsibility of resiliency. Fault isolation concepts \u00b6 Control plane is a complex orchestration with many dependencies so are more difficult to make them resilient. But the lower level of APIs makes it less risky. Still they will fail more often. Latency is not a strong requirement, 100s ms. Do not rely on Auto Scaling Group for EC2 in case of AZ failure, but plan for overcapacity to support the load after AZ failure. Examples of control plan and data plane Service Control plane Data plane Amazon S3 create bucket, put bucket policy GetObject, PutObject DynamoDB Create table, update table GetItem ... ELB CreateLoadBalancer, CreateTargetGroup The load balancer itself Route 53 CreateHostedZone ... DNS resolution, health checks IAM CreateRole Authn, Authz RDS Create DB instance The data base Static stability: system still running even dependencies fail without the need to make changes. Use the following approaches: prevent circular dependencies, pre-provision capacity, maintain existing state, eliminate synchronous interaction. One way AWS achieve static statbility is by removing control plane deplendencis from the data plane in AWS services. Relying on data plane operaions for recovery can help make your system more staticall stable, this may include pre-provisioning resources, or relying on data plane operations. Region, AZ \u00b6 Non-AZ affinity: a write operation can cross AZs and with data replications it can make a lot of hops AZ-affinity helps to reduce the number of hops, but it enforces using NLB and not ALB. AWS Partitions \u00b6 There are Isolated infrastructure and services. There are commercial partition, China , and GovCloud partition. Within a partition there are regions. Not cross IAM definition sharing. Type Service Planes Zonal RDS, EC2, EBS Control plane is regional while data plane is zonal Regional S3, SQS, DynamoDB Control plane and data plane are regional Global CloudFront, Global accelerator, Route 53, IAM Control plane is single region and data plane is global There are three categories of global services: partitional, Edge and globally-scoped operations. Global services havd a single control plane and a distributed, highly available data plane. Avoid control plane dependencies in global services in your recovery path, implement static stability. Cell-based architecture and shuffle sharding \u00b6 Goal is to reduce the blast-radius. Use the concept of Cell which is a construct to isolate compute, routing, and storage (workload). Cells are not AZs but cross AZs. Cell shares nothing with each other. Cell can scale-up and out. Cell have a maximum size. It is not fitting for all type of workloads. API Gateway uses this Cell-based architecture. Some e-commerce fullfilment delivery centers use cell-based architecture. Considerations to address: Cell size Router robustness Partitioning dimension Cross-cell use cases Cell migration Alignment to AZs and Regions Shards is another construct to isolate blast radius, but now at the level of resources. Any serverless services is using the concept of shuffle sharding. High Availability \u00b6 Availability = uptime / (uptime + downtime) Availability = a1 * a2 *.... * an . It cannot be better than it's least available dependency. Spare components improves availability. Measuring availability \u00b6 Metrics to consider: Mean time between failure MTBF, try to increase it Mean time to detection MTTD Mean time to repair MTTR consider server-side and client-side request success rate. Define what unit of work to be used: HTTP request, message in queue, async job. define downtime, for example, drop below 95% availabiltiy for any API during a 5 minutes window. Use CW embedded metric format EMF to combine logs and metrics Latency \u00b6 Latency impacts availability. Analyze histograms for latency distribution trends. Use percentiles and trimmed man to measure latency. Multi-AZ patterns: AZ gray failure \u00b6 System follows differential observability. Different perspective: system versus application Detection tools: cloudwatch contributor insights. Disaster recovery \u00b6 Review the core principles of DR and then the AWS options whitepaper . Data Migration Service \u00b6 As a managed service, AWS DMS is a server (EC2) in the AWS Cloud that runs replication software, to move data from one database to another running in AWS. Schema transformation can be performed using Schema Conversion Tool (SCT). SCT can run on-premises. Sources can be on-premises DBs, and the targets can still be on-premises DBs but really a lot of AWS data store like RDS, Redshift, DynamoDB, S3, Elasticache, Kinesis data streams, DocumentDB... We can use continuous replication with DMS: First (1) use the AWS Schema Conversion Tool to convert the source schema and code to match that of the target database, then (2) use the AWS Database Migration Service to migrate data from the source database to the target database. Example of configuration, with EC2 instance type, engine version, allocated storage space for the replication instance, network connectivity... Once server is defined, we need to define data provider to specify source and target endpoints and create a migration project to do schema conversion (from Oracle or SQL server to MySQL or PostgreSQL as target). It is possible to leverage AWS Database Migration Service (AWS DMS) as a bridge between Amazon S3 and Amazon Kinesis Data Streams as an easier way to do so: See Streaming Data from S3 to Kinesis using DMS . AWS DMS allows migration of full and change data capture (CDC) files to these services. DMS FAQ Replication to Aurora \u00b6 For MySQL engine we have different options: Use RDS DB snapshot from RDS and restore it in Aurora Create an Autora read replica from the RDS MySQL source, when replication lag is 0, we can then promote Aurora as Write and Read. For external MySQL, use XtraBackup to create a file backup, upload it to S3 and import the file into Aurora from S3. Or use mysqldump utility to move to Aurora MySQL DB. Use DMS if both DBs run in parallel AWS Backup \u00b6 Fully-managed service that makes it easy to centralize and automate data protection across AWS services, in the cloud, and on premises. It supports cross-region backups and in multiple AWS accounts across the entire AWS Organization. Src: EC2, EBS, EFS, Amazon FSx for windows, Lustre, and Storage Gateway, RDS, DynamoDB.. Target is S3 bucket or Vault Lock. Vault lock is used for Write Once Read Many state. It is to defend on malicious delete as backup cannot be deleted. We can create automated backup schedules and retention policies, lifecycle policies to expire unncecessary backups after a period of time. AWS Backup helps to support the regulatory compliance or business policies for data protection. Application Discovery Service \u00b6 Managed service to help plan the migration to the AWS cloud by collecting usage and configuration data about the on-premises servers. It is integrated with AWS Migration Hub, which simplifies your migration tracking as it aggregates your migration status information into a single console. It offers two ways of performing discovery: Agentless: work on VM inventory, configuration, and performance history Agent-based: by deploying AD Agent on each of your VMs and physical servers, it collects static configuration data, detailed time-series system-performance information, inbound and outbound network connections, and processes that are running. Resilience Patterns \u00b6 client side, use circuit braker, retries with jitter, multiplexing connection with new protocol like HTTP/3, gRPC server side, apply caching strategy, cache-aside is more resilient, inline cache, like DynamoDB DAX, may be a single point of failure.","title":"Resilience"},{"location":"sa/resilience/#resilience","text":"Resiliency is the ability of a workload to recover from infrastructure or service disruptions, dynamically acquire computing resources to meet demand, and mitigate disruptions, such as misconfigurations or transient network issues. Apply the sharing responsibility of resiliency.","title":"Resilience"},{"location":"sa/resilience/#fault-isolation-concepts","text":"Control plane is a complex orchestration with many dependencies so are more difficult to make them resilient. But the lower level of APIs makes it less risky. Still they will fail more often. Latency is not a strong requirement, 100s ms. Do not rely on Auto Scaling Group for EC2 in case of AZ failure, but plan for overcapacity to support the load after AZ failure. Examples of control plan and data plane Service Control plane Data plane Amazon S3 create bucket, put bucket policy GetObject, PutObject DynamoDB Create table, update table GetItem ... ELB CreateLoadBalancer, CreateTargetGroup The load balancer itself Route 53 CreateHostedZone ... DNS resolution, health checks IAM CreateRole Authn, Authz RDS Create DB instance The data base Static stability: system still running even dependencies fail without the need to make changes. Use the following approaches: prevent circular dependencies, pre-provision capacity, maintain existing state, eliminate synchronous interaction. One way AWS achieve static statbility is by removing control plane deplendencis from the data plane in AWS services. Relying on data plane operaions for recovery can help make your system more staticall stable, this may include pre-provisioning resources, or relying on data plane operations.","title":"Fault isolation concepts"},{"location":"sa/resilience/#region-az","text":"Non-AZ affinity: a write operation can cross AZs and with data replications it can make a lot of hops AZ-affinity helps to reduce the number of hops, but it enforces using NLB and not ALB.","title":"Region, AZ"},{"location":"sa/resilience/#aws-partitions","text":"There are Isolated infrastructure and services. There are commercial partition, China , and GovCloud partition. Within a partition there are regions. Not cross IAM definition sharing. Type Service Planes Zonal RDS, EC2, EBS Control plane is regional while data plane is zonal Regional S3, SQS, DynamoDB Control plane and data plane are regional Global CloudFront, Global accelerator, Route 53, IAM Control plane is single region and data plane is global There are three categories of global services: partitional, Edge and globally-scoped operations. Global services havd a single control plane and a distributed, highly available data plane. Avoid control plane dependencies in global services in your recovery path, implement static stability.","title":"AWS Partitions"},{"location":"sa/resilience/#cell-based-architecture-and-shuffle-sharding","text":"Goal is to reduce the blast-radius. Use the concept of Cell which is a construct to isolate compute, routing, and storage (workload). Cells are not AZs but cross AZs. Cell shares nothing with each other. Cell can scale-up and out. Cell have a maximum size. It is not fitting for all type of workloads. API Gateway uses this Cell-based architecture. Some e-commerce fullfilment delivery centers use cell-based architecture. Considerations to address: Cell size Router robustness Partitioning dimension Cross-cell use cases Cell migration Alignment to AZs and Regions Shards is another construct to isolate blast radius, but now at the level of resources. Any serverless services is using the concept of shuffle sharding.","title":"Cell-based architecture and shuffle sharding"},{"location":"sa/resilience/#high-availability","text":"Availability = uptime / (uptime + downtime) Availability = a1 * a2 *.... * an . It cannot be better than it's least available dependency. Spare components improves availability.","title":"High Availability"},{"location":"sa/resilience/#measuring-availability","text":"Metrics to consider: Mean time between failure MTBF, try to increase it Mean time to detection MTTD Mean time to repair MTTR consider server-side and client-side request success rate. Define what unit of work to be used: HTTP request, message in queue, async job. define downtime, for example, drop below 95% availabiltiy for any API during a 5 minutes window. Use CW embedded metric format EMF to combine logs and metrics","title":"Measuring availability"},{"location":"sa/resilience/#latency","text":"Latency impacts availability. Analyze histograms for latency distribution trends. Use percentiles and trimmed man to measure latency.","title":"Latency"},{"location":"sa/resilience/#multi-az-patterns-az-gray-failure","text":"System follows differential observability. Different perspective: system versus application Detection tools: cloudwatch contributor insights.","title":"Multi-AZ patterns: AZ gray failure"},{"location":"sa/resilience/#disaster-recovery","text":"Review the core principles of DR and then the AWS options whitepaper .","title":"Disaster recovery"},{"location":"sa/resilience/#data-migration-service","text":"As a managed service, AWS DMS is a server (EC2) in the AWS Cloud that runs replication software, to move data from one database to another running in AWS. Schema transformation can be performed using Schema Conversion Tool (SCT). SCT can run on-premises. Sources can be on-premises DBs, and the targets can still be on-premises DBs but really a lot of AWS data store like RDS, Redshift, DynamoDB, S3, Elasticache, Kinesis data streams, DocumentDB... We can use continuous replication with DMS: First (1) use the AWS Schema Conversion Tool to convert the source schema and code to match that of the target database, then (2) use the AWS Database Migration Service to migrate data from the source database to the target database. Example of configuration, with EC2 instance type, engine version, allocated storage space for the replication instance, network connectivity... Once server is defined, we need to define data provider to specify source and target endpoints and create a migration project to do schema conversion (from Oracle or SQL server to MySQL or PostgreSQL as target). It is possible to leverage AWS Database Migration Service (AWS DMS) as a bridge between Amazon S3 and Amazon Kinesis Data Streams as an easier way to do so: See Streaming Data from S3 to Kinesis using DMS . AWS DMS allows migration of full and change data capture (CDC) files to these services. DMS FAQ","title":"Data Migration Service"},{"location":"sa/resilience/#replication-to-aurora","text":"For MySQL engine we have different options: Use RDS DB snapshot from RDS and restore it in Aurora Create an Autora read replica from the RDS MySQL source, when replication lag is 0, we can then promote Aurora as Write and Read. For external MySQL, use XtraBackup to create a file backup, upload it to S3 and import the file into Aurora from S3. Or use mysqldump utility to move to Aurora MySQL DB. Use DMS if both DBs run in parallel","title":"Replication to Aurora"},{"location":"sa/resilience/#aws-backup","text":"Fully-managed service that makes it easy to centralize and automate data protection across AWS services, in the cloud, and on premises. It supports cross-region backups and in multiple AWS accounts across the entire AWS Organization. Src: EC2, EBS, EFS, Amazon FSx for windows, Lustre, and Storage Gateway, RDS, DynamoDB.. Target is S3 bucket or Vault Lock. Vault lock is used for Write Once Read Many state. It is to defend on malicious delete as backup cannot be deleted. We can create automated backup schedules and retention policies, lifecycle policies to expire unncecessary backups after a period of time. AWS Backup helps to support the regulatory compliance or business policies for data protection.","title":"AWS Backup"},{"location":"sa/resilience/#application-discovery-service","text":"Managed service to help plan the migration to the AWS cloud by collecting usage and configuration data about the on-premises servers. It is integrated with AWS Migration Hub, which simplifies your migration tracking as it aggregates your migration status information into a single console. It offers two ways of performing discovery: Agentless: work on VM inventory, configuration, and performance history Agent-based: by deploying AD Agent on each of your VMs and physical servers, it collects static configuration data, detailed time-series system-performance information, inbound and outbound network connections, and processes that are running.","title":"Application Discovery Service"},{"location":"sa/resilience/#resilience-patterns","text":"client side, use circuit braker, retries with jitter, multiplexing connection with new protocol like HTTP/3, gRPC server side, apply caching strategy, cache-aside is more resilient, inline cache, like DynamoDB DAX, may be a single point of failure.","title":"Resilience Patterns"},{"location":"sa/saas/","text":"Software as a Service \u00b6 A licensing and delivery model whereby software is centrally managed and hosted by a provider and available to customers on a subsciption fee, or pay-per-user basis, or pay per usage basis. Concepts \u00b6 Everything done in SaaS is about multi-tenancy, data isolation and sharing resources like compute, networking and storage as part of the infrastructure to optimize cost of running the business. Figure 1 As illustrated in the figure above an onboarding / shared service component is needed to manage the multi-tenant platform. Any software vendor is bringing their own solution and it is highlighted as application specific domain. As an example, if we take a big data platform provider such application domain may look like in the following architecture diagram, which supports a map/reduce job execution environment with data ingestion, transformation and persistence: Figure 2 A control plane manages cluster of worker nodes. Each worker node runs an executor that executes jobs. Jobs are the custom piece of code in Python, Scala, Java which performs the data ingestion, data transformation & enrichment, MAP/Reduce logic and persists the results to distributed storage. Footprint scales dynamically based on aggregate load of all tenants. All tenants are managed via a single operational control plane or lens. The Analytic component is very important to help marketing team to understand how the customers are using the platform, and how to optimize the usage of the platform. For the application domain, new features are deployed to all tenants, more quickly via DevOps practices. Multi tenancy support approaches \u00b6 There are multiple patterns for multi-tenancy, some linked to business requirements and sometime to technical reasons: Figure 3 Silo : Each tenant gets unique set of infrastructure resources. As environments are partitioned, there is no cross-tenant impacts. Agility is compromised. Needed for strong regulatory and compliance. We can tune the configuration per tenant and get specific SLAs. It costs more and operations are more complex. The analytics services need to aggregate from those silos. Tenant may be mapped to AWS Account and a linked account to aggregate billing. Or based in VPC. Pool : Shared resources, centralized management, simplified deployment. Compliance is a challenge and cross-tenant impacts with a all or nothing availability. The advantages are cost optimization and operation. Bridge : A mix of silo and pool. Can be applied to the different level of the solution architecture, for example, web and data tier can be pool, and app layer in silo. It is important to decide what approach the SaaS architecture needs to support. Needs \u00b6 Agility is the major requirements to go to SaaS, which means: On-boarding without friction: including environment provisioning. Frequent feature release: get to customer's hands as quick as possible. Release on daily frequency should be possible. Rapid market response, to pivot to a new direction. Instant customer feedback: with metrics on feature adoption. The important metrics to consider: Usage, consumption, system/tenant health. Survey and customer satisfaction. Engagement data. Architecture Landscape \u00b6 There are a set of shared services that we will find in any SaaS solution which supports the high level view introduced in figure 1. Those services are: Admin Console : SaaS provider administrative application. It may include a landing web app to get tenant registering. The administration services are supporting the SaaS business. On-Boarding : Complex solution to provision all the environment. Identity : Identity is a very important element of SaaS. Connect users to tenant. Tenant management : As a multi-tenancy platform, tenant is a main business entity. User management : Each tenant has one to many users of the vendor software. Metering and billing : How to get tenant metrics and how to isolate billing. Analytics : Service responsible to gather usage metrics, and drive new use case of cost optimization, customer churn assessment... Monitoring and infrastructure management: For integrating into the cloud provider and manage compute, storage, networking resources. The figure below illustrates the integration of those services within the landscape: Figure 4 Amazon Cognito is used as an OpenID identity provider . The app domain is the specific ISV solution. DevSecOps is to support devOps practices and security validation. The shared services can be developed using a serverless / container approach or using AMI images for EC2 instances. We will adopt a container approach so we can more quickly adapt those service runtime execution according to the demand, and deploy them on Kubernetes cluster. The shared services deployment on EKS will look like in the following diagram within the VPC of the software vendor (not all services are presented): Figure 5 Some of those services will use AWS managed services like DynamoDB to persist tenant and user data, Cognito for authentication to their platform, EFS for sharing file, S3... The figure above demonstrates that the control-plane services are replicated into different availability zone, inside of the same EKS cluster. Bridge or Pool runs in the SaaS vendor VPC, and Web and App tiers are shared and persist data in same DB (may be different schema or a dedicated tenantID column in tables). Now on the SaaS provider's customer side we may use kubernetes namespace to deliver tenant isolation. The SaaS control plane runs and the customers own use of the solution runs in the same k8s cluster. Dedicated worker nodes can be used. In the figure 6, the ISV solution is instantiated as pods in different namespaces (gree, purple, red colors) for each tenant. The control plane, shared services run in the same cluster. All the solution is managed inside of this unique k8s cluster. Figure 6 Silo Isolation model may be achieve with VPC per tenant. Therefore the supporting approach will be to deploy the solution in dedicated EKS cluster in the customers VPC. Figure 7 Registering a tenant generates Terraform scripts to create the needed infrastructure and deploy the needed components to support one of the tenant isolation selected. The type of isolation can also being related to the tenant profile. Gold being on its own cluster for example. Coming back to the big-data processing platform example, we also need to consider data and storage partitioning , how to isolate data for each tenant. Long term persistence of object can be done in S3, now buckets can be defined in the VPC of the SaaS provider or in the customer's VPC. All the metadata about customer's interactions with the platform, the jaob execution can be ingected back to the ISV platform and then analytics can be used to do product and features usage assessment. As mentioned before the solution could also being deployed on EC2s which may looks like: Which maps to the following provisionned environment with classical HA deployment within a region / VPC, two AZs, private and public subnets and gateway & application load balancer. IAM roles and policies are used to support isolation. IAM policies should help isolating some resources in AWS services like S3 bucket or dynamoDB tables, while other like queues can be shared. Operations \u00b6 For SaaS, we need to focusing on monitoring the environments and applications health, and sometime at the tenant level (tenant experience). May be considering the following dashboard elements: Most active tenants. Tenant resources consumption. Feature / services usage. consumption is for resource usages, more an internal metric. % of resource used by a tenant. metering is a billing construct: # of users, product features usage,... Those numbers are very important to get how the revenue are coming from and the cost too. The figure below illustrates a classical e-commerce market place use case where basic tier customers are using a lot of resources and do not generate a lot of revenue, as they are trying product features and no more, while advanced tier better target their usage and own sell operation and therefore are generating bigger revenue. What to measure when focusing on consumption? request count, execution time (latency of a function), CPU impact, Memory impact? Deeper dive \u00b6 The content on this note comes from: Saas at AWS Tod Golding form AWS SaaS factory: Deconstructing SaaS: A Deep Dive into Building Multi-tenant Solu has the diagrams above plus a deep discussion on how to secure and support tenants to access pooled resources like content in DynamoDB. (Need further study). Serverless SaaS documentation DevCon 2022: Building a customizable, multi-tenant serverless orchestration framework for bulk-data ingestion . SaaS youtube playlist . SaaS head start, using ready-made solutions to accelerate adoption . Open source project - SaaS Boost with the corresponding AWS SaaS boost git repo . AWS SaaS factory reference architecture git repo Aug 2021 blog: Building a Multi-Tenant SaaS Solution Using AWS Serverless Services","title":"SaaS"},{"location":"sa/saas/#software-as-a-service","text":"A licensing and delivery model whereby software is centrally managed and hosted by a provider and available to customers on a subsciption fee, or pay-per-user basis, or pay per usage basis.","title":"Software as a Service"},{"location":"sa/saas/#concepts","text":"Everything done in SaaS is about multi-tenancy, data isolation and sharing resources like compute, networking and storage as part of the infrastructure to optimize cost of running the business. Figure 1 As illustrated in the figure above an onboarding / shared service component is needed to manage the multi-tenant platform. Any software vendor is bringing their own solution and it is highlighted as application specific domain. As an example, if we take a big data platform provider such application domain may look like in the following architecture diagram, which supports a map/reduce job execution environment with data ingestion, transformation and persistence: Figure 2 A control plane manages cluster of worker nodes. Each worker node runs an executor that executes jobs. Jobs are the custom piece of code in Python, Scala, Java which performs the data ingestion, data transformation & enrichment, MAP/Reduce logic and persists the results to distributed storage. Footprint scales dynamically based on aggregate load of all tenants. All tenants are managed via a single operational control plane or lens. The Analytic component is very important to help marketing team to understand how the customers are using the platform, and how to optimize the usage of the platform. For the application domain, new features are deployed to all tenants, more quickly via DevOps practices.","title":"Concepts"},{"location":"sa/saas/#multi-tenancy-support-approaches","text":"There are multiple patterns for multi-tenancy, some linked to business requirements and sometime to technical reasons: Figure 3 Silo : Each tenant gets unique set of infrastructure resources. As environments are partitioned, there is no cross-tenant impacts. Agility is compromised. Needed for strong regulatory and compliance. We can tune the configuration per tenant and get specific SLAs. It costs more and operations are more complex. The analytics services need to aggregate from those silos. Tenant may be mapped to AWS Account and a linked account to aggregate billing. Or based in VPC. Pool : Shared resources, centralized management, simplified deployment. Compliance is a challenge and cross-tenant impacts with a all or nothing availability. The advantages are cost optimization and operation. Bridge : A mix of silo and pool. Can be applied to the different level of the solution architecture, for example, web and data tier can be pool, and app layer in silo. It is important to decide what approach the SaaS architecture needs to support.","title":"Multi tenancy support approaches"},{"location":"sa/saas/#needs","text":"Agility is the major requirements to go to SaaS, which means: On-boarding without friction: including environment provisioning. Frequent feature release: get to customer's hands as quick as possible. Release on daily frequency should be possible. Rapid market response, to pivot to a new direction. Instant customer feedback: with metrics on feature adoption. The important metrics to consider: Usage, consumption, system/tenant health. Survey and customer satisfaction. Engagement data.","title":"Needs"},{"location":"sa/saas/#architecture-landscape","text":"There are a set of shared services that we will find in any SaaS solution which supports the high level view introduced in figure 1. Those services are: Admin Console : SaaS provider administrative application. It may include a landing web app to get tenant registering. The administration services are supporting the SaaS business. On-Boarding : Complex solution to provision all the environment. Identity : Identity is a very important element of SaaS. Connect users to tenant. Tenant management : As a multi-tenancy platform, tenant is a main business entity. User management : Each tenant has one to many users of the vendor software. Metering and billing : How to get tenant metrics and how to isolate billing. Analytics : Service responsible to gather usage metrics, and drive new use case of cost optimization, customer churn assessment... Monitoring and infrastructure management: For integrating into the cloud provider and manage compute, storage, networking resources. The figure below illustrates the integration of those services within the landscape: Figure 4 Amazon Cognito is used as an OpenID identity provider . The app domain is the specific ISV solution. DevSecOps is to support devOps practices and security validation. The shared services can be developed using a serverless / container approach or using AMI images for EC2 instances. We will adopt a container approach so we can more quickly adapt those service runtime execution according to the demand, and deploy them on Kubernetes cluster. The shared services deployment on EKS will look like in the following diagram within the VPC of the software vendor (not all services are presented): Figure 5 Some of those services will use AWS managed services like DynamoDB to persist tenant and user data, Cognito for authentication to their platform, EFS for sharing file, S3... The figure above demonstrates that the control-plane services are replicated into different availability zone, inside of the same EKS cluster. Bridge or Pool runs in the SaaS vendor VPC, and Web and App tiers are shared and persist data in same DB (may be different schema or a dedicated tenantID column in tables). Now on the SaaS provider's customer side we may use kubernetes namespace to deliver tenant isolation. The SaaS control plane runs and the customers own use of the solution runs in the same k8s cluster. Dedicated worker nodes can be used. In the figure 6, the ISV solution is instantiated as pods in different namespaces (gree, purple, red colors) for each tenant. The control plane, shared services run in the same cluster. All the solution is managed inside of this unique k8s cluster. Figure 6 Silo Isolation model may be achieve with VPC per tenant. Therefore the supporting approach will be to deploy the solution in dedicated EKS cluster in the customers VPC. Figure 7 Registering a tenant generates Terraform scripts to create the needed infrastructure and deploy the needed components to support one of the tenant isolation selected. The type of isolation can also being related to the tenant profile. Gold being on its own cluster for example. Coming back to the big-data processing platform example, we also need to consider data and storage partitioning , how to isolate data for each tenant. Long term persistence of object can be done in S3, now buckets can be defined in the VPC of the SaaS provider or in the customer's VPC. All the metadata about customer's interactions with the platform, the jaob execution can be ingected back to the ISV platform and then analytics can be used to do product and features usage assessment. As mentioned before the solution could also being deployed on EC2s which may looks like: Which maps to the following provisionned environment with classical HA deployment within a region / VPC, two AZs, private and public subnets and gateway & application load balancer. IAM roles and policies are used to support isolation. IAM policies should help isolating some resources in AWS services like S3 bucket or dynamoDB tables, while other like queues can be shared.","title":"Architecture Landscape"},{"location":"sa/saas/#operations","text":"For SaaS, we need to focusing on monitoring the environments and applications health, and sometime at the tenant level (tenant experience). May be considering the following dashboard elements: Most active tenants. Tenant resources consumption. Feature / services usage. consumption is for resource usages, more an internal metric. % of resource used by a tenant. metering is a billing construct: # of users, product features usage,... Those numbers are very important to get how the revenue are coming from and the cost too. The figure below illustrates a classical e-commerce market place use case where basic tier customers are using a lot of resources and do not generate a lot of revenue, as they are trying product features and no more, while advanced tier better target their usage and own sell operation and therefore are generating bigger revenue. What to measure when focusing on consumption? request count, execution time (latency of a function), CPU impact, Memory impact?","title":"Operations"},{"location":"sa/saas/#deeper-dive","text":"The content on this note comes from: Saas at AWS Tod Golding form AWS SaaS factory: Deconstructing SaaS: A Deep Dive into Building Multi-tenant Solu has the diagrams above plus a deep discussion on how to secure and support tenants to access pooled resources like content in DynamoDB. (Need further study). Serverless SaaS documentation DevCon 2022: Building a customizable, multi-tenant serverless orchestration framework for bulk-data ingestion . SaaS youtube playlist . SaaS head start, using ready-made solutions to accelerate adoption . Open source project - SaaS Boost with the corresponding AWS SaaS boost git repo . AWS SaaS factory reference architecture git repo Aug 2021 blog: Building a Multi-Tenant SaaS Solution Using AWS Serverless Services","title":"Deeper dive"},{"location":"sa/sol-design/","text":"Solution design with AWS services \u00b6 This section presents some examples of SA design. Stateless with persistence app \u00b6 Starting simple with one EC2 and the webapp deployed on it. Upgrading the EC2 instance type, brings the webapp down. Use Elastic IP address so client keep IP @. Note Recall that you need to be able to SSH to the EC2 instance, install Java or Python or nodejs, scp the tar of you app to the linux EC2 /home/users or somewhere else. Do not forget to use the .pem file downloaded when creating the EC2. scp -i ec2host.pem ... But to be more efficient we can add a DNS domain and subdomain with Route 53 records, two ELBs with Health Checks, one per AZ, restricted security groups and public and private subnets, auto scaling group, and may be use one reserved EC2 instance per AZ to reduce long term cost. Stateful app \u00b6 Keep data (shopping cart) into session. We can use the stickiness on ELB and session affinity. Or use a user cookie, with the app verifying the cookie content. When we need to keep more data than what the cookie can save, then we keep the sessionID in the cookie and deploy an ElasticCache to store data with the key of sessionID. Final transactional data can be persisted in RDS, and with RDS replication we can route specific traffic (\"/products\") which is read/only to the RDS Read replicas. Which can also being supported by having the product in the ElasticCache. Sharing images solution \u00b6 Disaster Recovery Solution \u00b6 We try to address disaster and recovery for different needs, knowing that not all applications need active/active deployment. Backup multi-regions \u00b6 The simplest resilience solution is to use backup and restore mechanism. Data and configuration can be moved to S3 in the second region. For even longer time we can use Glacier. Use database service to ensure HA at the zone level, and replicate data within AZ. RPO will be average time between snapshots - and RTO at the day level. Warm region \u00b6 For applications, where we want to limit out of services time, the approach is to replicate AMI images so app servers, in DR region, can be restarted quickly. And Database are replicated and warm on the second region. Web servers are also warm but not receiving traffic. If something go down in region 1, the internet facing router (53) will route to local balancers in second region. RTO is now in minutes, and RPO average time between DB snapshots. Active - Active between multi regions \u00b6 Write global - read local pattern \u00b6 Users close to one region will read from this region and all write operations go to a global service / region. Database replications and snapshot replications are done to keep data eventually consistent between regions. Those synchronisations are in sub second. Write to origin - read local pattern \u00b6 To increase in complexity, R/W can go the local region. So when a user writes new records, he/she is associated to a region, so the application is sharding the data. When the user moved to another region, write operation will still go to the first region, while read could happened on the region close to him. This applies to applications with write to read ratio around 50%. Write / read local (anti) pattern \u00b6 This pattern uses two master DB, one in each region so user can write and read locally. Dual writes, in each region, at the same time may generate the same key but record will have different data. You have inconsistency, and it is difficult to figure out, and rollback. So use this pattern only if you cannot do the two previous patterns. AWS Services supporting HA and DR multi-regions \u00b6 S3 EBS dynamoDB SaaS deployment for an ISV \u00b6 See also SaaS considerations . The ISV provides a data platform to manage data with performance and feature set compare to data warehouse but at the scale and low cost of data lake. On top of the data their platform helps to develop ML model, visualize data and build integrate view of distributed, structured and unstructured data. They developed a multi-tenancy SaaS platform to manage their customer deployments. Customers will create instance of this managed service in their own VPC and integrate with their own data source and data pipeline, to organize this data processing into data ingestion, data engineering, serving layer and analytics. Part of their platform is also to provide easy integration into AWS data services like Kinesis, S3, RDS, Redshift\u2026 This ISV is gathering data from the customer usage of their platform, get real time metrics, and manage all this in the AWS cloud using existing managed services as S3, Glacier for long persistence, streaming, redshift,\u2026 Kinesis","title":"Solution design"},{"location":"sa/sol-design/#solution-design-with-aws-services","text":"This section presents some examples of SA design.","title":"Solution design with AWS services"},{"location":"sa/sol-design/#stateless-with-persistence-app","text":"Starting simple with one EC2 and the webapp deployed on it. Upgrading the EC2 instance type, brings the webapp down. Use Elastic IP address so client keep IP @. Note Recall that you need to be able to SSH to the EC2 instance, install Java or Python or nodejs, scp the tar of you app to the linux EC2 /home/users or somewhere else. Do not forget to use the .pem file downloaded when creating the EC2. scp -i ec2host.pem ... But to be more efficient we can add a DNS domain and subdomain with Route 53 records, two ELBs with Health Checks, one per AZ, restricted security groups and public and private subnets, auto scaling group, and may be use one reserved EC2 instance per AZ to reduce long term cost.","title":"Stateless with persistence app"},{"location":"sa/sol-design/#stateful-app","text":"Keep data (shopping cart) into session. We can use the stickiness on ELB and session affinity. Or use a user cookie, with the app verifying the cookie content. When we need to keep more data than what the cookie can save, then we keep the sessionID in the cookie and deploy an ElasticCache to store data with the key of sessionID. Final transactional data can be persisted in RDS, and with RDS replication we can route specific traffic (\"/products\") which is read/only to the RDS Read replicas. Which can also being supported by having the product in the ElasticCache.","title":"Stateful app"},{"location":"sa/sol-design/#sharing-images-solution","text":"","title":"Sharing images solution"},{"location":"sa/sol-design/#disaster-recovery-solution","text":"We try to address disaster and recovery for different needs, knowing that not all applications need active/active deployment.","title":"Disaster Recovery Solution"},{"location":"sa/sol-design/#backup-multi-regions","text":"The simplest resilience solution is to use backup and restore mechanism. Data and configuration can be moved to S3 in the second region. For even longer time we can use Glacier. Use database service to ensure HA at the zone level, and replicate data within AZ. RPO will be average time between snapshots - and RTO at the day level.","title":"Backup multi-regions"},{"location":"sa/sol-design/#warm-region","text":"For applications, where we want to limit out of services time, the approach is to replicate AMI images so app servers, in DR region, can be restarted quickly. And Database are replicated and warm on the second region. Web servers are also warm but not receiving traffic. If something go down in region 1, the internet facing router (53) will route to local balancers in second region. RTO is now in minutes, and RPO average time between DB snapshots.","title":"Warm region"},{"location":"sa/sol-design/#active-active-between-multi-regions","text":"","title":"Active - Active between multi regions"},{"location":"sa/sol-design/#aws-services-supporting-ha-and-dr-multi-regions","text":"S3 EBS dynamoDB","title":"AWS Services supporting HA and DR multi-regions"},{"location":"sa/sol-design/#saas-deployment-for-an-isv","text":"See also SaaS considerations . The ISV provides a data platform to manage data with performance and feature set compare to data warehouse but at the scale and low cost of data lake. On top of the data their platform helps to develop ML model, visualize data and build integrate view of distributed, structured and unstructured data. They developed a multi-tenancy SaaS platform to manage their customer deployments. Customers will create instance of this managed service in their own VPC and integrate with their own data source and data pipeline, to organize this data processing into data ingestion, data engineering, serving layer and analytics. Part of their platform is also to provide easy integration into AWS data services like Kinesis, S3, RDS, Redshift\u2026 This ISV is gathering data from the customer usage of their platform, get real time metrics, and manage all this in the AWS cloud using existing managed services as S3, Glacier for long persistence, streaming, redshift,\u2026 Kinesis","title":"SaaS deployment for an ISV"},{"location":"sa/well-architectured/","text":"AWS Well Architectured \u00b6 The AWS Well-Architected Framework helps you understand the pros and cons of decisions you make while building systems on AWS. By using the Framework, customers will learn architectural best practices for designing and operating reliable, secure, efficient, and cost-effective systems in the cloud. Those are the questions to ask for designing a cloud native solution by understanding the potential impact . All hardware are becoming software. Workload represents interrelated applications, infrastructure, policies, governance and operations. Six pilards \u00b6 When architecting technology solutions, never neglect the six pillars of: Operational Excellence Security Reliability Performance Efficiency Cost Optimization Sustainability Design Principles \u00b6 Stop guessing your capacity needs : use as much or as little capacity as you need, and scale up and down automatically. Test systems at production scale , then decommission the resources. Automate to make architectural experimentation easier . Allow for evolutionary architectures : the capability to automate and test on demand lowers the risk of impact from design changes. This allows systems to evolve over time so that businesses can take advantage of innovations as a standard practice. Drive architectures using data : In the cloud, you can collect data on how your architectural choices affect the behavior of your workload. This lets you make fact-based decisions on how to improve your workload. Improve through game days to simulate events in production. This will help you understand where improvements can be made and can help develop organizational experience in dealing with events. Jumpstart an assessment \u00b6 In AWS console, search for well architected . With Lenses to define a set of question to ask. Operational Excellence \u00b6 Support development and run workloads effectively, gain insight into their operations, and to continuously improve supporting processes and procedures. Four area of focus: Organization : define clear responsabilities, roles, and success interdependencies. Prepare : design telemetry (logs, metrics...), improve flow, mitigate deployment risks, understand operational readiness. Operate : understand workload health, operation health, achievement of business outcome. Runbooks and playbooks should define escalation process, and define owneship for each action. Evolve : learn from experience, make improvements, share with teams. Design principles: Perform operations as code. Make frequent, small, reversible changes. Refine operations procedures frequently. Set up regular game days to review and validate that all procedures are effective. Anticipate failure: Perform \u201cpre-mortem\u201d exercises to identify potential sources of failure so that they can be removed or mitigated. Learn from all operational failures. Use established runbooks for well-understood events, and use playbooks to aid in investigation and resolution of issues. AWS CloudWatch is used to aggregate and present business, workload, and operations level views of operations activities. Assessment \u00b6 Questions to assess How do you determine what your priorities are? How do you structure your organization to support your business outcomes? How does your organizational culture support your business outcomes? How do you design your workload so that you can understand its state? How do you reduce defects, ease remediation, and improve flow into production? How do you mitigate deployment risks? How do you know that you are ready to support a workload? How do you understand the health of your workload? How do you understand the health of your operations? How do you manage workload and operations events? How do you evolve operations? Deeper dive \u00b6 100 and 200 labs Security \u00b6 Emcompass the ability to protect data, systems and assets by controlling access and get visibility on who does what. Design principles Apply security at all layers Automate security best practices Protect data in transit and at rest Questions to assess How do you manage identities for people and machines? How do you manage permissions for people and machines? How do you detect and investigate security events? How do you protect your network resources? How do you protect your compute resources? How do you classify your data? How do you protect your data at rest? How do you protect your data in transit? How do you anticipate, respond to, and recover from incidents? CloudTrail logs, AWS API calls, and CloudWatch provide monitoring of metrics with alarming, and AWS Config provides configuration history. Ensure that you have a way to quickly grant access for your security team, and automate the isolation of instances as well as the capturing of data and state for forensics. Reliability \u00b6 The ability of a workload to perform its intended function correctly and consistently . Reliability requires that your workload be aware of failures as they occur and take action to avoid impact on availability. Workloads must be able to both withstand failures and automatically repair issues. Design principles: Automatically recover from failure Test recovery procedures Scale horizontally to increase aggregate workload availability Stop guessing capacity: monitor demand and workload utilization, and automate the addition or removal of resources to maintain the optimal level to satisfy demand without over- or under-provisioning Manage change in automation Before architecting any system, define foundational requirements that influence reliability: Questions to assess Comments How do you manage service quotas and constraints? How do you plan your network topology? How do you design your workload service architecture? How do you design interactions in a distributed system to prevent failures? Search to improve mean time between failures (MTBF) How do you design interactions in a distributed system to mitigate or withstand failures? Look to improve mean time to recovery (MTTR) How do you monitor workload resources? Monitor Logs and metrics How do you design your workload to adapt to changes in demand? Add or remove resources automatically to adapt to the demand How do you implement change? Controlled changes to deploy new feature, patched or replaced in a predictable manner How do you back up data? Helps to address RTO and RPO How do you use fault isolation to protect your workload? Components outside of the boundary should not be affected by the failure. How do you design your workload to withstand component failures? How do you test reliability? testing is the only way to ensure that it will operate as designed How do you plan for disaster recovery (DR)? Regularly back up your data and test your backup files to ensure that you can recover from both logical and physical errors Use AZ, regions and bulkhead (elements of an application are isolated into pools so that if one fails, the others will continue to function) Lab from AWS Performance efficiency \u00b6 Use computing resources efficiently to meet system requirements, and to maintain that efficiency as demand changes and technologies evolve. Design principles: Democratize advanced technologies: delegate to your cloud vendor. Go global in minutes Use serverless architectures Experiment more often Consider mechanical sympathy: always use the technology approach that aligns best with your workload goal In AWS, compute is available in three forms: instances, containers, and functions. Storage is available in three forms: object, block, and file. Databases include relational, key-value, document, in-memory, graph, time series, and ledger databases. Questions to assess Comments How do you select the best performing architecture? Use a data-driven approach to select the patterns and implementation for your architecture and achieve a cost effective solution. How do you select your compute solution? Varies based on application design, usage patterns, and configuration settings How do you select your storage solution? The optimal storage solution for a system varies based on the kind of access method (block, file, or object), patterns of access (random or sequential), required throughput, frequency of access (online, offline, archival), frequency of update (WORM, dynamic), and availability and durability constraints How do you select your database solution? Consider requirements for availability, consistency, partition tolerance, latency, durability, scalability, and query capability How do you configure your networking solution? varies based on latency, throughput requirements, jitter, and bandwidth How do you evolve your workload to take advantage of new releases? How do you monitor your resources to ensure they are performing? How do you use tradeoffs to improve performance? improve performance by trading consistency, durability, and space for time and latency. Amazon CloudWatch is a monitoring and observability service that provides you with data and actionable insights to monitor your workload, respond to system-wide performance changes, optimize resource utilization, and get a unified view of operational health AWS cloudformation to define infrastructure as code. Cost optimization \u00b6 Run systems to deliver business value at the lowest price point possible. Design principles: Implement Cloud Financial Management practices / team Adopt a concumption model Measure overall efficiency: Measure the business output of the workload and the costs associated with delivering it Stop spending mone on undifferentiated heavy lifting Questions to assess Comments How do you govern usage? How do you monitor usage and cost? How do you decommission resources? How do you evaluate cost when you select services? Trade off between low level service like EC2, S3, EBS versus higher level like DynamoDB How do you meet cost targets when you select resource type, size and number? How do you plan for data transfer charges? As your requirements change, be aggressive in decommissioning resources, entire services, and systems that you no longer require. Sustainability \u00b6 Focuses on environmental impacts, especially energy consumption and efficiency. Scale infrastructure to continually match user load and ensure that only the minimum resources required to support users are deployed identify redundancy, underutilization, and potential decommission targets Implement patterns for performing load smoothing and maintaining consistent high utilization of deployed resources to minimize the resources consumed. Monitor workload activity to identify application components that consume the most resources. Understand how data is used within your workload, consumed by your users, transferred, and stored. Lifecycle data to more efficient, less performant storage when requirements decrease, and delete data that\u2019s no longer required. Adopt shared storage and single sources of truth to avoid data duplication and reduce the total storage requirements of your workload Back up data only when difficult to recreate Minimize the amount of hardware needed to provision and deploy Use automation and infrastructure as code to bring pre-production environments up when needed and take them down when not used. More readings \u00b6 system design exercices using AWS services Disaster Recovery AWS","title":"Well Architectured"},{"location":"sa/well-architectured/#aws-well-architectured","text":"The AWS Well-Architected Framework helps you understand the pros and cons of decisions you make while building systems on AWS. By using the Framework, customers will learn architectural best practices for designing and operating reliable, secure, efficient, and cost-effective systems in the cloud. Those are the questions to ask for designing a cloud native solution by understanding the potential impact . All hardware are becoming software. Workload represents interrelated applications, infrastructure, policies, governance and operations.","title":"AWS Well Architectured"},{"location":"sa/well-architectured/#six-pilards","text":"When architecting technology solutions, never neglect the six pillars of: Operational Excellence Security Reliability Performance Efficiency Cost Optimization Sustainability","title":"Six pilards"},{"location":"sa/well-architectured/#design-principles","text":"Stop guessing your capacity needs : use as much or as little capacity as you need, and scale up and down automatically. Test systems at production scale , then decommission the resources. Automate to make architectural experimentation easier . Allow for evolutionary architectures : the capability to automate and test on demand lowers the risk of impact from design changes. This allows systems to evolve over time so that businesses can take advantage of innovations as a standard practice. Drive architectures using data : In the cloud, you can collect data on how your architectural choices affect the behavior of your workload. This lets you make fact-based decisions on how to improve your workload. Improve through game days to simulate events in production. This will help you understand where improvements can be made and can help develop organizational experience in dealing with events.","title":"Design Principles"},{"location":"sa/well-architectured/#jumpstart-an-assessment","text":"In AWS console, search for well architected . With Lenses to define a set of question to ask.","title":"Jumpstart an assessment"},{"location":"sa/well-architectured/#operational-excellence","text":"Support development and run workloads effectively, gain insight into their operations, and to continuously improve supporting processes and procedures. Four area of focus: Organization : define clear responsabilities, roles, and success interdependencies. Prepare : design telemetry (logs, metrics...), improve flow, mitigate deployment risks, understand operational readiness. Operate : understand workload health, operation health, achievement of business outcome. Runbooks and playbooks should define escalation process, and define owneship for each action. Evolve : learn from experience, make improvements, share with teams. Design principles: Perform operations as code. Make frequent, small, reversible changes. Refine operations procedures frequently. Set up regular game days to review and validate that all procedures are effective. Anticipate failure: Perform \u201cpre-mortem\u201d exercises to identify potential sources of failure so that they can be removed or mitigated. Learn from all operational failures. Use established runbooks for well-understood events, and use playbooks to aid in investigation and resolution of issues. AWS CloudWatch is used to aggregate and present business, workload, and operations level views of operations activities.","title":"Operational Excellence"},{"location":"sa/well-architectured/#security","text":"Emcompass the ability to protect data, systems and assets by controlling access and get visibility on who does what. Design principles Apply security at all layers Automate security best practices Protect data in transit and at rest Questions to assess How do you manage identities for people and machines? How do you manage permissions for people and machines? How do you detect and investigate security events? How do you protect your network resources? How do you protect your compute resources? How do you classify your data? How do you protect your data at rest? How do you protect your data in transit? How do you anticipate, respond to, and recover from incidents? CloudTrail logs, AWS API calls, and CloudWatch provide monitoring of metrics with alarming, and AWS Config provides configuration history. Ensure that you have a way to quickly grant access for your security team, and automate the isolation of instances as well as the capturing of data and state for forensics.","title":"Security"},{"location":"sa/well-architectured/#reliability","text":"The ability of a workload to perform its intended function correctly and consistently . Reliability requires that your workload be aware of failures as they occur and take action to avoid impact on availability. Workloads must be able to both withstand failures and automatically repair issues. Design principles: Automatically recover from failure Test recovery procedures Scale horizontally to increase aggregate workload availability Stop guessing capacity: monitor demand and workload utilization, and automate the addition or removal of resources to maintain the optimal level to satisfy demand without over- or under-provisioning Manage change in automation Before architecting any system, define foundational requirements that influence reliability: Questions to assess Comments How do you manage service quotas and constraints? How do you plan your network topology? How do you design your workload service architecture? How do you design interactions in a distributed system to prevent failures? Search to improve mean time between failures (MTBF) How do you design interactions in a distributed system to mitigate or withstand failures? Look to improve mean time to recovery (MTTR) How do you monitor workload resources? Monitor Logs and metrics How do you design your workload to adapt to changes in demand? Add or remove resources automatically to adapt to the demand How do you implement change? Controlled changes to deploy new feature, patched or replaced in a predictable manner How do you back up data? Helps to address RTO and RPO How do you use fault isolation to protect your workload? Components outside of the boundary should not be affected by the failure. How do you design your workload to withstand component failures? How do you test reliability? testing is the only way to ensure that it will operate as designed How do you plan for disaster recovery (DR)? Regularly back up your data and test your backup files to ensure that you can recover from both logical and physical errors Use AZ, regions and bulkhead (elements of an application are isolated into pools so that if one fails, the others will continue to function) Lab from AWS","title":"Reliability"},{"location":"sa/well-architectured/#performance-efficiency","text":"Use computing resources efficiently to meet system requirements, and to maintain that efficiency as demand changes and technologies evolve. Design principles: Democratize advanced technologies: delegate to your cloud vendor. Go global in minutes Use serverless architectures Experiment more often Consider mechanical sympathy: always use the technology approach that aligns best with your workload goal In AWS, compute is available in three forms: instances, containers, and functions. Storage is available in three forms: object, block, and file. Databases include relational, key-value, document, in-memory, graph, time series, and ledger databases. Questions to assess Comments How do you select the best performing architecture? Use a data-driven approach to select the patterns and implementation for your architecture and achieve a cost effective solution. How do you select your compute solution? Varies based on application design, usage patterns, and configuration settings How do you select your storage solution? The optimal storage solution for a system varies based on the kind of access method (block, file, or object), patterns of access (random or sequential), required throughput, frequency of access (online, offline, archival), frequency of update (WORM, dynamic), and availability and durability constraints How do you select your database solution? Consider requirements for availability, consistency, partition tolerance, latency, durability, scalability, and query capability How do you configure your networking solution? varies based on latency, throughput requirements, jitter, and bandwidth How do you evolve your workload to take advantage of new releases? How do you monitor your resources to ensure they are performing? How do you use tradeoffs to improve performance? improve performance by trading consistency, durability, and space for time and latency. Amazon CloudWatch is a monitoring and observability service that provides you with data and actionable insights to monitor your workload, respond to system-wide performance changes, optimize resource utilization, and get a unified view of operational health AWS cloudformation to define infrastructure as code.","title":"Performance efficiency"},{"location":"sa/well-architectured/#cost-optimization","text":"Run systems to deliver business value at the lowest price point possible. Design principles: Implement Cloud Financial Management practices / team Adopt a concumption model Measure overall efficiency: Measure the business output of the workload and the costs associated with delivering it Stop spending mone on undifferentiated heavy lifting Questions to assess Comments How do you govern usage? How do you monitor usage and cost? How do you decommission resources? How do you evaluate cost when you select services? Trade off between low level service like EC2, S3, EBS versus higher level like DynamoDB How do you meet cost targets when you select resource type, size and number? How do you plan for data transfer charges? As your requirements change, be aggressive in decommissioning resources, entire services, and systems that you no longer require.","title":"Cost optimization"},{"location":"sa/well-architectured/#sustainability","text":"Focuses on environmental impacts, especially energy consumption and efficiency. Scale infrastructure to continually match user load and ensure that only the minimum resources required to support users are deployed identify redundancy, underutilization, and potential decommission targets Implement patterns for performing load smoothing and maintaining consistent high utilization of deployed resources to minimize the resources consumed. Monitor workload activity to identify application components that consume the most resources. Understand how data is used within your workload, consumed by your users, transferred, and stored. Lifecycle data to more efficient, less performant storage when requirements decrease, and delete data that\u2019s no longer required. Adopt shared storage and single sources of truth to avoid data duplication and reduce the total storage requirements of your workload Back up data only when difficult to recreate Minimize the amount of hardware needed to provision and deploy Use automation and infrastructure as code to bring pre-production environments up when needed and take them down when not used.","title":"Sustainability"},{"location":"sa/well-architectured/#more-readings","text":"system design exercices using AWS services Disaster Recovery AWS","title":"More readings"},{"location":"serverless/","text":"Serverless \u00b6 Serverless would optimize for getting to market quickly, would remove the most amount of undifferentiated heavy lifting and allow dev teams to move as quickly as possible. Container \u00b6 A container is a standardized unit that packages your code and its dependencies. This package is designed to run reliably on any platform, because the container creates its own independent environment The difference between containers and virtual machines (VMs) can be illustrated by the following figure: In AWS, containers run on EC2 instances. For example, you might have a large instance and run a few containers on that instance. While running one instance is easy to manage, it lacks high availability and scalability. Most companies and organizations run many containers on many EC2 instances across several Availability Zones Amazon Elastic Container Service (Amazon ECS) \u00b6 Amazon ECS is an end-to-end container orchestration service that helps you spin up new containers and manages them across a cluster of EC2 instances, without managing a control plane. It maintains application availability and allows you to scale your containers up or down to meet your application's capacity requirements. Integrated with familiar features like Elastic Load Balancing, EBS volumes, VPC, and IAM. Simple APIs let you integrate and use your own schedulers or connect Amazon ECS into your existing software delivery process. It is possible to run container into two modes: EC2 or Fargate. For EC2 we can create those instances upfront or use an auto scaling group and start instances. We need to install the Amazon ECS container agent on each EC2 instances, the docker engine, and manage the EC2 ourselves... With Fargate, as a serverless approach, only specifying container configuration, services... are needed. It is possible to have Fargate and EC2 auto scaling group inside your ECS cluster. ECS with EC2 launch type is charged based on EC2 instances and EBS volumes used. ECS with Fargate launch type is charged based on vCPU and memory resources that the containerized application requests. The container is run by defining Task and Service. Here is task definition example, which includes capacity and the docker image reference: Running the task, creates a Service (we can also define the service and deploy it): See demo for NGInx for detailed configuration Create a task definition: Specify the container images, environment variables and any resources configurations... Add a Application Load Balancer. The task definition is a text file, in JSON format, that describes one or more containers. First we create cluster: it is a regional grouping of container instances. Cluster may have one to many EC2 instances. Try to use Fargate as runtime engine. With all the resources created automatically: It creates a VPC with two public subnets. Task definitions can be defined outside of a cluster, but services are associating task to cluster, subnets, security groups... Service Auto Scaling \u00b6 ECS service auto scaling helps to automatically increase/decrease ECS task number. It uses AWS Application Auto Scaling which specifies the service usage in term of CPU, memory and request count per target. There are 3 ways to define scaling rules: Target Tracking, based on value for a specific CloudWatch metric. Step Scaling, based on CloudWatch Alarm. Scheduled Scaling, based on a specified date/time. Others \u00b6 IAM Roles are defined for each container. Other role can be added to access ECS, ECR, S3, CloudWatch, ... EC2 Instance Profile is the IAM Role used by the ECS Agent on the EC2 instance to execute ECS-specific actions such as pulling Docker images from ECR and storing the container logs into CloudWatch Logs ECS Task Role is the IAM Role used by the ECS task itself. Use when your container wants to call other AWS services like S3, SQS, etc. Event Brige can define a rule to run a ECS task. See also ecs anywhere Fargate \u00b6 When running ECS and EKS on EC2, we are still responsible for maintaining the underlying EC2 instances. With Fargates we do no have to manage EC2 instances. AWS Fargate is a purpose-built serverless compute engine for containers. Fargate scales and manages the infrastructure, it removes the need to provision and manage servers, let you specify and pay for resources per application, and improves security through application isolation by design. It natively integrates with AWS Identity and Access Management (IAM) and Amazon Virtual Private Cloud (VPC). It uses the following constructs: Task Definition : to define application containers, image URL, CPU and memory needed... It is immutable, so any update creates new version. It can define 1 to 10 container definitions. The container definition part includes memory reservation and cpu allocation. Task : A running instance of a Task Definition. A task can have multiple containers running in parallel. Each task has its own Elastic Network Interface with a private IP @ from the subnet. Cluster : infrastructure isolation boundary. Tasks run in the cluster. Service : Endpoint to support ELB integration and do mapping to tasks Pricing is based on what we provision, then the task level CPU and memory and the per-second billing with one minute minimum. To share data between containers, Fargate provides 4 GB volumes space per task, and the volume is mounted in the container. By default, Fargate tasks are given a minimum of 20 GiB of free ephemeral storage. For IAM security, the same policies can be set as we do for EC2. We still need to add an execution role to define access to ECR to download images and CloudWatch for monitoring. Step function \u00b6 AWS Step Functions is a fully managed service that you can use to coordinate the components of distributed applications and microservices using visual workflows. You build small applications that each perform a discrete function (or step) in your workflow, which means that you can scale and change your applications quickly. AWS Elastic Beanstalk \u00b6 With Elastic Beanstalk, developers upload their application. Then, Elastic Beanstalk automatically handles the deployment details of capacity provisioning, load balancing, auto-scaling, and application health monitoring. AWS App runner \u00b6 AWS App Runner, a fully managed container application service that makes it easy for customers without any prior containers or infrastructure experience to build, deploy, and run containerized web applications and APIs Amazon Lightsail \u00b6 Lightsail is a VPS provider and is a useful way to get started with AWS for users who need a solution to build and host their applications on AWS Cloud. Lightsail provides developers with compute, storage, and networking capacity and capabilities to deploy and manage websites and web applications in the cloud. Lightsail includes VMs, containers, databases, content delivery network (CDN), load balancers, Domain Name System (DNS) management, Read more \u00b6 10 Things Serverless Architects Should Know","title":"Introduction"},{"location":"serverless/#serverless","text":"Serverless would optimize for getting to market quickly, would remove the most amount of undifferentiated heavy lifting and allow dev teams to move as quickly as possible.","title":"Serverless"},{"location":"serverless/#container","text":"A container is a standardized unit that packages your code and its dependencies. This package is designed to run reliably on any platform, because the container creates its own independent environment The difference between containers and virtual machines (VMs) can be illustrated by the following figure: In AWS, containers run on EC2 instances. For example, you might have a large instance and run a few containers on that instance. While running one instance is easy to manage, it lacks high availability and scalability. Most companies and organizations run many containers on many EC2 instances across several Availability Zones","title":"Container"},{"location":"serverless/#amazon-elastic-container-service-amazon-ecs","text":"Amazon ECS is an end-to-end container orchestration service that helps you spin up new containers and manages them across a cluster of EC2 instances, without managing a control plane. It maintains application availability and allows you to scale your containers up or down to meet your application's capacity requirements. Integrated with familiar features like Elastic Load Balancing, EBS volumes, VPC, and IAM. Simple APIs let you integrate and use your own schedulers or connect Amazon ECS into your existing software delivery process. It is possible to run container into two modes: EC2 or Fargate. For EC2 we can create those instances upfront or use an auto scaling group and start instances. We need to install the Amazon ECS container agent on each EC2 instances, the docker engine, and manage the EC2 ourselves... With Fargate, as a serverless approach, only specifying container configuration, services... are needed. It is possible to have Fargate and EC2 auto scaling group inside your ECS cluster. ECS with EC2 launch type is charged based on EC2 instances and EBS volumes used. ECS with Fargate launch type is charged based on vCPU and memory resources that the containerized application requests. The container is run by defining Task and Service. Here is task definition example, which includes capacity and the docker image reference: Running the task, creates a Service (we can also define the service and deploy it): See demo for NGInx for detailed configuration Create a task definition: Specify the container images, environment variables and any resources configurations... Add a Application Load Balancer. The task definition is a text file, in JSON format, that describes one or more containers. First we create cluster: it is a regional grouping of container instances. Cluster may have one to many EC2 instances. Try to use Fargate as runtime engine. With all the resources created automatically: It creates a VPC with two public subnets. Task definitions can be defined outside of a cluster, but services are associating task to cluster, subnets, security groups...","title":"Amazon Elastic Container Service (Amazon ECS)"},{"location":"serverless/#service-auto-scaling","text":"ECS service auto scaling helps to automatically increase/decrease ECS task number. It uses AWS Application Auto Scaling which specifies the service usage in term of CPU, memory and request count per target. There are 3 ways to define scaling rules: Target Tracking, based on value for a specific CloudWatch metric. Step Scaling, based on CloudWatch Alarm. Scheduled Scaling, based on a specified date/time.","title":"Service Auto Scaling"},{"location":"serverless/#others","text":"IAM Roles are defined for each container. Other role can be added to access ECS, ECR, S3, CloudWatch, ... EC2 Instance Profile is the IAM Role used by the ECS Agent on the EC2 instance to execute ECS-specific actions such as pulling Docker images from ECR and storing the container logs into CloudWatch Logs ECS Task Role is the IAM Role used by the ECS task itself. Use when your container wants to call other AWS services like S3, SQS, etc. Event Brige can define a rule to run a ECS task. See also ecs anywhere","title":"Others"},{"location":"serverless/#fargate","text":"When running ECS and EKS on EC2, we are still responsible for maintaining the underlying EC2 instances. With Fargates we do no have to manage EC2 instances. AWS Fargate is a purpose-built serverless compute engine for containers. Fargate scales and manages the infrastructure, it removes the need to provision and manage servers, let you specify and pay for resources per application, and improves security through application isolation by design. It natively integrates with AWS Identity and Access Management (IAM) and Amazon Virtual Private Cloud (VPC). It uses the following constructs: Task Definition : to define application containers, image URL, CPU and memory needed... It is immutable, so any update creates new version. It can define 1 to 10 container definitions. The container definition part includes memory reservation and cpu allocation. Task : A running instance of a Task Definition. A task can have multiple containers running in parallel. Each task has its own Elastic Network Interface with a private IP @ from the subnet. Cluster : infrastructure isolation boundary. Tasks run in the cluster. Service : Endpoint to support ELB integration and do mapping to tasks Pricing is based on what we provision, then the task level CPU and memory and the per-second billing with one minute minimum. To share data between containers, Fargate provides 4 GB volumes space per task, and the volume is mounted in the container. By default, Fargate tasks are given a minimum of 20 GiB of free ephemeral storage. For IAM security, the same policies can be set as we do for EC2. We still need to add an execution role to define access to ECR to download images and CloudWatch for monitoring.","title":"Fargate"},{"location":"serverless/#step-function","text":"AWS Step Functions is a fully managed service that you can use to coordinate the components of distributed applications and microservices using visual workflows. You build small applications that each perform a discrete function (or step) in your workflow, which means that you can scale and change your applications quickly.","title":"Step function"},{"location":"serverless/#aws-elastic-beanstalk","text":"With Elastic Beanstalk, developers upload their application. Then, Elastic Beanstalk automatically handles the deployment details of capacity provisioning, load balancing, auto-scaling, and application health monitoring.","title":"AWS Elastic Beanstalk"},{"location":"serverless/#aws-app-runner","text":"AWS App Runner, a fully managed container application service that makes it easy for customers without any prior containers or infrastructure experience to build, deploy, and run containerized web applications and APIs","title":"AWS App runner"},{"location":"serverless/#amazon-lightsail","text":"Lightsail is a VPS provider and is a useful way to get started with AWS for users who need a solution to build and host their applications on AWS Cloud. Lightsail provides developers with compute, storage, and networking capacity and capabilities to deploy and manage websites and web applications in the cloud. Lightsail includes VMs, containers, databases, content delivery network (CDN), load balancers, Domain Name System (DNS) management,","title":"Amazon Lightsail"},{"location":"serverless/#read-more","text":"10 Things Serverless Architects Should Know","title":"Read more"},{"location":"serverless/apigtw/","text":"API Gateway \u00b6 Introduction \u00b6 Fully managed service to define, deploy, monitor APIs: HTTP, REST, WebSocket. It forms the app-facing part of AWS Serverless. It supports hundred of thousands of concurrent API calls. Integrates with Cognito user pools, IAM policies, lambda authorizer... CloudTrail logging and monitoring of API usage and API changes. CloudWatch access logging and execution logging,... Hands-on \u00b6 API Gateway console . Basic API to front end Lambda . Pricing \u00b6 HTTP APIs are designed with minimal features so that they can be offered at a lower price. WebSocket APIs maintain persistent connections with clients for full-duplex communication Deeper dive \u00b6 Using AWS Lambda with Amazon API Gateway .","title":"API Gateway"},{"location":"serverless/apigtw/#api-gateway","text":"","title":"API Gateway"},{"location":"serverless/apigtw/#introduction","text":"Fully managed service to define, deploy, monitor APIs: HTTP, REST, WebSocket. It forms the app-facing part of AWS Serverless. It supports hundred of thousands of concurrent API calls. Integrates with Cognito user pools, IAM policies, lambda authorizer... CloudTrail logging and monitoring of API usage and API changes. CloudWatch access logging and execution logging,...","title":"Introduction"},{"location":"serverless/apigtw/#hands-on","text":"API Gateway console . Basic API to front end Lambda .","title":"Hands-on"},{"location":"serverless/apigtw/#pricing","text":"HTTP APIs are designed with minimal features so that they can be offered at a lower price. WebSocket APIs maintain persistent connections with clients for full-duplex communication","title":"Pricing"},{"location":"serverless/apigtw/#deeper-dive","text":"Using AWS Lambda with Amazon API Gateway .","title":"Deeper dive"},{"location":"serverless/eks/","text":"Elastic Kubernetes Service \u00b6 Amazon EKS is a fully managed service to run Kubernetes. It is integrated with VPC for isolation, IAM for authentication, ELB for load distribution, and ECR for container image registry. Major characteristics \u00b6 Scale K8s control plane across multiple AZs. No need to install, operate and maintain k8s cluster. Automatically scales control plane instances based on load, detects and replaces unhealthy control plane instance. It supports EC2 to deploy worker nodes or Fargate to deploy serverless containers or on to AWS Outposts . Fully compatible with other CNSF kubernetes. Can be deployed on-premises wwith Amazon EKS Distro (EKS-D) distribution. The EKS node types are: Managed node groups : EC2 (could be On-demand or spot instances) created by you but managed by AWS, assigned to a ASG managed by EKS. Self-managed nodes : nodes are managed by you and attached to EKS cluster by using an ASG. AWS Fargate which represents a cost optimized deployment for EKS worker nodes. Each time a pod is created it is assigned to a EC2 instance. It works with ALB. Data volumes (EBS, EFS, FSx) are defined with StorageClass and they need to have Container Storage Interface compliant driver. See Pricing calculator : pay for cluster control plane, EC3 instance or Fargate or AWS outposts. Cluster management \u00b6 EKS runs a single tenant Kubernetes control plane for each cluster. 3 etcd instance in 3 AZs within one region. EKS uses IAM to provide authentication to our Kubernetes cluster, and k8s RBAC for authorization. ECS comparisons \u00b6 An EC2 instance with the ECS agent installed and configured is called a container instance. In Amazon EKS, it is called a worker node. An ECS container is called a task. In Amazon EKS, it is called a pod. While Amazon ECS runs on AWS native technology, Amazon EKS runs Kubernetes. What to do the first time \u00b6 Install kubernetes tools Download eksctl ( eksctl.io ). (It also installs kubectl ) brew tap weaveworks/tap brew install weaveworks/tap/eksctl # Verify it eksctl version Be sure to have a EC2 key-pair, if not, create one with the following command: aws ec2 create-key-pair --region us-west-2 --key-name myKeyPair Create IAM Role with EKS Cluster role and attach the required Amazon EKS IAM managed policy to it. Kubernetes clusters managed by Amazon EKS make calls to other AWS services on your behalf to manage the resources that you use with the service. # under the labs/eks folder aws iam create-role \\ --role-name myAmazonEKSClusterRole \\ --assume-role-policy-document file:// \"eks-cluster-role-trust-policy.json\" # Attach the required Amazon EKS managed IAM policy to the role. aws iam attach-role-policy \\ --policy-arn arn:aws:iam::aws:policy/AmazonEKSClusterPolicy \\ --role-name myAmazonEKSClusterRole Working with cluster \u00b6 Create cluster to be deployed on EC2, in a VPC, subnets, and security groups. It can be done with different ways: Using CloudFormation and a predifined stack: aws cloudformation create-stack \\ --region us-west-2 \\ --stack-name my-eks-vpc-stack \\ --template-url https://s3.us-west-2.amazonaws.com/amazon-eks/cloudformation/2020-10-29/amazon-eks-vpc-private-subnets.yaml Using eksctl create cluster will use CloudFormation to deploy a EKS cluster with managed nodes: eksctl create cluster \\ --name my-cluster \\ --region us-west-2 \\ --with-oidc \\ --ssh-access \\ --ssh-public-key my-ke-ypair \\ --instance-types = m5.xlarge \\ --managed Using CDK see examples in the product doc , see also a CDK example in labs/cdk/eks-single folder or the labs/eks/eks-cdk folder for a python based deployment. Find cluster credentials were added in ~/.kube/config As an alternate, create Fargate profile to declare which pods run on Fargate. See instructions EKS fargate getting started . Fargate profiles are associated to namespaces. Only private subnets are supported for pods that are running on Fargate. Pods that match a selector are scheduled on Fargate. Kubernetes affinity/anti-affinity rules do not apply and aren't necessary with Amazon EKS Fargate pod. Verify nodes and pods kubectl get nodes -o wide # across namespaces kubectl get pods --all-namespaces -o wide Add resources like node group, with IAM role of WorkerNode IAM Users and Roles are bound to an EKS Kubernetes cluster via a ConfigMap named aws-auth . Deploy the Delete cluster \u00b6 List all services kubectl get svc --all-namespaces Delete any services that have an associated EXTERNAL-IP value. These services are fronted by an Elastic Load Balancing load balancer, and you must delete them in Kubernetes to allow the load balancer and associated resources to be properly released. kubectl delete svc <service-name> Delete the cluster estctl delete cluster --name <cluster name> EKS Blueprint \u00b6 The EKS Blueprints is an open-source development framework that abstracts the complexities of cloud infrastructure from developers. Concepts \u00b6 A blueprint combines clusters, add-ons, and teams into a cohesive object that can be deployed as a whole. Team is a logical grouping of IAM identities that has access to a Kubernetes namespace(s), or cluster administrative access depending upon the team type. Once a blueprint is configured, it can be easily deployed across any number of AWS accounts and regions. Blueprints also leverage GitOps tooling to facilitate cluster bootstrapping and workload onboarding. EKS with CDK Hands-on \u00b6 This is a summary of the steps to get a running demonstration of creating EKS and Day 2 add-on. Single Cluster \u00b6 Using CDK typescript here are the commands: mkdir my-eks-blueprints cd my-eks-blueprints cdk init app --language typescript npm i typescript@~4.8.4 npm i @aws-quickstart/eks-blueprints See the code in labs/cdk/eks-single Create a Cluster using the eks-blueprints package, which is published as a npm module. import * as cdk from 'aws-cdk-lib' ; import { Construct } from 'constructs' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; export default class ClusterConstruct extends Construct { constructor ( scope : Construct , id : string , props ?: cdk . StackProps ) { super ( scope , id ); const account = props ? . env ? . account ! ; const region = props ? . env ? . region ! ; const blueprint = blueprints . EksBlueprint . builder () . account ( account ) . region ( region ) . addOns () . teams () . build ( scope , id + '-stack' ); } } And in the app #!/usr/bin/env node import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import ClusterConstruct from '../lib/my-eks-blueprints-stack' ; const app = new cdk . App (); const account = process . env . CDK_DEFAULT_ACCOUNT ! ; const region = process . env . CDK_DEFAULT_REGION ; const env = { account , region } new ClusterConstruct ( app , 'cluster' , { env }); Deploy the cluster: cdk deploy cluster-stack , then config kubectl export KUBE_CONFIG = $( aws cloudformation describe-stacks --stack-name cluster-stack | jq -r '.Stacks[0].Outputs[] | select(.OutputKey|match(\"ConfigCommand\"))| .OutputValue' ) $KUBE_CONFIG kubectl get svc EKS Blueprints Patterns On board teams \u00b6 We want two teams: platform and application teams. mkdir teams && cd teams && mkdir platform-team && mkdir application-team aws iam create-user --user-name platform aws iam create-user --user-name application Under platform-team create a init.ts , Add a IAM Principal to add users to the platform using their IAM credentials import { ArnPrincipal } from \"aws-cdk-lib/aws-iam\"; import { PlatformTeam } from '@aws-quickstart/eks-blueprints'; export class TeamPlatform extends PlatformTeam { constructor(accountID: string) { super({ name: \"platform\", users: [new ArnPrincipal(`arn:aws:iam::${accountID}:user/platform`)] }) } } And do the same for application team. Then modify the cluster definition to add team instances: The cdk deploy cluster-stack will create a new namespace for the team application. import { TeamPlatform , TeamApplication } from '../teams' ; ... . teams ( new TeamPlatform ( account ), new TeamApplication ( 'burnham' , account )) A command like kubectl describe role -n team-burnham gives information on the role and actions that member can do. Using Kubernetes constructs such as namespaces, quotas, and network policieswe can prevent applications deployed in different namespaces from communicating with each other. To see the application user access limitation, login to the console in incognito mode, use the account ID, application as user and be sure to have setup a password in IAM for the application user. Once logged assume the role of cluster-stack-teamburnhamAccessRole3.... Then go to the EKS console. We should see an error message that the Team Burnham user is NOT allowed to list deployments in all the namespaces. But selecting the team-burnham namespace we should see pods and other elements. The user platform with the role cluster-stack-teamplatformAccessRole5... can access the cluster as admin and see all namespaces. Adding add-ons \u00b6 See the list of supported add-ons . To add them use the addOn() function in the blueprint: const blueprint = blueprints.EksBlueprint.builder() .account(account) .region(region) .addOns(new blueprints.ClusterAutoScalerAddOn) .teams(new TeamPlatform(account), new TeamApplication('burnham',account)) .build(scope, id+'-stack'); } Demonstrate EKS \u00b6 Based on the EKS workshop , using Cloud9, quick summary of steps Create workspace in Cloud9 Install kubernetes tools . The list below is for the new versions: Update the CDK to be version 2.5 rm $( which cdk ) npm install -g aws-cdk@2.50.0 cdk --version Install kubectl and AWS CLI sudo curl --silent --location -o /usr/local/bin/kubectl https://s3.us-west-2.amazonaws.com/amazon-eks/1.23.7/2022-06-29/bin/linux/amd64/kubectl sudo chmod +x /usr/local/bin/kubectl curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" unzip awscliv2.zip sudo ./aws/install Install jq, gettext... sudo yum -y install jq gettext bash-completion moreutils # Verify the path for command in kubectl jq envsubst aws do which $command & >/dev/null && echo \" $command in path\" || echo \" $command NOT FOUND\" done # Enable kubectl bash_completion kubectl completion bash >> ~/.bash_completion . /etc/profile.d/bash_completion.sh . ~/.bash_completion Create IAM role named eks-blueprints-cdk-workshop-admin with AdministratorAccess, and modify the Cloud9, EC2 instance IAM role in Actions > Security > Modify IAM Role. Update Cloud9 workspace to disable Cloud9 to manage IAM credentials dynamically (This is not compatible with the EKS IAM authentication). Gear > AWS Settings > . Save region and account as env variable and configure aws CLI: echo \"export ACCOUNT_ID= ${ ACCOUNT_ID } \" | tee -a ~/.bash_profile echo \"export AWS_REGION= ${ AWS_REGION } \" | tee -a ~/.bash_profile aws configure set default.region ${ AWS_REGION } aws configure get default.region # validate that the Cloud9 IDE is using the correct IAM role aws sts get-caller-identity --query Arn | grep eks-blueprints-cdk-workshop-admin -q && echo \"IAM role valid\" || echo \"IAM role NOT valid\" If not done before, bootstrap CDK (the following command is to bootstrap CDK in 3 regions) cdk bootstrap --trust = $ACCOUNT_ID \\ --cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\ aws:// $ACCOUNT_ID / $AWS_REGION aws:// $ACCOUNT_ID /us-east-2 aws:// $ACCOUNT_ID /us-east-1 Create Encryption Key in KMS aws kms create-alias --alias-name alias/myKey --target-key-id $( aws kms create-key --query KeyMetadata.Arn --output text ) aws kms describe-key --key-id alias/myKey Declare a cluster using a config file like (labs/eks/eks-cluster.yam) and use eksctl create cluster -f eks-cluster.yaml Update the kubeconfig file to interact with you cluster: aws eks update-kubeconfig --name eksworkshop-eksctl --region ${AWS_REGION} To use GitOps approach create a CodeCommit Repository. If using GitHub, needs to set Personal Access Token aws codecommit create-repository --repository-name my-eks-blueprints-pipeline Deeper Dive \u00b6 Product documentation - Elastic Kubernetes Service EKS workshops EKS Blueprints for CDK Workshop Getting started with Amazon EKS \u2013 eksctl EKS Best Practices Guides EKS Blueprint Amazon EKS Blueprints for Terraform EKS SaaS workshop","title":"EKS"},{"location":"serverless/eks/#elastic-kubernetes-service","text":"Amazon EKS is a fully managed service to run Kubernetes. It is integrated with VPC for isolation, IAM for authentication, ELB for load distribution, and ECR for container image registry.","title":"Elastic Kubernetes Service"},{"location":"serverless/eks/#major-characteristics","text":"Scale K8s control plane across multiple AZs. No need to install, operate and maintain k8s cluster. Automatically scales control plane instances based on load, detects and replaces unhealthy control plane instance. It supports EC2 to deploy worker nodes or Fargate to deploy serverless containers or on to AWS Outposts . Fully compatible with other CNSF kubernetes. Can be deployed on-premises wwith Amazon EKS Distro (EKS-D) distribution. The EKS node types are: Managed node groups : EC2 (could be On-demand or spot instances) created by you but managed by AWS, assigned to a ASG managed by EKS. Self-managed nodes : nodes are managed by you and attached to EKS cluster by using an ASG. AWS Fargate which represents a cost optimized deployment for EKS worker nodes. Each time a pod is created it is assigned to a EC2 instance. It works with ALB. Data volumes (EBS, EFS, FSx) are defined with StorageClass and they need to have Container Storage Interface compliant driver. See Pricing calculator : pay for cluster control plane, EC3 instance or Fargate or AWS outposts.","title":"Major characteristics"},{"location":"serverless/eks/#cluster-management","text":"EKS runs a single tenant Kubernetes control plane for each cluster. 3 etcd instance in 3 AZs within one region. EKS uses IAM to provide authentication to our Kubernetes cluster, and k8s RBAC for authorization.","title":"Cluster management"},{"location":"serverless/eks/#ecs-comparisons","text":"An EC2 instance with the ECS agent installed and configured is called a container instance. In Amazon EKS, it is called a worker node. An ECS container is called a task. In Amazon EKS, it is called a pod. While Amazon ECS runs on AWS native technology, Amazon EKS runs Kubernetes.","title":"ECS comparisons"},{"location":"serverless/eks/#what-to-do-the-first-time","text":"Install kubernetes tools Download eksctl ( eksctl.io ). (It also installs kubectl ) brew tap weaveworks/tap brew install weaveworks/tap/eksctl # Verify it eksctl version Be sure to have a EC2 key-pair, if not, create one with the following command: aws ec2 create-key-pair --region us-west-2 --key-name myKeyPair Create IAM Role with EKS Cluster role and attach the required Amazon EKS IAM managed policy to it. Kubernetes clusters managed by Amazon EKS make calls to other AWS services on your behalf to manage the resources that you use with the service. # under the labs/eks folder aws iam create-role \\ --role-name myAmazonEKSClusterRole \\ --assume-role-policy-document file:// \"eks-cluster-role-trust-policy.json\" # Attach the required Amazon EKS managed IAM policy to the role. aws iam attach-role-policy \\ --policy-arn arn:aws:iam::aws:policy/AmazonEKSClusterPolicy \\ --role-name myAmazonEKSClusterRole","title":"What to do the first time"},{"location":"serverless/eks/#working-with-cluster","text":"Create cluster to be deployed on EC2, in a VPC, subnets, and security groups. It can be done with different ways: Using CloudFormation and a predifined stack: aws cloudformation create-stack \\ --region us-west-2 \\ --stack-name my-eks-vpc-stack \\ --template-url https://s3.us-west-2.amazonaws.com/amazon-eks/cloudformation/2020-10-29/amazon-eks-vpc-private-subnets.yaml Using eksctl create cluster will use CloudFormation to deploy a EKS cluster with managed nodes: eksctl create cluster \\ --name my-cluster \\ --region us-west-2 \\ --with-oidc \\ --ssh-access \\ --ssh-public-key my-ke-ypair \\ --instance-types = m5.xlarge \\ --managed Using CDK see examples in the product doc , see also a CDK example in labs/cdk/eks-single folder or the labs/eks/eks-cdk folder for a python based deployment. Find cluster credentials were added in ~/.kube/config As an alternate, create Fargate profile to declare which pods run on Fargate. See instructions EKS fargate getting started . Fargate profiles are associated to namespaces. Only private subnets are supported for pods that are running on Fargate. Pods that match a selector are scheduled on Fargate. Kubernetes affinity/anti-affinity rules do not apply and aren't necessary with Amazon EKS Fargate pod. Verify nodes and pods kubectl get nodes -o wide # across namespaces kubectl get pods --all-namespaces -o wide Add resources like node group, with IAM role of WorkerNode IAM Users and Roles are bound to an EKS Kubernetes cluster via a ConfigMap named aws-auth . Deploy the","title":"Working with cluster"},{"location":"serverless/eks/#delete-cluster","text":"List all services kubectl get svc --all-namespaces Delete any services that have an associated EXTERNAL-IP value. These services are fronted by an Elastic Load Balancing load balancer, and you must delete them in Kubernetes to allow the load balancer and associated resources to be properly released. kubectl delete svc <service-name> Delete the cluster estctl delete cluster --name <cluster name>","title":"Delete cluster"},{"location":"serverless/eks/#eks-blueprint","text":"The EKS Blueprints is an open-source development framework that abstracts the complexities of cloud infrastructure from developers.","title":"EKS Blueprint"},{"location":"serverless/eks/#concepts","text":"A blueprint combines clusters, add-ons, and teams into a cohesive object that can be deployed as a whole. Team is a logical grouping of IAM identities that has access to a Kubernetes namespace(s), or cluster administrative access depending upon the team type. Once a blueprint is configured, it can be easily deployed across any number of AWS accounts and regions. Blueprints also leverage GitOps tooling to facilitate cluster bootstrapping and workload onboarding.","title":"Concepts"},{"location":"serverless/eks/#eks-with-cdk-hands-on","text":"This is a summary of the steps to get a running demonstration of creating EKS and Day 2 add-on.","title":"EKS with CDK Hands-on"},{"location":"serverless/eks/#demonstrate-eks","text":"Based on the EKS workshop , using Cloud9, quick summary of steps Create workspace in Cloud9 Install kubernetes tools . The list below is for the new versions: Update the CDK to be version 2.5 rm $( which cdk ) npm install -g aws-cdk@2.50.0 cdk --version Install kubectl and AWS CLI sudo curl --silent --location -o /usr/local/bin/kubectl https://s3.us-west-2.amazonaws.com/amazon-eks/1.23.7/2022-06-29/bin/linux/amd64/kubectl sudo chmod +x /usr/local/bin/kubectl curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" unzip awscliv2.zip sudo ./aws/install Install jq, gettext... sudo yum -y install jq gettext bash-completion moreutils # Verify the path for command in kubectl jq envsubst aws do which $command & >/dev/null && echo \" $command in path\" || echo \" $command NOT FOUND\" done # Enable kubectl bash_completion kubectl completion bash >> ~/.bash_completion . /etc/profile.d/bash_completion.sh . ~/.bash_completion Create IAM role named eks-blueprints-cdk-workshop-admin with AdministratorAccess, and modify the Cloud9, EC2 instance IAM role in Actions > Security > Modify IAM Role. Update Cloud9 workspace to disable Cloud9 to manage IAM credentials dynamically (This is not compatible with the EKS IAM authentication). Gear > AWS Settings > . Save region and account as env variable and configure aws CLI: echo \"export ACCOUNT_ID= ${ ACCOUNT_ID } \" | tee -a ~/.bash_profile echo \"export AWS_REGION= ${ AWS_REGION } \" | tee -a ~/.bash_profile aws configure set default.region ${ AWS_REGION } aws configure get default.region # validate that the Cloud9 IDE is using the correct IAM role aws sts get-caller-identity --query Arn | grep eks-blueprints-cdk-workshop-admin -q && echo \"IAM role valid\" || echo \"IAM role NOT valid\" If not done before, bootstrap CDK (the following command is to bootstrap CDK in 3 regions) cdk bootstrap --trust = $ACCOUNT_ID \\ --cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\ aws:// $ACCOUNT_ID / $AWS_REGION aws:// $ACCOUNT_ID /us-east-2 aws:// $ACCOUNT_ID /us-east-1 Create Encryption Key in KMS aws kms create-alias --alias-name alias/myKey --target-key-id $( aws kms create-key --query KeyMetadata.Arn --output text ) aws kms describe-key --key-id alias/myKey Declare a cluster using a config file like (labs/eks/eks-cluster.yam) and use eksctl create cluster -f eks-cluster.yaml Update the kubeconfig file to interact with you cluster: aws eks update-kubeconfig --name eksworkshop-eksctl --region ${AWS_REGION} To use GitOps approach create a CodeCommit Repository. If using GitHub, needs to set Personal Access Token aws codecommit create-repository --repository-name my-eks-blueprints-pipeline","title":"Demonstrate EKS"},{"location":"serverless/eks/#deeper-dive","text":"Product documentation - Elastic Kubernetes Service EKS workshops EKS Blueprints for CDK Workshop Getting started with Amazon EKS \u2013 eksctl EKS Best Practices Guides EKS Blueprint Amazon EKS Blueprints for Terraform EKS SaaS workshop","title":"Deeper Dive"},{"location":"serverless/eventbridge/","text":"EventBridge \u00b6 Amazon EventBridge is a serverless event bus service (Formerly CloudWatch Event), which can send message to Lambda functions, SQS, SNS based on rules. The new marketing positioning is: EventBridge is a service for building scalable event-driven applications, enabling highly agile software development via fully managed and secure integrations, cutting costs and reducing time to production. By default there is an EventBridge event hub already created. We can ingest, filter, transform and deliver events without writing custom code. The following figure illustrates the typical source of events and how EventBridge can process them and send JSON to different sinks. Filtering logic may be applied. We can also integrate to Partners SaaS services like Datadog, Zendeck. We can define our own custom app to integrate with the Event bus. We can also archive events to be able to replay them. When creating rules (See when EC2 is stopped) we can use a sandbox feature to test the event type we want to work on and define and test the rule. then specify the target, for example a SNS topic. It can infer the data schema from the event as source, and use a SchemaRegistry. The SchemaRegitry will help generate code for our applications. From this schema definition, in OpenAPI 2.0 format, we can get code sample to get rhe definition of the events and the marshalizer. In term of solution design, we can define a central event based to aggregate all the events from AWS Organizations in a single AWS account or region. Apps in different accounts can be authorized to send event to this central hub via resource-based policy. Tutorial Amazon EventBridge CDK Construct Library Event","title":"EventBridge"},{"location":"serverless/eventbridge/#eventbridge","text":"Amazon EventBridge is a serverless event bus service (Formerly CloudWatch Event), which can send message to Lambda functions, SQS, SNS based on rules. The new marketing positioning is: EventBridge is a service for building scalable event-driven applications, enabling highly agile software development via fully managed and secure integrations, cutting costs and reducing time to production. By default there is an EventBridge event hub already created. We can ingest, filter, transform and deliver events without writing custom code. The following figure illustrates the typical source of events and how EventBridge can process them and send JSON to different sinks. Filtering logic may be applied. We can also integrate to Partners SaaS services like Datadog, Zendeck. We can define our own custom app to integrate with the Event bus. We can also archive events to be able to replay them. When creating rules (See when EC2 is stopped) we can use a sandbox feature to test the event type we want to work on and define and test the rule. then specify the target, for example a SNS topic. It can infer the data schema from the event as source, and use a SchemaRegistry. The SchemaRegitry will help generate code for our applications. From this schema definition, in OpenAPI 2.0 format, we can get code sample to get rhe definition of the events and the marshalizer. In term of solution design, we can define a central event based to aggregate all the events from AWS Organizations in a single AWS account or region. Apps in different accounts can be authorized to send event to this central hub via resource-based policy. Tutorial Amazon EventBridge CDK Construct Library Event","title":"EventBridge"},{"location":"serverless/lambda/","text":"Lambda \u00b6 Introduction \u00b6 With AWS Lambda, we can run code without provisioning or managing servers or containers. Upload the source code, and Lambda takes care of everything required to run and scale the code with high availability. Getting started tutorial with free tier A Lambda function has three primary components \u2013 trigger, code, and configuration. Triggers describe when a Lambda function should run. A trigger integrates the Lambda function with other AWS services, enabling us to run our Lambda function in response to certain API calls that occur in our AWS account. Configuration includes compute resources, execution timeout, IAM roles (lambda_basic_execution)... Code: Java, Node.js, C#, Go, or Python Pay only for what we use: # of requests and CPU time. Lambda functions always operate from an AWS-owned VPC. By default, the function has the full ability to make network requests to any public internet address \u2014 this includes access to any of the public AWS APIs. Only enable our functions to run inside the context of a private subnet in our VPC, when we need to interact with a private resource located in a private subnet. AWS Lambda automatically monitors Lambda functions and reports metrics through Amazon CloudWatch. To help us monitoring the code as it executes, Lambda automatically tracks the number of requests, the latency per request, and the number of requests resulting in an error and publishes the associated metrics. We can leverage these metrics to set custom alarms. To reuse code in more than one function, consider creating a Layer and deploying it there. A layer is a ZIP archive that contains libraries, a custom runtime, or other dependencies. Lambda support versioning and we can maintain one or more versions of our lambda function. Each lambda function has a unique ARN. Lambda also supports Alias for each of the functions. Lambda alias is a pointer to a specific lambda function version. Alias enables you to promote new lambda function versions to production and if you need to rollback a function, you can simply update the alias to point to the desired version. Event source needs to use Alias ARN for invoking the lambda function. Criteria to use lambda. \u00b6 Per region deployment. Must run under 15 min. Memory from 128MB to 10GB. 100 concurrent calls. Code in compressed zip should be under 50MB and 250MB uncompressed. Disk capacity for /tmp is limited to 10GB. Security control \u00b6 AWS Lambda supports resource-based permissions policies for Lambda functions and layers, for example to allow granting usage permission to other AWS accounts on per-resource basis. Resource-based policies let you grant usage permission to other AWS accounts on a per-resource basis. Hands-on \u00b6 Can start from a blueprint To get the function to upload logs to cloudWatch, select an existing role, or create a new one Here the example of role created Add the code as an implementation of an handler function: Create a test event (a request) and run the test and get the resources and logs output. Verify configuration and monitoring. Python function with dependencies \u00b6 It is common to have a function that needs libraries not in the standard python 3.x environment. So the approach is to use a zip file as source of the function with all the dependencies inside it. The process to build such zip can be summarized as: lambda is the folder with code and future dependencies. It has a requirements.txt file to define dependencies. do a pip install --target ./package -r requirements.txt zip the content of the package directory in a zip in the lambda folder cd package zip -r ../lambda-layer.zip . The zip can be used as a layer, so reusable between different lambda function. For that upload the zip to a s3 bucket and create a layer in the Lambda console, referencing the zip in s3 bucket. A layer can be added to any lambda function, then the libraries included in the layer can be imported in the code. Example is the XRay tracing capability in Python. We can also add the lambda-function code in the zip and modify the existing function, something like: zip lambda-layer.zip lambda-handler.py aws lambda update-function-code --function-name ApigwLambdaCdkStack-SageMakerMapperLambda2EFF1AC9-bERmXFWzvWSC --zip-file fileb://lambda-layer.zip Other personal implementations \u00b6 S3 to Lambda to S3 for data transformation Big data SaaS: lambda to call SageMaker Edge Function \u00b6 When we need to customize the CDN content, we can use Edge Function to run close to the end users. CloudFront provides two types: CloudFront functions or Lambda@Edge. Can be used for: Website security and privacy. Dynamic web application at the Edge. Search engine optimization (SEO). Intelligently route across origins and data centers. Bot mitigation at the Edge. Real-time image transformation. User authentication and authorization. Layers \u00b6 Layer helps to define reusable elements to run a function. More reading \u00b6 Using an Amazon S3 trigger to invoke a Lambda function . Tutorial: Resize Images on the Fly with Amazon S3, AWS Lambda, and Amazon API Gateway . Best Practices for Developing on AWS Lambda .","title":"Lambda"},{"location":"serverless/lambda/#lambda","text":"","title":"Lambda"},{"location":"serverless/lambda/#introduction","text":"With AWS Lambda, we can run code without provisioning or managing servers or containers. Upload the source code, and Lambda takes care of everything required to run and scale the code with high availability. Getting started tutorial with free tier A Lambda function has three primary components \u2013 trigger, code, and configuration. Triggers describe when a Lambda function should run. A trigger integrates the Lambda function with other AWS services, enabling us to run our Lambda function in response to certain API calls that occur in our AWS account. Configuration includes compute resources, execution timeout, IAM roles (lambda_basic_execution)... Code: Java, Node.js, C#, Go, or Python Pay only for what we use: # of requests and CPU time. Lambda functions always operate from an AWS-owned VPC. By default, the function has the full ability to make network requests to any public internet address \u2014 this includes access to any of the public AWS APIs. Only enable our functions to run inside the context of a private subnet in our VPC, when we need to interact with a private resource located in a private subnet. AWS Lambda automatically monitors Lambda functions and reports metrics through Amazon CloudWatch. To help us monitoring the code as it executes, Lambda automatically tracks the number of requests, the latency per request, and the number of requests resulting in an error and publishes the associated metrics. We can leverage these metrics to set custom alarms. To reuse code in more than one function, consider creating a Layer and deploying it there. A layer is a ZIP archive that contains libraries, a custom runtime, or other dependencies. Lambda support versioning and we can maintain one or more versions of our lambda function. Each lambda function has a unique ARN. Lambda also supports Alias for each of the functions. Lambda alias is a pointer to a specific lambda function version. Alias enables you to promote new lambda function versions to production and if you need to rollback a function, you can simply update the alias to point to the desired version. Event source needs to use Alias ARN for invoking the lambda function.","title":"Introduction"},{"location":"serverless/lambda/#criteria-to-use-lambda","text":"Per region deployment. Must run under 15 min. Memory from 128MB to 10GB. 100 concurrent calls. Code in compressed zip should be under 50MB and 250MB uncompressed. Disk capacity for /tmp is limited to 10GB.","title":"Criteria to use lambda."},{"location":"serverless/lambda/#security-control","text":"AWS Lambda supports resource-based permissions policies for Lambda functions and layers, for example to allow granting usage permission to other AWS accounts on per-resource basis. Resource-based policies let you grant usage permission to other AWS accounts on a per-resource basis.","title":"Security control"},{"location":"serverless/lambda/#hands-on","text":"Can start from a blueprint To get the function to upload logs to cloudWatch, select an existing role, or create a new one Here the example of role created Add the code as an implementation of an handler function: Create a test event (a request) and run the test and get the resources and logs output. Verify configuration and monitoring.","title":"Hands-on"},{"location":"serverless/lambda/#python-function-with-dependencies","text":"It is common to have a function that needs libraries not in the standard python 3.x environment. So the approach is to use a zip file as source of the function with all the dependencies inside it. The process to build such zip can be summarized as: lambda is the folder with code and future dependencies. It has a requirements.txt file to define dependencies. do a pip install --target ./package -r requirements.txt zip the content of the package directory in a zip in the lambda folder cd package zip -r ../lambda-layer.zip . The zip can be used as a layer, so reusable between different lambda function. For that upload the zip to a s3 bucket and create a layer in the Lambda console, referencing the zip in s3 bucket. A layer can be added to any lambda function, then the libraries included in the layer can be imported in the code. Example is the XRay tracing capability in Python. We can also add the lambda-function code in the zip and modify the existing function, something like: zip lambda-layer.zip lambda-handler.py aws lambda update-function-code --function-name ApigwLambdaCdkStack-SageMakerMapperLambda2EFF1AC9-bERmXFWzvWSC --zip-file fileb://lambda-layer.zip","title":"Python function with dependencies"},{"location":"serverless/lambda/#other-personal-implementations","text":"S3 to Lambda to S3 for data transformation Big data SaaS: lambda to call SageMaker","title":"Other personal implementations"},{"location":"serverless/lambda/#edge-function","text":"When we need to customize the CDN content, we can use Edge Function to run close to the end users. CloudFront provides two types: CloudFront functions or Lambda@Edge. Can be used for: Website security and privacy. Dynamic web application at the Edge. Search engine optimization (SEO). Intelligently route across origins and data centers. Bot mitigation at the Edge. Real-time image transformation. User authentication and authorization.","title":"Edge Function"},{"location":"serverless/lambda/#layers","text":"Layer helps to define reusable elements to run a function.","title":"Layers"},{"location":"serverless/lambda/#more-reading","text":"Using an Amazon S3 trigger to invoke a Lambda function . Tutorial: Resize Images on the Fly with Amazon S3, AWS Lambda, and Amazon API Gateway . Best Practices for Developing on AWS Lambda .","title":"More reading"},{"location":"templ/","text":"Templates to drive some architecture and design discussions \u00b6 Whiteboard best practices \u00b6 Complement the discussion with diagrams, flows, and more engaging with participants. Discovery and collaborate on a one of a kind solution that the participants own. Take care of time management (segmentation of the agenda and what needs to be addressed )and space management (name section of the whiteboard, use 25% (agenda), 50% (main), 25% (takeaways) sections). BE PREPARED. Example of sections: topics, takeaways, next steps (action for me and then action for customer) Brings your own supplies: markers, eraser.. Use Uppercase, limit color Apply the technique of Touch, Turn, Talk Be sure to ask a participant if she/he is confortable to draw and go to the whiteboard. Do not assume you can keep the content of the whiteboard with a photo. Ask permission. For diagramming: Focusing on core elements Labeling elements by function, not service name Asking before erasing (or using alternative methods like strikethrough or an X) Planning iconography in advance Discover architecture requirements \u00b6 This is a simple agenda, parking lots zone, use cases and system context map in one drawio template: Be sure to address: Do you understand the business? Do you know how they make money? Do you understand the problem? Who will be using this/who is the customer? What are the expectations from the customer? What does success look like? Does the customer have an established timeline or deadline? Use Well-Architected Pillars to approach a problem: Cost: What\u2019s their budget? How do they make money? Reliability: What happens if this app fails? Is there an SLA, internal or external? Operations: What does the team look like that is building this? Who will support it once it\u2019s built? Do they need help? Is there a partner they work with? Performance: Are there performance requirements? How fast does it need to be? Do they already have a similar app? Have they had any problems with it? Security: Are there security or compliance requirements? Any PII data? Do you have any upcoming audits? How do those tend to do? Do they need help?","title":"Conduct Design session"},{"location":"templ/#templates-to-drive-some-architecture-and-design-discussions","text":"","title":"Templates to drive some architecture and design discussions"},{"location":"templ/#whiteboard-best-practices","text":"Complement the discussion with diagrams, flows, and more engaging with participants. Discovery and collaborate on a one of a kind solution that the participants own. Take care of time management (segmentation of the agenda and what needs to be addressed )and space management (name section of the whiteboard, use 25% (agenda), 50% (main), 25% (takeaways) sections). BE PREPARED. Example of sections: topics, takeaways, next steps (action for me and then action for customer) Brings your own supplies: markers, eraser.. Use Uppercase, limit color Apply the technique of Touch, Turn, Talk Be sure to ask a participant if she/he is confortable to draw and go to the whiteboard. Do not assume you can keep the content of the whiteboard with a photo. Ask permission. For diagramming: Focusing on core elements Labeling elements by function, not service name Asking before erasing (or using alternative methods like strikethrough or an X) Planning iconography in advance","title":"Whiteboard best practices"},{"location":"templ/#discover-architecture-requirements","text":"This is a simple agenda, parking lots zone, use cases and system context map in one drawio template: Be sure to address: Do you understand the business? Do you know how they make money? Do you understand the problem? Who will be using this/who is the customer? What are the expectations from the customer? What does success look like? Does the customer have an established timeline or deadline? Use Well-Architected Pillars to approach a problem: Cost: What\u2019s their budget? How do they make money? Reliability: What happens if this app fails? Is there an SLA, internal or external? Operations: What does the team look like that is building this? Who will support it once it\u2019s built? Do they need help? Is there a partner they work with? Performance: Are there performance requirements? How fast does it need to be? Do they already have a similar app? Have they had any problems with it? Security: Are there security or compliance requirements? Any PII data? Do you have any upcoming audits? How do those tend to do? Do they need help?","title":"Discover architecture requirements"}]}