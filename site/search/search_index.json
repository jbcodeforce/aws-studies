{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Amazon Web Service studies \u00b6 Created in 2002, and launched as AWS in 2004 with SQS as first service offering. 2003 amazon.com was $5.2B retail businesses. 7800 employees Why cloud \u00b6 moving from capex to variable expense economy of scale: EC2 instance with different pricing model. Usage from hundreds of thousands of customers is aggregated in the cloud Elactic capacity: pay for what you use. Scale up and down so no need to guessed capacity. Speed and agility to define infrastructure in minutes not weeks Focus on business apps, not IT infrastructure and data centers. Global reach in a minutes Use cases \u00b6 Enable to build scalable apps, adaptable to business demand Extend Enterprise IT Support flexible big data analytics Infrastructure \u00b6 AWS is a global infrastructure with 27 regions and 2 to 6 availability zones per region. Ex: us-west-1-2a. AZ is one or more DC with redundant power, networking and connectivity. Isolated from disasters. Interconnected with low latency network. AWS services are local or global: EC2 is a regional service. Region-scoped services come with availabiltiy and resiliency. IAM is a global service. AWS Local Zone location is an extension of an AWS Region where you can run your latency sensitive application in geography close to end-users. AWS Wavelength enables developers to build applications that deliver single-digit millisecond latencies to mobile devices and end-users. AWS infrastructure deployments that embed AWS compute and storage services within the telecommunications providers\u2019 datacenters at the edge of the 5G networks, and seamlessly access the breadth of AWS services in the region. Choose an AWS region, depending of your requirements like: compliance with data governance and legal requirements close to users to reduce latency availability of service within a region pricing Interact with AWS \u00b6 Management console: services are placed in categories: compute, serverless, database, analytics... CLI SDK for C++, Go, Java, JavaScript, .NET, Node.js, PHP, Python, and Ruby Organization \u00b6 When you work with the AWS Cloud, managing security and compliance is a shared responsibility between AWS and you: aws is the security of the cloud you are responsible for the security in the cloud: secure workloads and applications that you deploy onto the cloud. Helps to group accounts, and simplify account creation. Consolidate billing. Concepts: root user: a single sign-in identity that has complete access to all AWS services and resources in the account Organization unit (OU) account be part of 1 OU Define service control policies Organization console IAM Identity and Access Management \u00b6 This is global services so defined at the account level and cross regions Define user (physical person), group and roles, and permissions Do not use root user, but create user and always use them when login. jerome and mathieu are users get user as administrator, meaning it will be part of an admin group with admin priviledges, like AdmintratorAccess Assign users to groups ( admin and developers ) and assign policies to groups and not to individual user. Groups can only contain users, not other groups Users can belong to multiple groups Users are defined as global service encompasses all regions AWS Account has a unique ID but can be set with an alias (e.g. jbcodeforce ) so to sign in to the console the URL becomes https://jbcodeforce.signin.aws.amazon.com/console Policies are written in JSON, to define permissions Allow , Deny for users to access AWS services, groups and roles Policy applies to Principal: account/user/role, list the actions (what is allowed or denied) on the given resources Least privilege permission: Give users the minimal amount of permissions they need to do their job Policy is a json doc: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"iam:List*\" ], \"Resource\" : \"*\" } ] } Policy can define the password type > Account settings > Password policy , and when users are allowed to change the password. Multi Factor Authentication - always protect root account. MFA = password + device we own. The device could be a universal 2 nd factor security key. (ubikey) Authy is a multi-device service. We can have multiple users on the same device IAM is not used for website authentication and authorization For identity federation, SAML standard is used IAM Roles \u00b6 To get AWS services doing work on other service, we use IAM Role. Roles are assigned per application, or EC2 or lambda function... Maintaining roles is more efficient than maintaining users. When you assume a role, IAM dynamically provides temporary credentials that expire after a defined period of time, between 15 minutes and 36 hours. When connected to an EC2 machine via ssh or using EC2 instance connect, we need to set the IAM roles for who can use the EC2. A command like aws iam list-users will not work until a role it attached. To authorize access to a EC2 instance, we use IAM Roles. The DemoEC2Role, for example, is defined to access EC2 in read only: This role is then defined in the EC2 / Security > attach IAM role. Security tools \u00b6 In IAM, use > Credentials report to download account based report. In IAM, use > Users > select one user (jerome) and then Access Advisor : Access Advisor shows the services that this user can access and when those services were last accessed AWS CLI \u00b6 We can access AWS using the CLI or the SDK which both use access keys generated from the console (> Users > jerome > Security credentials > Access Keys). The keys are saved in ~/.aws/credentials aws-cli version 2 aws --version # get your users aws iam list-users With Cloudshell we can use aws cli and then have file that will be kept in the filesystems of the cloud shell. [aws-shell] is also available to facilitate the user experience. EBS Volume \u00b6 Elastic Block Store Volume is a network drive attached to the instance. It is locked to an AZ, and uses provisioned capacity in GBs and IOPS. Create a EBS while creating the EC2 instance and keep it not deleted on shutdown Once logged, add a filesystem, mount to a folder and modify boot so the volume is mounted at start time. Which looks like: # List existing block storage, verify our created storage is present lsblk # Verify file system type sudo file -s /dev/xdvf # Create a ext4 file system on the device sudo mkfs -t ext4 /dev/xvdb # make a mount point sudo mkdir /data sudo mount /dev/xvdb /data # Add entry in /etc/fstab with line like: /dev/xvdb /data ext4 default,nofail 0 2 EBS is already a redundant storage, replicated within an AZ. EC2 instance has a logical volume that can be attached to two or more EBS RAID 0 volumes, where write operations are distributed among them. It is used to increate IOPS without any fault tolerance. If one fails, we lost data. It could be used for database with built-in replication or Kafka. RAID 1 is for better fault tolerance: a write operation is going to all attached volumes. Volume types \u00b6 GP2 : used for most workload up to 16 TB at 16000 IOPS max (3 IOPS per GB brustable to 3000) io 1 : critical app with large database workloads. max ratio 50:1 IOPS/GB. Min 100 iops and 4G to 16T st 1 : Streaming workloads requiring consistent, fast throughput at a low price. For Big data, Data warehouses, Log processing, Apache Kafka sc 1 : throughput oriented storage. 500G- 16T, 500MiB/s. Max IOPs at 250. Used for cold HDD, and infrequently accessed data. Encryption has a minimum impact on latency. It encrypts data at rest and during snapshots. Instance store is a volume attached to the instance, used for root folder. It is a ephemeral storage but has millions read per s and 700k write IOPS. It provides the best disk performance and can be used to have high performance cache for our applications. If we need to run a high-performance database that requires an IOPS of 210,000 for its underlying filesystem, we need instance store and DB replication in place. Snapshots \u00b6 Used to backup disk and stored on S3. Snapshot Lifecycle policies helps to create snapshot with scheduling it by defining policies. To move a volume to another AZ or data center we can create a volume from a snapshot. Elastic File System \u00b6 Managed Network FS for multi AZs. (3x gp2 cost), controlled by using security group. This security group needs to add in bound rule of type NFS connected / linked to the SG of the EC2. Only Linux based AMI. Encryption is supported using KMS. 1000 concurrent clients 10GB+/s throughput, bursting or provisioned. Support different performance mode, like max I/O or general purpose Support storage tiers to move files after n days, infrequent EFS-IA for files rarely accessed. Use amazon EFS util tool in each EC2 instance to mount the EFS to a target mount point. Relational Database Service - RDS \u00b6 Managed service for SQL based database (mySQL, Postgresql, SQL server, Oracle). Must provision EC2 instance and EBS volume. Support multi AZs for DR with automatic failover to standby, app uses one unique DNS name. Continuous backup and restore to specific point of time restore. It uses gp2 or io1 EBS. Transaction logs are backed-up every 5 minutes. Support user triggered snapshot. Read replicas: helps to scale the read operations. Can create up to 5 replicas within AZ, cross AZ and cross region. Replication is asynch. Use cases include, reporting, analytics, ML model AWS charge for network when for example data goes from one AZ to another. Support at rest Encryption. Master needs to be encrypted to get encrypted replicas. We can create a snapshot from unencrypted DB and then copy it by enabling the encryption for this snapshot. From there we can create an Encrypted DB Our responsibility: Check the ports / IP / security group inbound rules in DB\u2019s SG In-database user creation and permissions or manage through IAM Creating a database with or without public access Ensure parameter groups or DB is configured to only allow SSL connections From a solution architecture point of view: Operations : small downtime when failover happens. For maintenance, scaling in read replicas, updating underlying ec2 instance, or restore EBS, there will be manual intervention. Security : AWS responsible for OS security, we are responsible for setting up KMS, security groups, IAM policies, authorizing users in DB, enforcing SSL. Reliability : Multi AZ feature helps to address it, with failover mechanism in case of failures Performance : depends on EC2 instance type, EBS volume type, ability to add Read Replicas. Doesn\u2019t auto-scale, adapt to workload manually. Aurora \u00b6 Proprietary SQL database, work using postgresql and mysql driver. It is cloud optimized and claims 5x performance improvement over mySQL on RDS, and 3x for postgresql. Can grow up to 64 TB. Sub 10ms replica lag, up to 15 replicas. Failover in Aurora is instantaneous. It\u2019s HA (High Availability) native. Use 1 master - 5 readers to create 6 copies over 3 AZs. 3 copies of 6 need for reads. Peer to peer replication. Use 100s volumes. Autoscaling on the read operation. It is CQRS at DB level, and read can be global. Use writer end point and reader endpoint. It also supports one write with multiple reader and parallel query, multiple writes and serverless to automate scaling down to zero (No capacity planning needed and pay per second). With Aurora global database one primary region is used for write and then up to 5 read only regions with replica lag up to 1 s. Promoting another region (for disaster recovery) has an RTO of < 1 minute Operations : less operation, auto scaling storage. Security : AWS responsible for OS security, we are responsible for setting up KMS, security groups, IAM policies, authorizing users in DB, enforcing SSL. Reliability : Multi AZ, HA Performance : 5x performance, up to 15 read replicas. ElastiCache \u00b6 Get a managed Redis or Memcached cluster. Applications queries ElastiCache, if not available, get from RDS and store in ElastiCache. Key-Value store. It can be used for user session store so user interaction can go to different application instances. Redis is a multi AZ with Auto-Failover, supports read replicas to scale and for high availability. It can persist data using AOF persistence, and has backup and restore features. Memcached is a multi-node for partitioning of data (sharding), and no persistence, no backup and restore. It is based on a multi-threaded architecture. Some patterns for ElastiCache: Lazy Loading : all the read data is cached, data can become stale in cache Write Through : Adds or update data in the cache when written to a DB (no stale data) Session Store : store temporary session data in a cache (using TTL features) Sub millisecond performance, in memory read replicas for sharding. DynamoDB \u00b6 AWS proprietary NoSQL database, Serverless, provisioned capacity, auto scaling, on demand capacity. Highly Available, Multi AZ by default, Read and Writes are decoupled, and DAX can be used for read cache. The read operations can be eventually consistent or strongly consistent. DynamoDB Streams to integrate with AWS Lambda. Route 53 \u00b6 It is a managed Domain Name System. DNS is a collection of rules and records which helps clients understand how to reach a server through URLs. Here is a quick figure to summary the process DNS records Time to Live (TTL), is set to get the web browser to keep the DNS resolution in cache. High TTL is around 24 hours, low TTL at 60s will make more DNS calls. TTL should be set to strike a balance between how long the value should be cached vs how much pressure should go on the DNS. Need to define the TTL for the app depending on the expected deployment model. A hosted zone is a container that holds information about how we want to route traffic for a domain. Two types are supported: public or private within a VPC. Route 53 is a registrar. We can buy domain name. Use dig <hostname> to get the DNS resolution record. CNAME vs Alias \u00b6 CNAME is a DNS record to maps one domain name to another. CNAME should point to a ALB. Alias is used to point a hostname of an AWS resource and can work on root domain (domainname.com). Routing \u00b6 A simple routing policy to get an IP @ from a hostname could not have health check defined. The weighted routing policy controls the % of the requests that go to specific endpoint. Can do blue-green traffic management. It can also help to split traffic between two regions. It can be associated with Health Checks The latency routing Policy redirects to the server that has the least latency close to the client. Latency is evaluated in terms of user to designated AWS Region. Health check monitors the health and performance of the app servers or endpoints and assess DNS failure. We can have HTTP, TCP or HTTPS health checks. We can define from which region to run the health check. They are charged per HC / month. It is recommended to have one HC per app deployment. It can also monitor latency. The failover routing policy helps us to specify a record set to point to a primary and then a secondary instance for DR. The Geo Location routing policy is based on user's location, and we may specify how the traffic from a given country should go to this specific IP. Need to define a \u201cdefault\u201d policy in case there\u2019s no match on location. The Multi Value routing policy is used to access multiple resources. The record set, associates a Route 53 health checks with records. The client on DNS request gets up to 8 healthy records returned for each Multi Value query. If one fails then the client can try one other IIP @ from the list. Some application patterns \u00b6 For solution architecture, we need to assess cost, performance, reliability, security and operational excellence. Stateless App \u00b6 The step to grow a stateless app: add vertical scaling by changing the EC2 profile, but while changing, user has out of service. Second step is to scale horizontal, each EC2 instance has static IP address and DNS is configured with 'A record' to get each EC2 end point. But if one instance is gone, the client App will see it down until TTL expires. The reference architecture includes DNS record set with alias record (to point to ALB. Using alias as ALB address may change over time) with TTL of 1 hour. Use load balancers in 3 AZs (to survive disaster) to hide the horizontal scaling of EC2 instances (managed with auto scaling group) where the app runs. Health checks are added to keep system auto adaptable and hide system down, and restricted security group rules to control EC2 instance accesses. ALB and EC instances are in multi different AZs. The EC instances can be set up with reserved capacity to control cost. Stateful app \u00b6 In this case we will add the pattern of shopping cart. If we apply the same architecture as before, at each interaction of the user, it is possible the traffic will be sent to another EC2 instance that started to process the shopping cart. Using ELB with stickiness will help to keep the traffic to the same EC2, but in case of EC2 failure we still loose the cart. An alternate is to use user cookies to keep the cart at each interaction. It is back to a stateless app as state is managed by client and cookie. For security reason the app needs to validate the cookie content. cookie has a limit of 4K data. Another solution is to keep session data into an elastic cache, like Redis, and use the sessionId as key and persisted in a user cookie. So EC2 managing the interaction can get the cart data from the cache using the sessionID. It can be enhanced with a RDS to keep user data. Which can also support the CQRS pattern with read replicas. Cache can be update with data from RDS so if the user is requesting data in session, it hits the cache. Cache and database are set on multi AZ, as well as EC2 instance and load balancer, all to support disaster. Security groups need to be defined to get all traffic to the ELB and limited traffic between ELB and EC2 and between EC2 and cache and EC2 and DB. Another example of stateful app is the ones using image stored on disk. In this case EC2 EBS volume will work only for one app instance, but for multi app scaling out, we need to have a Elastic FS which can be Multi AZ too. Deploying app \u00b6 The easiest solution is to create AMI containing OS, dependencies and app binary. This is completed with User Data to get dynamic configuration. Database data can be restored from Snapshot, and the same for EFS data. Elastic Beanstalk is a developer centric view of the app, hiding the complexity of the IaaS. From one git repository it can automatically handle the deployment details of capacity provisioning, load balancing, auto-scaling, and application health monitoring. S3 \u00b6 Amazon S3 allows people to store objects (files) in buckets (directories), which must have a globally unique name (cross users!). They are defined at the region level. Object in a bucket, is referenced as a key which can be seen as a file path in a file system. The max size for an object is 5 TB but big file needs to be uploaded in multipart using 5GB max size. S3 supports versioning at the bucket level. So file can be restored from previous version, and even deleted file can be retrieved from a previous version. Use cases \u00b6 Backup and restore DR Archive Data lakes Hybrid cloud storage: seamless connection between on-premises applications and S3 with AWS Storage Gateway. Cloud-native application data GETTIG started Security control \u00b6 Objects can also be encrypted, and different mechanisms are available: SSE-S3 : server-side encrypted S3 objects using keys handled & managed by AWS using AES-256 protocol must set x-amz-server-side-encryption: \"AES256\" header in the POST request. SSE-KMS : leverage AWS Key Management Service to manage encryption keys. x-amz-server-side-encryption: \"aws:kms\" header. Server side encrypted. It gives user control of the key rotation policy and audit trail. SSE-C : when we want to manage our own encryption keys. Server-side encrypted. Encryption key must be provided in HTTP headers, for every HTTP request made. HTTPS is mandatory Client Side Encryption : encrypt before sending object. Explicit DENY in an IAM policy will take precedence over a bucket policy permission. S3 Website \u00b6 We can have static web site on S3. Once html pages are uploaded, setting the properties as static web site from the bucket. The bucket needs to be public, and have a security policy to allow any user to GetObject action. The URL may look like: <bucket-name>.s3-website.<AWS-region>.amazonaws.com Cross Origin resource sharing CORS : The web browser requests won\u2019t be fulfilled unless the other origin allows for the requests, using CORS Headers Access-Control-Allow-Origin . If a client does a cross-origin request on our S3 bucket, we need to enable the correct CORS headers: this is done by adding a security policy with CORS configuration like: <CORSConfiguration> <CORSRule> <AllowedOrigin> enter-bucket-url-here </AllowedOrigin> <AllowedMethod> GET </AllowedMethod> <MaxAgeSeconds> 3000 </MaxAgeSeconds> <AllowedHeader> Authorization </AllowedHeader> </CORSRule> </CORSConfiguration> Finally S3 is eventually consistent. S3 replication \u00b6 Once versioning enabled, a bucket can be replicated in the same region or cross regions. S3 replication is done on at least 3 AZs. Each AZ can be up to 8 data centers. One DC down does not impact S3 availability. The replication is done asynchronously. SRR is for log aggregation for example, while CRR is used for compliance and DR or replication across accounts. Delete operations are not replicated. S3 Storage classes \u00b6 When uploading a document into an existing bucket we can specify the storage class for keep data over time. Different levels are offered with different cost and SLA. To prevent accidental file deletes, we can setup MFA Delete to use MFA tokens before deleting objects. Amazon Glacier is for archiving, like writing to tapes. We can transition objects between storage classes. For infrequently accessed object, move them to STANDARD_IA. For archive objects, that we don\u2019t need in real-time, use GLACIER or DEEP_ARCHIVE. Moving objects can be automated using a lifecycle configuration At the bucket level, a user can define lifecycle rules for when to transition an object to another storage class. To improve performance, a big file can be split and then uploaded with local connection to the closed edge access and then use AWS private network to copy between buckets in different region. S3 to Kafka lab AWS Athena \u00b6 AWS Athena runs analytics directly on S3 files, using SQL language to query the files (CSV, JSON, Avro, Parquet...). S3 Access Logs log all the requests made to buckets, and Athena can then be used to run serverless analytics on top of the logs files. Queries are done on high availability capability so will succeed, and scale based on the data size. No need for complex ETL jobs to prepare your data for analytics. Integrated with AWS Glue Data Catalog , allowing you to create a unified metadata repository across various services, crawl data sources to discover schemas and populate your Catalog with new and modified table and partition definitions, and maintain schema versioning. AWS CLI \u00b6 The cli needs to be configured: aws configure with the credential, key and region to access. Use IAM user to get a new credentials key. When using CLI in a EC2 instance always use an IAM role to control security credentials. This role can come with a policy authorizing exactly what the EC2 instances should be able to do. Also within a EC2 instance, it is possible to use the URL http://169.254.169.254/latest/meta-data to get information about the EC2. We can retrieve the IAM Role name from that metadata. CloudFront \u00b6 CDN service with DDoS protection. It caches data to the edge to improve web browsing and app performance. 216 Edge locations. The origins of those files are S3 buckets, Custom resource accessible via HTTP. CloudFront keeps cache for the data read. For the edge to access the S3 bucket, it uses an origin access identity (OAI), managed as IAM role. For EC2 instance, the security group needs to accept traffic from edge location IP addresses. It is possible to control with geo restriction. It also supports the concept of signed URL. When you want to distribute content to different user groups over the world, attach a policy with: URL expiration IP ranges to access the data from Trusted signers (which AWS accounts can create signed URLs) How long should the URL be valid for? Shared content (movie, music): make it short (a few minutes) Private content (private to the user): you can make it last for years Signed URL = access to individual files (one signed URL per file) Signed Cookies = access to multiple files (one signed cookie for many files) Storage \u00b6 Snowball \u00b6 Move TB of data in and out AWS using physical device to ship data. The edge has 100TB and compute power to do some local processing on data. Snow mobile is a truck with 100 PB capacity. Once on site, it is transferred to S3. Snowball Edge brings computing capabilities to allow data pre-processing while it's being moved in Snowball, so we save time on the pre-processing side as well. Hybrid cloud storage \u00b6 Storage gateway expose an API in front of S3. Three gateway types: file : S3 bucket accessible using NFS or SMB protocols. Controlled access via IAM roles. File gateway is installed on-premise and communicate with AWS. volume : this is a block storage using iSCSI protocol. On-premise and visible as a local volume backed by S3. tape : same approach but with virtual tape library. Can go to S3 and Glacier. Storage comparison \u00b6 S3: Object Storage Glacier: Object Archival EFS: Network File System for Linux instances, POSIX filesystem FSx for Windows: Network File System for Windows servers FSx for Lustre: High Performance Computing Linux file system EBS volumes: Network storage for one EC2 instance at a time Instance Storage: Physical storage for your EC2 instance (high IOPS) Storage Gateway: File Gateway, Volume Gateway (cache & stored), Tape Gateway Snowball / Snowmobile: to move large amount of data to the cloud, physically Database: for specific workloads, usually with indexing and querying Integration and middleware: SQS, Kinesis Active MQ \u00b6 SQS: Standard queue \u00b6 Oldest queueing service on AWS. The default retention is 4 days up to 14 days. low latency < 10ms. Duplicate messages is possible and out of order too. Consumer deletes the message. It is auto scaling. Specific SDK to integrate to SendMessage... Consumers receive, process and then delete. Parallelism is possible on the different messages. The consumers can be in an auto scaling group so with CloudWatch, it is possible to monitor the queue size / # of instances and on the CloudWatch alarm action, trigger scaling. Max mesage size is 256KB. Message has metadata out of the box. After a message is polled by a consumer, it becomes invisible to other consumers. By default, the \u201cmessage visibility timeout\u201d is 30 seconds, which means the message has 30 seconds to be processed (Amazon SQS prevents other consumers from receiving and processing the message). After the message visibility timeout is over, the message is \u201cvisible\u201d in SQS, so it may be processed twice. But a consumer could call the ChangeMessageVisibility API to get more time. When the visibility timeout is high (hours), and the consumer crashes then the re-processing of all the message will take time. If it is set too low (seconds), we may get duplicates Encryption in fight via HTTPS, at rest encryption with KMS keys. Comes with monitoring. If a consumer fails to process a message within the Visibility Timeout, the message goes back to the queue. But if we can set a threshold of how many times a message can go back to the queue. After the MaximumReceives threshold is exceeded, the message goes into a dead letter queue (DLQ) (which has a limit of 14 days to process). Delay queues let you postpone the delivery of new messages to a queue for a number of seconds. If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period. The default (minimum) delay for a queue is 0 seconds. The maximum is 15 minutes Queue can be set as FIFO to guaranty the order: limited to throughput at 300 msg/s without batching or 3000 msg/s with batching. I can also support exactly once. It can be set to remove duplicate by looking at the content. Simple Notification Service is for topic pub/sub \u00b6 SNS supports up to 10,000,000 subscriptions per topic, 100,000 topics limit. The subscribers can publish to topic via SDK and can use different protocols like: HTTP / HTTPS (with delivery retries \u2013 how many times), SMTP, SMS, ... The subscribers can be a SQS, a Lambda, Emails... Many AWS services can send data directly to SNS for notifications: CloudWatch (for alarms), Auto Scaling Groups notifications, Amazon S3 (on bucket events), CloudFormation. SNS can be combined with SQS: Producers push once in SNS, receive in all SQS queues that they subscribed to. It is fully decoupled without any data loss. SQS allows for data persistence, delayed processing and retries. SNS cannot send messages to SQS FIFO queues. Kinesis \u00b6 It is like a managed alternative to Kafka. It uses the same principle and feature set. Data can be kept up to 7 days. hability to replay data, multiple apps consume the same stream. Only one consumer per shard Kinesis Streams : low latency streaming ingest at scale. They offer patterns for data stream processing. Kinesis Analytics : perform real-time analytics on streams using SQL Kinesis Firehose : load streams into S3, Redshift, ElasticSearch. No administration, auto scaling, serverless. One stream is made of many different shards (like Kafka partition). Capacity of 1MB/s or 1000 messages/s at write PER SHARD, and 2MB/s at read PER SHARD. Billing is per shard provisioned, can have as many shards as we want. Batching available or per message calls. captured Metrics are: # of incoming/outgoing bytes, # incoming/outgoing records, Write / read provisioned throughput exceeded, and iterator age ms. It offer a CLI to get stream, list streams, list shard... EventBridge \u00b6 EventBridge is a serverless event bus that makes it easier to build event-driven applications at scale using events generated from your applications, integrated Software-as-a-Service (SaaS) applications, and AWS services. You can ingest, filter, transform and deliver events without writing custom code. Integrate schema registry stores a collection of easy-to-find event schemas and enables you to download code bindings for those schemas in your IDE so you can represent events as a strongly-typed objects in your code Serverless \u00b6 Serveless on AWS is supported by a lot of services: AWS Lambda : Limited by time - short executions, runs on-demand, and automated scaling. Pay per call, duration and memory used. DynamoDB : no sql db, with HA supported by replication across three AZs. millions req/s, trillions rows, 100s TB storage. low latency on read. Support event driven programming with streams: lambda function can read the stream (24h retention). Table oriented, with dynamic attribute but primary key. 400KB max size for one document. It uses the concept of Read Capacity Unit and Write CU. It supports auto-scaling and on-demand throughput. A burst credit is authorized, when empty we get ProvisionedThroughputException. Finally it use the DynamoDB Accelerator to cache data to authorize micro second latency for cached reads. Supports transactions and bulk tx with up to 10 items. AWS Cognito : gives users an identity to interact with the app. AWS API Gateway : API versioning, websocket support, different environment, support authentication and authorization. Handle request throttling. Cache API response. SDK. Support different security approaches: IAM: Great for users / roles already within your AWS account Handle authentication + authorization Leverages Sig v4 Custom Authorizer: Great for 3 rd party tokens Very flexible in terms of what IAM policy is returned Handle Authentication + Authorization Pay per Lambda invocation Cognito User Pool: You manage your own user pool (can be backed by Facebook, Google login etc\u2026) No need to write any custom code Must implement authorization in the backend Amazon S3 AWS SNS & SQS AWS Kinesis Data Firehose Aurora Serverless Step Functions Fargate Lambda@Edge is used to deploy Lambda functions alongside your CloudFront CDN, it is for building more responsive applications, closer to the end user. Lambda is deployed globally. Here are some use cases: Website security and privacy, dynamic webapp at the edge, search engine optimization (SEO), intelligent route across origins and data centers, bot mitigation at the edge, real-time image transformation, A/B testing, user authentication and authorization, user prioritization, user tracking and analytics. Serverless architecture patterns \u00b6 Few write / Lot of reads app (ToDo) \u00b6 The mobile application access application via REST HTTPS through API gateway. This use serverless and users should be able to directly interact with s3 buckets. They first get JWT token to authenticate and the API gateway validates such token. The Gateway delegates to a Lambda function which goes to Dynamo DB. Each of the component supports auto scaling. To improve read throughput cache is used with DAX. Also some of the REST request could be cached in the API gateway. As the application needs to access S3 directly, Cognito generates temporary credentials with STS so the application can authenticate to S3. User's credentials are not saved on the client app. Restricted policy is set to control access to S3 too. To improve throughput we can add DAX as a caching layer in front of DynamoDB: this will also reduce the sizing for DynamoDB. Some of the responses can also be cached at the API gateway level. Serverless hosted web site (Blog) \u00b6 The public web site should scale globally, focus to scale on read, pure static files with some writes. To secure access to S3 content, we use Origin Access Identity and Bucket policy to authorize read only from OAI. To get a welcome message sent when a user register to the app, we can add dynamoDB streams to get changes to the dynamoDB and then calls a lambda that will send an email with the Simple Email Service. DynamoDB Global Table can be used to expose data in different regions by using DynamoDB streams. Microservice \u00b6 Services use REST api to communicate. The service can be dockerized and run with ECS. Integration via REST is done via API gateway and load balancer. Paid content \u00b6 User has to pay to get content (video). We have a DB for users. This is a Serverless solution. Videos are saved in S3. To serve the video, we use Signed URL. So a lambda will build those URLs. CloudFront is used to access videos globally. OAI for security so users cannot bypass it. We can't use S3 signed URL as they are not efficient for global access. Software update distribution \u00b6 The EC2 will be deployed in multi-zones and all is exposed with CloudFront to cache. Big Data pipeline \u00b6 The solution applies the traditional collect, inject, transform and query pattern. IoT Core allows to collect data from IoT devices. Kinesis is used to get data as streams, and then FireHose upload every minute to S3. A Lambda can already do transformation from FireHose. As new files are added to S3 bucket, it trigger a Lambda to call queries defined in Athena. Athena pull the data and build a report published to another S3 bucket that will be used by QuickSight to visualize the data. Other Database considerations \u00b6 Redshift \u00b6 It is based on Postgresql. but not used for OLTP, it is used for analytical processing and data warehousing, scale to PBs. It is Columnar storage of data. It uses massively parallel query execution. Data can be loaded from S3, DynamoDB, DMS and other DBs. It can scale from 1 to 128 nodes, and each node has 160GB per node. The architecture is based on a leader node to support query planning and aggregate results, and compute nodes to perform the queries and send results back. Redshift spectrum performs queries directly on top of S3.","title":"Introduction"},{"location":"#amazon-web-service-studies","text":"Created in 2002, and launched as AWS in 2004 with SQS as first service offering. 2003 amazon.com was $5.2B retail businesses. 7800 employees","title":"Amazon Web Service studies"},{"location":"#why-cloud","text":"moving from capex to variable expense economy of scale: EC2 instance with different pricing model. Usage from hundreds of thousands of customers is aggregated in the cloud Elactic capacity: pay for what you use. Scale up and down so no need to guessed capacity. Speed and agility to define infrastructure in minutes not weeks Focus on business apps, not IT infrastructure and data centers. Global reach in a minutes","title":"Why cloud"},{"location":"#use-cases","text":"Enable to build scalable apps, adaptable to business demand Extend Enterprise IT Support flexible big data analytics","title":"Use cases"},{"location":"#infrastructure","text":"AWS is a global infrastructure with 27 regions and 2 to 6 availability zones per region. Ex: us-west-1-2a. AZ is one or more DC with redundant power, networking and connectivity. Isolated from disasters. Interconnected with low latency network. AWS services are local or global: EC2 is a regional service. Region-scoped services come with availabiltiy and resiliency. IAM is a global service. AWS Local Zone location is an extension of an AWS Region where you can run your latency sensitive application in geography close to end-users. AWS Wavelength enables developers to build applications that deliver single-digit millisecond latencies to mobile devices and end-users. AWS infrastructure deployments that embed AWS compute and storage services within the telecommunications providers\u2019 datacenters at the edge of the 5G networks, and seamlessly access the breadth of AWS services in the region. Choose an AWS region, depending of your requirements like: compliance with data governance and legal requirements close to users to reduce latency availability of service within a region pricing","title":"Infrastructure"},{"location":"#interact-with-aws","text":"Management console: services are placed in categories: compute, serverless, database, analytics... CLI SDK for C++, Go, Java, JavaScript, .NET, Node.js, PHP, Python, and Ruby","title":"Interact with AWS"},{"location":"#organization","text":"When you work with the AWS Cloud, managing security and compliance is a shared responsibility between AWS and you: aws is the security of the cloud you are responsible for the security in the cloud: secure workloads and applications that you deploy onto the cloud. Helps to group accounts, and simplify account creation. Consolidate billing. Concepts: root user: a single sign-in identity that has complete access to all AWS services and resources in the account Organization unit (OU) account be part of 1 OU Define service control policies Organization console","title":"Organization"},{"location":"#iam-identity-and-access-management","text":"This is global services so defined at the account level and cross regions Define user (physical person), group and roles, and permissions Do not use root user, but create user and always use them when login. jerome and mathieu are users get user as administrator, meaning it will be part of an admin group with admin priviledges, like AdmintratorAccess Assign users to groups ( admin and developers ) and assign policies to groups and not to individual user. Groups can only contain users, not other groups Users can belong to multiple groups Users are defined as global service encompasses all regions AWS Account has a unique ID but can be set with an alias (e.g. jbcodeforce ) so to sign in to the console the URL becomes https://jbcodeforce.signin.aws.amazon.com/console Policies are written in JSON, to define permissions Allow , Deny for users to access AWS services, groups and roles Policy applies to Principal: account/user/role, list the actions (what is allowed or denied) on the given resources Least privilege permission: Give users the minimal amount of permissions they need to do their job Policy is a json doc: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"iam:List*\" ], \"Resource\" : \"*\" } ] } Policy can define the password type > Account settings > Password policy , and when users are allowed to change the password. Multi Factor Authentication - always protect root account. MFA = password + device we own. The device could be a universal 2 nd factor security key. (ubikey) Authy is a multi-device service. We can have multiple users on the same device IAM is not used for website authentication and authorization For identity federation, SAML standard is used","title":"IAM Identity and Access Management"},{"location":"#iam-roles","text":"To get AWS services doing work on other service, we use IAM Role. Roles are assigned per application, or EC2 or lambda function... Maintaining roles is more efficient than maintaining users. When you assume a role, IAM dynamically provides temporary credentials that expire after a defined period of time, between 15 minutes and 36 hours. When connected to an EC2 machine via ssh or using EC2 instance connect, we need to set the IAM roles for who can use the EC2. A command like aws iam list-users will not work until a role it attached. To authorize access to a EC2 instance, we use IAM Roles. The DemoEC2Role, for example, is defined to access EC2 in read only: This role is then defined in the EC2 / Security > attach IAM role.","title":"IAM Roles"},{"location":"#security-tools","text":"In IAM, use > Credentials report to download account based report. In IAM, use > Users > select one user (jerome) and then Access Advisor : Access Advisor shows the services that this user can access and when those services were last accessed","title":"Security tools"},{"location":"#aws-cli","text":"We can access AWS using the CLI or the SDK which both use access keys generated from the console (> Users > jerome > Security credentials > Access Keys). The keys are saved in ~/.aws/credentials aws-cli version 2 aws --version # get your users aws iam list-users With Cloudshell we can use aws cli and then have file that will be kept in the filesystems of the cloud shell. [aws-shell] is also available to facilitate the user experience.","title":"AWS CLI"},{"location":"#ebs-volume","text":"Elastic Block Store Volume is a network drive attached to the instance. It is locked to an AZ, and uses provisioned capacity in GBs and IOPS. Create a EBS while creating the EC2 instance and keep it not deleted on shutdown Once logged, add a filesystem, mount to a folder and modify boot so the volume is mounted at start time. Which looks like: # List existing block storage, verify our created storage is present lsblk # Verify file system type sudo file -s /dev/xdvf # Create a ext4 file system on the device sudo mkfs -t ext4 /dev/xvdb # make a mount point sudo mkdir /data sudo mount /dev/xvdb /data # Add entry in /etc/fstab with line like: /dev/xvdb /data ext4 default,nofail 0 2 EBS is already a redundant storage, replicated within an AZ. EC2 instance has a logical volume that can be attached to two or more EBS RAID 0 volumes, where write operations are distributed among them. It is used to increate IOPS without any fault tolerance. If one fails, we lost data. It could be used for database with built-in replication or Kafka. RAID 1 is for better fault tolerance: a write operation is going to all attached volumes.","title":"EBS Volume"},{"location":"#volume-types","text":"GP2 : used for most workload up to 16 TB at 16000 IOPS max (3 IOPS per GB brustable to 3000) io 1 : critical app with large database workloads. max ratio 50:1 IOPS/GB. Min 100 iops and 4G to 16T st 1 : Streaming workloads requiring consistent, fast throughput at a low price. For Big data, Data warehouses, Log processing, Apache Kafka sc 1 : throughput oriented storage. 500G- 16T, 500MiB/s. Max IOPs at 250. Used for cold HDD, and infrequently accessed data. Encryption has a minimum impact on latency. It encrypts data at rest and during snapshots. Instance store is a volume attached to the instance, used for root folder. It is a ephemeral storage but has millions read per s and 700k write IOPS. It provides the best disk performance and can be used to have high performance cache for our applications. If we need to run a high-performance database that requires an IOPS of 210,000 for its underlying filesystem, we need instance store and DB replication in place.","title":"Volume types"},{"location":"#snapshots","text":"Used to backup disk and stored on S3. Snapshot Lifecycle policies helps to create snapshot with scheduling it by defining policies. To move a volume to another AZ or data center we can create a volume from a snapshot.","title":"Snapshots"},{"location":"#elastic-file-system","text":"Managed Network FS for multi AZs. (3x gp2 cost), controlled by using security group. This security group needs to add in bound rule of type NFS connected / linked to the SG of the EC2. Only Linux based AMI. Encryption is supported using KMS. 1000 concurrent clients 10GB+/s throughput, bursting or provisioned. Support different performance mode, like max I/O or general purpose Support storage tiers to move files after n days, infrequent EFS-IA for files rarely accessed. Use amazon EFS util tool in each EC2 instance to mount the EFS to a target mount point.","title":"Elastic File System"},{"location":"#relational-database-service-rds","text":"Managed service for SQL based database (mySQL, Postgresql, SQL server, Oracle). Must provision EC2 instance and EBS volume. Support multi AZs for DR with automatic failover to standby, app uses one unique DNS name. Continuous backup and restore to specific point of time restore. It uses gp2 or io1 EBS. Transaction logs are backed-up every 5 minutes. Support user triggered snapshot. Read replicas: helps to scale the read operations. Can create up to 5 replicas within AZ, cross AZ and cross region. Replication is asynch. Use cases include, reporting, analytics, ML model AWS charge for network when for example data goes from one AZ to another. Support at rest Encryption. Master needs to be encrypted to get encrypted replicas. We can create a snapshot from unencrypted DB and then copy it by enabling the encryption for this snapshot. From there we can create an Encrypted DB Our responsibility: Check the ports / IP / security group inbound rules in DB\u2019s SG In-database user creation and permissions or manage through IAM Creating a database with or without public access Ensure parameter groups or DB is configured to only allow SSL connections From a solution architecture point of view: Operations : small downtime when failover happens. For maintenance, scaling in read replicas, updating underlying ec2 instance, or restore EBS, there will be manual intervention. Security : AWS responsible for OS security, we are responsible for setting up KMS, security groups, IAM policies, authorizing users in DB, enforcing SSL. Reliability : Multi AZ feature helps to address it, with failover mechanism in case of failures Performance : depends on EC2 instance type, EBS volume type, ability to add Read Replicas. Doesn\u2019t auto-scale, adapt to workload manually.","title":"Relational Database Service - RDS"},{"location":"#aurora","text":"Proprietary SQL database, work using postgresql and mysql driver. It is cloud optimized and claims 5x performance improvement over mySQL on RDS, and 3x for postgresql. Can grow up to 64 TB. Sub 10ms replica lag, up to 15 replicas. Failover in Aurora is instantaneous. It\u2019s HA (High Availability) native. Use 1 master - 5 readers to create 6 copies over 3 AZs. 3 copies of 6 need for reads. Peer to peer replication. Use 100s volumes. Autoscaling on the read operation. It is CQRS at DB level, and read can be global. Use writer end point and reader endpoint. It also supports one write with multiple reader and parallel query, multiple writes and serverless to automate scaling down to zero (No capacity planning needed and pay per second). With Aurora global database one primary region is used for write and then up to 5 read only regions with replica lag up to 1 s. Promoting another region (for disaster recovery) has an RTO of < 1 minute Operations : less operation, auto scaling storage. Security : AWS responsible for OS security, we are responsible for setting up KMS, security groups, IAM policies, authorizing users in DB, enforcing SSL. Reliability : Multi AZ, HA Performance : 5x performance, up to 15 read replicas.","title":"Aurora"},{"location":"#elasticache","text":"Get a managed Redis or Memcached cluster. Applications queries ElastiCache, if not available, get from RDS and store in ElastiCache. Key-Value store. It can be used for user session store so user interaction can go to different application instances. Redis is a multi AZ with Auto-Failover, supports read replicas to scale and for high availability. It can persist data using AOF persistence, and has backup and restore features. Memcached is a multi-node for partitioning of data (sharding), and no persistence, no backup and restore. It is based on a multi-threaded architecture. Some patterns for ElastiCache: Lazy Loading : all the read data is cached, data can become stale in cache Write Through : Adds or update data in the cache when written to a DB (no stale data) Session Store : store temporary session data in a cache (using TTL features) Sub millisecond performance, in memory read replicas for sharding.","title":"ElastiCache"},{"location":"#dynamodb","text":"AWS proprietary NoSQL database, Serverless, provisioned capacity, auto scaling, on demand capacity. Highly Available, Multi AZ by default, Read and Writes are decoupled, and DAX can be used for read cache. The read operations can be eventually consistent or strongly consistent. DynamoDB Streams to integrate with AWS Lambda.","title":"DynamoDB"},{"location":"#route-53","text":"It is a managed Domain Name System. DNS is a collection of rules and records which helps clients understand how to reach a server through URLs. Here is a quick figure to summary the process DNS records Time to Live (TTL), is set to get the web browser to keep the DNS resolution in cache. High TTL is around 24 hours, low TTL at 60s will make more DNS calls. TTL should be set to strike a balance between how long the value should be cached vs how much pressure should go on the DNS. Need to define the TTL for the app depending on the expected deployment model. A hosted zone is a container that holds information about how we want to route traffic for a domain. Two types are supported: public or private within a VPC. Route 53 is a registrar. We can buy domain name. Use dig <hostname> to get the DNS resolution record.","title":"Route 53"},{"location":"#cname-vs-alias","text":"CNAME is a DNS record to maps one domain name to another. CNAME should point to a ALB. Alias is used to point a hostname of an AWS resource and can work on root domain (domainname.com).","title":"CNAME vs Alias"},{"location":"#routing","text":"A simple routing policy to get an IP @ from a hostname could not have health check defined. The weighted routing policy controls the % of the requests that go to specific endpoint. Can do blue-green traffic management. It can also help to split traffic between two regions. It can be associated with Health Checks The latency routing Policy redirects to the server that has the least latency close to the client. Latency is evaluated in terms of user to designated AWS Region. Health check monitors the health and performance of the app servers or endpoints and assess DNS failure. We can have HTTP, TCP or HTTPS health checks. We can define from which region to run the health check. They are charged per HC / month. It is recommended to have one HC per app deployment. It can also monitor latency. The failover routing policy helps us to specify a record set to point to a primary and then a secondary instance for DR. The Geo Location routing policy is based on user's location, and we may specify how the traffic from a given country should go to this specific IP. Need to define a \u201cdefault\u201d policy in case there\u2019s no match on location. The Multi Value routing policy is used to access multiple resources. The record set, associates a Route 53 health checks with records. The client on DNS request gets up to 8 healthy records returned for each Multi Value query. If one fails then the client can try one other IIP @ from the list.","title":"Routing"},{"location":"#some-application-patterns","text":"For solution architecture, we need to assess cost, performance, reliability, security and operational excellence.","title":"Some application patterns"},{"location":"#stateless-app","text":"The step to grow a stateless app: add vertical scaling by changing the EC2 profile, but while changing, user has out of service. Second step is to scale horizontal, each EC2 instance has static IP address and DNS is configured with 'A record' to get each EC2 end point. But if one instance is gone, the client App will see it down until TTL expires. The reference architecture includes DNS record set with alias record (to point to ALB. Using alias as ALB address may change over time) with TTL of 1 hour. Use load balancers in 3 AZs (to survive disaster) to hide the horizontal scaling of EC2 instances (managed with auto scaling group) where the app runs. Health checks are added to keep system auto adaptable and hide system down, and restricted security group rules to control EC2 instance accesses. ALB and EC instances are in multi different AZs. The EC instances can be set up with reserved capacity to control cost.","title":"Stateless App"},{"location":"#stateful-app","text":"In this case we will add the pattern of shopping cart. If we apply the same architecture as before, at each interaction of the user, it is possible the traffic will be sent to another EC2 instance that started to process the shopping cart. Using ELB with stickiness will help to keep the traffic to the same EC2, but in case of EC2 failure we still loose the cart. An alternate is to use user cookies to keep the cart at each interaction. It is back to a stateless app as state is managed by client and cookie. For security reason the app needs to validate the cookie content. cookie has a limit of 4K data. Another solution is to keep session data into an elastic cache, like Redis, and use the sessionId as key and persisted in a user cookie. So EC2 managing the interaction can get the cart data from the cache using the sessionID. It can be enhanced with a RDS to keep user data. Which can also support the CQRS pattern with read replicas. Cache can be update with data from RDS so if the user is requesting data in session, it hits the cache. Cache and database are set on multi AZ, as well as EC2 instance and load balancer, all to support disaster. Security groups need to be defined to get all traffic to the ELB and limited traffic between ELB and EC2 and between EC2 and cache and EC2 and DB. Another example of stateful app is the ones using image stored on disk. In this case EC2 EBS volume will work only for one app instance, but for multi app scaling out, we need to have a Elastic FS which can be Multi AZ too.","title":"Stateful app"},{"location":"#deploying-app","text":"The easiest solution is to create AMI containing OS, dependencies and app binary. This is completed with User Data to get dynamic configuration. Database data can be restored from Snapshot, and the same for EFS data. Elastic Beanstalk is a developer centric view of the app, hiding the complexity of the IaaS. From one git repository it can automatically handle the deployment details of capacity provisioning, load balancing, auto-scaling, and application health monitoring.","title":"Deploying app"},{"location":"#s3","text":"Amazon S3 allows people to store objects (files) in buckets (directories), which must have a globally unique name (cross users!). They are defined at the region level. Object in a bucket, is referenced as a key which can be seen as a file path in a file system. The max size for an object is 5 TB but big file needs to be uploaded in multipart using 5GB max size. S3 supports versioning at the bucket level. So file can be restored from previous version, and even deleted file can be retrieved from a previous version.","title":"S3"},{"location":"#use-cases_1","text":"Backup and restore DR Archive Data lakes Hybrid cloud storage: seamless connection between on-premises applications and S3 with AWS Storage Gateway. Cloud-native application data GETTIG started","title":"Use cases"},{"location":"#security-control","text":"Objects can also be encrypted, and different mechanisms are available: SSE-S3 : server-side encrypted S3 objects using keys handled & managed by AWS using AES-256 protocol must set x-amz-server-side-encryption: \"AES256\" header in the POST request. SSE-KMS : leverage AWS Key Management Service to manage encryption keys. x-amz-server-side-encryption: \"aws:kms\" header. Server side encrypted. It gives user control of the key rotation policy and audit trail. SSE-C : when we want to manage our own encryption keys. Server-side encrypted. Encryption key must be provided in HTTP headers, for every HTTP request made. HTTPS is mandatory Client Side Encryption : encrypt before sending object. Explicit DENY in an IAM policy will take precedence over a bucket policy permission.","title":"Security control"},{"location":"#s3-website","text":"We can have static web site on S3. Once html pages are uploaded, setting the properties as static web site from the bucket. The bucket needs to be public, and have a security policy to allow any user to GetObject action. The URL may look like: <bucket-name>.s3-website.<AWS-region>.amazonaws.com Cross Origin resource sharing CORS : The web browser requests won\u2019t be fulfilled unless the other origin allows for the requests, using CORS Headers Access-Control-Allow-Origin . If a client does a cross-origin request on our S3 bucket, we need to enable the correct CORS headers: this is done by adding a security policy with CORS configuration like: <CORSConfiguration> <CORSRule> <AllowedOrigin> enter-bucket-url-here </AllowedOrigin> <AllowedMethod> GET </AllowedMethod> <MaxAgeSeconds> 3000 </MaxAgeSeconds> <AllowedHeader> Authorization </AllowedHeader> </CORSRule> </CORSConfiguration> Finally S3 is eventually consistent.","title":"S3 Website"},{"location":"#s3-storage-classes","text":"When uploading a document into an existing bucket we can specify the storage class for keep data over time. Different levels are offered with different cost and SLA. To prevent accidental file deletes, we can setup MFA Delete to use MFA tokens before deleting objects. Amazon Glacier is for archiving, like writing to tapes. We can transition objects between storage classes. For infrequently accessed object, move them to STANDARD_IA. For archive objects, that we don\u2019t need in real-time, use GLACIER or DEEP_ARCHIVE. Moving objects can be automated using a lifecycle configuration At the bucket level, a user can define lifecycle rules for when to transition an object to another storage class. To improve performance, a big file can be split and then uploaded with local connection to the closed edge access and then use AWS private network to copy between buckets in different region. S3 to Kafka lab","title":"S3 Storage classes"},{"location":"#aws-athena","text":"AWS Athena runs analytics directly on S3 files, using SQL language to query the files (CSV, JSON, Avro, Parquet...). S3 Access Logs log all the requests made to buckets, and Athena can then be used to run serverless analytics on top of the logs files. Queries are done on high availability capability so will succeed, and scale based on the data size. No need for complex ETL jobs to prepare your data for analytics. Integrated with AWS Glue Data Catalog , allowing you to create a unified metadata repository across various services, crawl data sources to discover schemas and populate your Catalog with new and modified table and partition definitions, and maintain schema versioning.","title":"AWS Athena"},{"location":"#aws-cli_1","text":"The cli needs to be configured: aws configure with the credential, key and region to access. Use IAM user to get a new credentials key. When using CLI in a EC2 instance always use an IAM role to control security credentials. This role can come with a policy authorizing exactly what the EC2 instances should be able to do. Also within a EC2 instance, it is possible to use the URL http://169.254.169.254/latest/meta-data to get information about the EC2. We can retrieve the IAM Role name from that metadata.","title":"AWS CLI"},{"location":"#cloudfront","text":"CDN service with DDoS protection. It caches data to the edge to improve web browsing and app performance. 216 Edge locations. The origins of those files are S3 buckets, Custom resource accessible via HTTP. CloudFront keeps cache for the data read. For the edge to access the S3 bucket, it uses an origin access identity (OAI), managed as IAM role. For EC2 instance, the security group needs to accept traffic from edge location IP addresses. It is possible to control with geo restriction. It also supports the concept of signed URL. When you want to distribute content to different user groups over the world, attach a policy with: URL expiration IP ranges to access the data from Trusted signers (which AWS accounts can create signed URLs) How long should the URL be valid for? Shared content (movie, music): make it short (a few minutes) Private content (private to the user): you can make it last for years Signed URL = access to individual files (one signed URL per file) Signed Cookies = access to multiple files (one signed cookie for many files)","title":"CloudFront"},{"location":"#storage","text":"","title":"Storage"},{"location":"#snowball","text":"Move TB of data in and out AWS using physical device to ship data. The edge has 100TB and compute power to do some local processing on data. Snow mobile is a truck with 100 PB capacity. Once on site, it is transferred to S3. Snowball Edge brings computing capabilities to allow data pre-processing while it's being moved in Snowball, so we save time on the pre-processing side as well.","title":"Snowball"},{"location":"#hybrid-cloud-storage","text":"Storage gateway expose an API in front of S3. Three gateway types: file : S3 bucket accessible using NFS or SMB protocols. Controlled access via IAM roles. File gateway is installed on-premise and communicate with AWS. volume : this is a block storage using iSCSI protocol. On-premise and visible as a local volume backed by S3. tape : same approach but with virtual tape library. Can go to S3 and Glacier.","title":"Hybrid cloud storage"},{"location":"#storage-comparison","text":"S3: Object Storage Glacier: Object Archival EFS: Network File System for Linux instances, POSIX filesystem FSx for Windows: Network File System for Windows servers FSx for Lustre: High Performance Computing Linux file system EBS volumes: Network storage for one EC2 instance at a time Instance Storage: Physical storage for your EC2 instance (high IOPS) Storage Gateway: File Gateway, Volume Gateway (cache & stored), Tape Gateway Snowball / Snowmobile: to move large amount of data to the cloud, physically Database: for specific workloads, usually with indexing and querying","title":"Storage comparison"},{"location":"#integration-and-middleware-sqs-kinesis-active-mq","text":"","title":"Integration and middleware: SQS, Kinesis Active MQ"},{"location":"#sqs-standard-queue","text":"Oldest queueing service on AWS. The default retention is 4 days up to 14 days. low latency < 10ms. Duplicate messages is possible and out of order too. Consumer deletes the message. It is auto scaling. Specific SDK to integrate to SendMessage... Consumers receive, process and then delete. Parallelism is possible on the different messages. The consumers can be in an auto scaling group so with CloudWatch, it is possible to monitor the queue size / # of instances and on the CloudWatch alarm action, trigger scaling. Max mesage size is 256KB. Message has metadata out of the box. After a message is polled by a consumer, it becomes invisible to other consumers. By default, the \u201cmessage visibility timeout\u201d is 30 seconds, which means the message has 30 seconds to be processed (Amazon SQS prevents other consumers from receiving and processing the message). After the message visibility timeout is over, the message is \u201cvisible\u201d in SQS, so it may be processed twice. But a consumer could call the ChangeMessageVisibility API to get more time. When the visibility timeout is high (hours), and the consumer crashes then the re-processing of all the message will take time. If it is set too low (seconds), we may get duplicates Encryption in fight via HTTPS, at rest encryption with KMS keys. Comes with monitoring. If a consumer fails to process a message within the Visibility Timeout, the message goes back to the queue. But if we can set a threshold of how many times a message can go back to the queue. After the MaximumReceives threshold is exceeded, the message goes into a dead letter queue (DLQ) (which has a limit of 14 days to process). Delay queues let you postpone the delivery of new messages to a queue for a number of seconds. If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period. The default (minimum) delay for a queue is 0 seconds. The maximum is 15 minutes Queue can be set as FIFO to guaranty the order: limited to throughput at 300 msg/s without batching or 3000 msg/s with batching. I can also support exactly once. It can be set to remove duplicate by looking at the content.","title":"SQS: Standard queue"},{"location":"#simple-notification-service-is-for-topic-pubsub","text":"SNS supports up to 10,000,000 subscriptions per topic, 100,000 topics limit. The subscribers can publish to topic via SDK and can use different protocols like: HTTP / HTTPS (with delivery retries \u2013 how many times), SMTP, SMS, ... The subscribers can be a SQS, a Lambda, Emails... Many AWS services can send data directly to SNS for notifications: CloudWatch (for alarms), Auto Scaling Groups notifications, Amazon S3 (on bucket events), CloudFormation. SNS can be combined with SQS: Producers push once in SNS, receive in all SQS queues that they subscribed to. It is fully decoupled without any data loss. SQS allows for data persistence, delayed processing and retries. SNS cannot send messages to SQS FIFO queues.","title":"Simple Notification Service is for topic pub/sub"},{"location":"#kinesis","text":"It is like a managed alternative to Kafka. It uses the same principle and feature set. Data can be kept up to 7 days. hability to replay data, multiple apps consume the same stream. Only one consumer per shard Kinesis Streams : low latency streaming ingest at scale. They offer patterns for data stream processing. Kinesis Analytics : perform real-time analytics on streams using SQL Kinesis Firehose : load streams into S3, Redshift, ElasticSearch. No administration, auto scaling, serverless. One stream is made of many different shards (like Kafka partition). Capacity of 1MB/s or 1000 messages/s at write PER SHARD, and 2MB/s at read PER SHARD. Billing is per shard provisioned, can have as many shards as we want. Batching available or per message calls. captured Metrics are: # of incoming/outgoing bytes, # incoming/outgoing records, Write / read provisioned throughput exceeded, and iterator age ms. It offer a CLI to get stream, list streams, list shard...","title":"Kinesis"},{"location":"#eventbridge","text":"EventBridge is a serverless event bus that makes it easier to build event-driven applications at scale using events generated from your applications, integrated Software-as-a-Service (SaaS) applications, and AWS services. You can ingest, filter, transform and deliver events without writing custom code. Integrate schema registry stores a collection of easy-to-find event schemas and enables you to download code bindings for those schemas in your IDE so you can represent events as a strongly-typed objects in your code","title":"EventBridge"},{"location":"#serverless","text":"Serveless on AWS is supported by a lot of services: AWS Lambda : Limited by time - short executions, runs on-demand, and automated scaling. Pay per call, duration and memory used. DynamoDB : no sql db, with HA supported by replication across three AZs. millions req/s, trillions rows, 100s TB storage. low latency on read. Support event driven programming with streams: lambda function can read the stream (24h retention). Table oriented, with dynamic attribute but primary key. 400KB max size for one document. It uses the concept of Read Capacity Unit and Write CU. It supports auto-scaling and on-demand throughput. A burst credit is authorized, when empty we get ProvisionedThroughputException. Finally it use the DynamoDB Accelerator to cache data to authorize micro second latency for cached reads. Supports transactions and bulk tx with up to 10 items. AWS Cognito : gives users an identity to interact with the app. AWS API Gateway : API versioning, websocket support, different environment, support authentication and authorization. Handle request throttling. Cache API response. SDK. Support different security approaches: IAM: Great for users / roles already within your AWS account Handle authentication + authorization Leverages Sig v4 Custom Authorizer: Great for 3 rd party tokens Very flexible in terms of what IAM policy is returned Handle Authentication + Authorization Pay per Lambda invocation Cognito User Pool: You manage your own user pool (can be backed by Facebook, Google login etc\u2026) No need to write any custom code Must implement authorization in the backend Amazon S3 AWS SNS & SQS AWS Kinesis Data Firehose Aurora Serverless Step Functions Fargate Lambda@Edge is used to deploy Lambda functions alongside your CloudFront CDN, it is for building more responsive applications, closer to the end user. Lambda is deployed globally. Here are some use cases: Website security and privacy, dynamic webapp at the edge, search engine optimization (SEO), intelligent route across origins and data centers, bot mitigation at the edge, real-time image transformation, A/B testing, user authentication and authorization, user prioritization, user tracking and analytics.","title":"Serverless"},{"location":"#serverless-architecture-patterns","text":"","title":"Serverless architecture patterns"},{"location":"#other-database-considerations","text":"","title":"Other Database considerations"},{"location":"#redshift","text":"It is based on Postgresql. but not used for OLTP, it is used for analytical processing and data warehousing, scale to PBs. It is Columnar storage of data. It uses massively parallel query execution. Data can be loaded from S3, DynamoDB, DMS and other DBs. It can scale from 1 to 128 nodes, and each node has 160GB per node. The architecture is based on a leader node to support query planning and aggregate results, and compute nodes to perform the queries and send results back. Redshift spectrum performs queries directly on top of S3.","title":"Redshift"},{"location":"gettingstarted/","text":"Getting started \u00b6 General AWS console from which we can login as root user or as an IAM user. Or the one for my account alias jbcodeforce : https://jbcodeforce.signin.aws.amazon.com/console and user jerome The credentials and API key are in ~/.aws/credentials Select a region to create your resources: N California Look at the services. aws CLI \u00b6 Installation: aws cli Defined users and groups with IAM \u00b6 my summary on IAM Search for IAM and then... login to the account https://jbcodeforce.signin.aws.amazon.com/console with admin user jerome Create groups (Developers), define basic policies. Add users (mathieu) assign him to a group Define policies \u00b6 Attached to the group level. Create a EC2 instance with Terraform \u00b6 Build a main.tf (/Code/Studies/terraform/learn-terraform-aws-instance/main.tf) like below, which uses the aws provider to provision a micro EC2 instance: terraf orm { required_providers { aws = { source = \"hashicorp/aws\" versio n = \"~> 3.27\" } } required_versio n = \">= 0.14.9\" } provider \"aws\" { pro f ile = \"default\" regio n = \"us-west-2\" } resource \"aws_instance\" \"app_server\" { ami = \"ami-830c94e3\" i nstan ce_ t ype = \"t2.micro\" ta gs = { Name = \"ExampleAppServerInstance\" } } Resource blocks contain arguments which you use to configure the resource. Arguments can include things like machine sizes, disk image names, or VPC IDs. terraform apply # inspect state terraform show Install nginx inside a EC2 t2.micro. \u00b6 Be sure to have a policy to authorize HTTP inbound traffic on port 80 for 0.0.0.0/0 In the user data add web server: #!/bin/bash yum update -y yum install -y httpd systemctl start httpd systemctl enable httpd echo \"<h1>Hello from $( hostname -f ) </h1>\" > /var/www/html/index.html Define load balancer \u00b6 Deploy a Web App on AWS Elastic Beanstalk \u00b6 Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS Guide","title":"Playground"},{"location":"gettingstarted/#getting-started","text":"General AWS console from which we can login as root user or as an IAM user. Or the one for my account alias jbcodeforce : https://jbcodeforce.signin.aws.amazon.com/console and user jerome The credentials and API key are in ~/.aws/credentials Select a region to create your resources: N California Look at the services.","title":"Getting started"},{"location":"gettingstarted/#aws-cli","text":"Installation: aws cli","title":"aws CLI"},{"location":"gettingstarted/#defined-users-and-groups-with-iam","text":"my summary on IAM Search for IAM and then... login to the account https://jbcodeforce.signin.aws.amazon.com/console with admin user jerome Create groups (Developers), define basic policies. Add users (mathieu) assign him to a group","title":"Defined users and groups with IAM"},{"location":"gettingstarted/#define-policies","text":"Attached to the group level.","title":"Define policies"},{"location":"gettingstarted/#create-a-ec2-instance-with-terraform","text":"Build a main.tf (/Code/Studies/terraform/learn-terraform-aws-instance/main.tf) like below, which uses the aws provider to provision a micro EC2 instance: terraf orm { required_providers { aws = { source = \"hashicorp/aws\" versio n = \"~> 3.27\" } } required_versio n = \">= 0.14.9\" } provider \"aws\" { pro f ile = \"default\" regio n = \"us-west-2\" } resource \"aws_instance\" \"app_server\" { ami = \"ami-830c94e3\" i nstan ce_ t ype = \"t2.micro\" ta gs = { Name = \"ExampleAppServerInstance\" } } Resource blocks contain arguments which you use to configure the resource. Arguments can include things like machine sizes, disk image names, or VPC IDs. terraform apply # inspect state terraform show","title":"Create a EC2 instance with Terraform"},{"location":"gettingstarted/#install-nginx-inside-a-ec2-t2micro","text":"Be sure to have a policy to authorize HTTP inbound traffic on port 80 for 0.0.0.0/0 In the user data add web server: #!/bin/bash yum update -y yum install -y httpd systemctl start httpd systemctl enable httpd echo \"<h1>Hello from $( hostname -f ) </h1>\" > /var/www/html/index.html","title":"Install nginx inside a EC2 t2.micro."},{"location":"gettingstarted/#define-load-balancer","text":"","title":"Define load balancer"},{"location":"gettingstarted/#deploy-a-web-app-on-aws-elastic-beanstalk","text":"Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS Guide","title":"Deploy a Web App on AWS Elastic Beanstalk"},{"location":"k8s/","text":"Kubernetes AWS Service \u00b6 ECR - Elastic Container Registry \u00b6 ECS - Elastic Container Service \u00b6 Amazon ECS is a fully managed container orchestration service, Amazon EKS is a fully managed Kubernetes service, both services support Fargate to provide serverless compute for containers. Fargate removes the need to provision and manage servers, lets us specify and pay for resources per application, and improves security through application isolation by design. Apply docker compose to Amazon ECS and Fargate EKS \u00b6 Other interesting stuff \u00b6 IBM Cloud pak for integration on AWS","title":"Kubernetes services"},{"location":"k8s/#kubernetes-aws-service","text":"","title":"Kubernetes AWS Service"},{"location":"k8s/#ecr-elastic-container-registry","text":"","title":"ECR - Elastic Container Registry"},{"location":"k8s/#ecs-elastic-container-service","text":"Amazon ECS is a fully managed container orchestration service, Amazon EKS is a fully managed Kubernetes service, both services support Fargate to provide serverless compute for containers. Fargate removes the need to provision and manage servers, lets us specify and pay for resources per application, and improves security through application isolation by design. Apply docker compose to Amazon ECS and Fargate","title":"ECS - Elastic Container Service"},{"location":"k8s/#eks","text":"","title":"EKS"},{"location":"k8s/#other-interesting-stuff","text":"IBM Cloud pak for integration on AWS","title":"Other interesting stuff"},{"location":"psa-role/","text":"Principal Solution Architect \u00b6 Role \u00b6 technical leader and a strategic influencer like a CTO architect solutions to significantly complex problems, high ambiguity, leverage technical, industry, and business context expertise (e.g., outcome priorities, customer experience, shared goals, business case) to influence the direction of longer-term business and technology strategies identify both immediate and future risks and constraints advise customers on how to make the right trade-offs in solutions design (e.g., extensibility, flexibility, scalability, maintainability) able to dive deeply into technical details own the design and delivery of a program of customer solutions including the overall strategy and end-to-end architecture. Apply the design principles of security, reliability, cost optimization, operational excellence, and performance efficiency. proactively look for opportunities to scale the solution to benefit other customers with similar problems or requirements lead the curation of thought leadership content and ensure delivered content is relevant to customer needs. Attitude \u00b6 relentlessly simplify and are able to deconstruct extraordinarily complex problems into their constituent building blocks, accelerating customer adoption and/or enabling teams across the organization to work in parallel on the problem. recognize problems both inside and outside your area, build consensus around a vision, and drive resolution leveraging the right resources","title":"SA"},{"location":"psa-role/#principal-solution-architect","text":"","title":"Principal Solution Architect"},{"location":"psa-role/#role","text":"technical leader and a strategic influencer like a CTO architect solutions to significantly complex problems, high ambiguity, leverage technical, industry, and business context expertise (e.g., outcome priorities, customer experience, shared goals, business case) to influence the direction of longer-term business and technology strategies identify both immediate and future risks and constraints advise customers on how to make the right trade-offs in solutions design (e.g., extensibility, flexibility, scalability, maintainability) able to dive deeply into technical details own the design and delivery of a program of customer solutions including the overall strategy and end-to-end architecture. Apply the design principles of security, reliability, cost optimization, operational excellence, and performance efficiency. proactively look for opportunities to scale the solution to benefit other customers with similar problems or requirements lead the curation of thought leadership content and ensure delivered content is relevant to customer needs.","title":"Role"},{"location":"psa-role/#attitude","text":"relentlessly simplify and are able to deconstruct extraordinarily complex problems into their constituent building blocks, accelerating customer adoption and/or enabling teams across the organization to work in parallel on the problem. recognize problems both inside and outside your area, build consensus around a vision, and drive resolution leveraging the right resources","title":"Attitude"},{"location":"sol-design/","text":"Solution design with AWS services \u00b6 We try to address disaster and recovery for different needs, knowing that not all applications need active/active deployment. Backup multi-regions \u00b6 The simplest resilience solution is to use backup and restore mechanism. Data and configuration can be moved to S3 in the second region. For even longer time we can use Glacier. Use database service to ensure HA at the zone level, and replicate data within AZ. RPO will be average time between snapshots - and RTO at the day level. Warm region \u00b6 For applications, where we want to limit out of services time, the approach is to replicate AMI images so app servers, in DR region, can be restarted quickly. And Database are replicated and warm on the second region. Web servers are also warm but not receiving traffic. If something go down in region 1, the internet facing router (53) will route to local balancers in second region. RTO is now in minutes, and RPO average time between DB snapshots. Active - Active between multi regions \u00b6 Write global - read local pattern \u00b6 Users close to one region will read from this region and all write operations go to a global service / region. Database replications and snapshot replications are done to keep data eventually consistent between regions. Those synchronisations are in sub second. Write to origin - read local pattern \u00b6 To increase in complexity, R/W can go the local region. So when a user writes new records, he/she is associated to a region, so the application is sharding the data. When the user moved to another region, write operation will still go to the first region, while read could happened on the region close to him. This applies to applications with write to read ratio around 50%. Write / read local (anti) pattern \u00b6 This pattern uses two master DB, one in each region so user can write and read locally. Dual writes, in each region, at the same time may generate the same key but record will have different data. You have inconsistency, and it is difficult to figure out, and rollback. So use this pattern only if you cannot do the two previous patterns. AWS Services supporting HA and DR multi-regions \u00b6 S3 EBS dynamoDB","title":"Solution design"},{"location":"sol-design/#solution-design-with-aws-services","text":"We try to address disaster and recovery for different needs, knowing that not all applications need active/active deployment.","title":"Solution design with AWS services"},{"location":"sol-design/#backup-multi-regions","text":"The simplest resilience solution is to use backup and restore mechanism. Data and configuration can be moved to S3 in the second region. For even longer time we can use Glacier. Use database service to ensure HA at the zone level, and replicate data within AZ. RPO will be average time between snapshots - and RTO at the day level.","title":"Backup multi-regions"},{"location":"sol-design/#warm-region","text":"For applications, where we want to limit out of services time, the approach is to replicate AMI images so app servers, in DR region, can be restarted quickly. And Database are replicated and warm on the second region. Web servers are also warm but not receiving traffic. If something go down in region 1, the internet facing router (53) will route to local balancers in second region. RTO is now in minutes, and RPO average time between DB snapshots.","title":"Warm region"},{"location":"sol-design/#active-active-between-multi-regions","text":"","title":"Active - Active between multi regions"},{"location":"sol-design/#write-global-read-local-pattern","text":"Users close to one region will read from this region and all write operations go to a global service / region. Database replications and snapshot replications are done to keep data eventually consistent between regions. Those synchronisations are in sub second.","title":"Write global - read local pattern"},{"location":"sol-design/#write-to-origin-read-local-pattern","text":"To increase in complexity, R/W can go the local region. So when a user writes new records, he/she is associated to a region, so the application is sharding the data. When the user moved to another region, write operation will still go to the first region, while read could happened on the region close to him. This applies to applications with write to read ratio around 50%.","title":"Write to origin - read local pattern"},{"location":"sol-design/#write-read-local-anti-pattern","text":"This pattern uses two master DB, one in each region so user can write and read locally. Dual writes, in each region, at the same time may generate the same key but record will have different data. You have inconsistency, and it is difficult to figure out, and rollback. So use this pattern only if you cannot do the two previous patterns.","title":"Write / read local (anti) pattern"},{"location":"sol-design/#aws-services-supporting-ha-and-dr-multi-regions","text":"S3 EBS dynamoDB","title":"AWS Services supporting HA and DR multi-regions"},{"location":"well-architectured/","text":"AWS Well Architectured \u00b6 The AWS Well-Architected Framework helps you understand the pros and cons of decisions you make while building systems on AWS. Those are the questions to ask for designing a cloud native solution by understanding the potential impact . All hardware are becoming software. Workload represents interrelated applications, infrastructure, policies, governance and operations. Six pilards \u00b6 When architecting technology solutions, never neglect the six pillars of: Operational Excellence Security Reliability Performance Efficiency Cost Optimization Sustainability Design Principles \u00b6 Stop guessing your capacity needs : use as much or as little capacity as you need, and scale up and down automatically. Test systems at production scale , then decommission the resources. Automate to make architectural experimentation easier . Allow for evolutionary architectures : the capability to automate and test on demand lowers the risk of impact from design changes. This allows systems to evolve over time so that businesses can take advantage of innovations as a standard practice. Drive architectures using data : In the cloud, you can collect data on how your architectural choices affect the behavior of your workload. This lets you make fact-based decisions on how to improve your workload. Improve through game days to simulate events in production. This will help you understand where improvements can be made and can help develop organizational experience in dealing with events. Operational Excellence \u00b6 Support development and run workloads effectively, gain insight into their operations, and to continuously improve supporting processes and procedures. Four area of focus: Organization: define clear responsabilities, roles, and success interdependencies Prepare: design telemetry (logs, metrics...), improve flow, mitigate deployment risks, understand operational readiness Operate: understand workload health, operation health, achievement of business outcome. Runbooks and playbooks should define escalation process, and define owneship for each action Evolve: learn from experience, make improvements, share with teams Design principles: Perform operations as code Make frequent, small, reversible changes Refine operations procedures frequently. Set up regular game days to review and validate that all procedures are effective. Anticipate failure: Perform \u201cpre-mortem\u201d exercises to identify potential sources of failure so that they can be removed or mitigated. Learn from all operational failures Use established runbooks for well-understood events, and use playbooks to aid in investigation and resolution of issues. CloudWatch is used to aggregate and present business, workload, and operations level views of operations activities. Questions to assess How do you determine what your priorities are? How do you structure your organization to support your business outcomes? How does your organizational culture support your business outcomes? How do you design your workload so that you can understand its state? How do you reduce defects, ease remediation, and improve flow into production? How do you mitigate deployment risks? How do you know that you are ready to support a workload? How do you understand the health of your workload? How do you understand the health of your operations? How do you manage workload and operations events? How do you evolve operations? Security \u00b6 Design principles Apply security at all layers Automate security best practices Protect data in transit and at rest Questions to assess How do you manage identities for people and machines? How do you manage permissions for people and machines? How do you detect and investigate security events? How do you protect your network resources? How do you protect your compute resources? How do you classify your data? How do you protect your data at rest? How do you protect your data in transit? How do you anticipate, respond to, and recover from incidents? CloudTrail logs, AWS API calls, and CloudWatch provide monitoring of metrics with alarming, and AWS Config provides configuration history. Ensure that you have a way to quickly grant access for your security team, and automate the isolation of instances as well as the capturing of data and state for forensics. Reliability \u00b6 The ability of a workload to perform its intended function correctly and consistently . Reliability requires that your workload be aware of failures as they occur and take action to avoid impact on availability. Workloads must be able to both withstand failures and automatically repair issues. Design principles: Automatically recover from failure Test recovery procedures Scale horizontally to increase aggregate workload availability Stop guessing capacity: monitor demand and workload utilization, and automate the addition or removal of resources to maintain the optimal level to satisfy demand without over- or under-provisioning Manage change in automation Before architecting any system, foundational requirements that influence reliability should be in place. Questions to assess Comments How do you manage service quotas and constraints? How do you plan your network topology? How do you design your workload service architecture? How do you design interactions in a distributed system to prevent failures? Search to improve mean time between failures (MTBF) How do you design interactions in a distributed system to mitigate or withstand failures? Look to improve mean time to recovery (MTTR) How do you monitor workload resources? Monitor Logs and metrics How do you design your workload to adapt to changes in demand? Add or remove resources automatically to adapt to the demand How do you implement change? Controlled changes to deploy new feature, patched or replaced in a predictable manner How do you back up data? Helps to addres RTO and RPO How do you use fault isolation to protect your workload? Components outside of the boundary should not be affected by the failure. How do you design your workload to withstand component failures? How do you test reliability? testing is the only way to ensure that it will operate as designed How do you plan for disaster recovery (DR)? Regularly back up your data and test your backup files to ensure that you can recover from both logical and physical errors Use AZ, regions and bulkhead (elements of an application are isolated into pools so that if one fails, the others will continue to function) Performance efficiency \u00b6 Use computing resources efficiently to meet system requirements, and to maintain that efficiency as demand changes and technologies evolve. Design principles: Democratize advanced technologies: delegate to your cloud vendor. Go global in minutes Use serverless architectures Experiment more often Consider mechanical sympathy: always use the technology approach that aligns best with your workload goal In AWS, compute is available in three forms: instances, containers, and functions. Storage is available in three forms: object, block, and file. Databases include relational, key-value, document, in-memory, graph, time series, and ledger databases. Questions to assess Comments How do you select the best performing architecture? Use a data-driven approach to select the patterns and implementation for your architecture and achieve a cost effective solution. How do you select your compute solution? Varies based on application design, usage patterns, and configuration settings How do you select your storage solution? The optimal storage solution for a system varies based on the kind of access method (block, file, or object), patterns of access (random or sequential), required throughput, frequency of access (online, offline, archival), frequency of update (WORM, dynamic), and availability and durability constraints How do you select your database solution? Consider requirements for availability, consistency, partition tolerance, latency, durability, scalability, and query capability How do you configure your networking solution? varies based on latency, throughput requirements, jitter, and bandwidth How do you evolve your workload to take advantage of new releases? How do you monitor your resources to ensure they are performing? How do you use tradeoffs to improve performance? improve performance by trading consistency, durability, and space for time and latency. Amazon CloudWatch is a monitoring and observability service that provides you with data and actionable insights to monitor your workload, respond to system-wide performance changes, optimize resource utilization, and get a unified view of operational health AWS cloudformation to define infrastructure as code. Cost optimization \u00b6 run systems to deliver business value at the lowest price point Design principles: Implement Cloud Financial Management practices / team Adopt a concumption model Measure overall efficiency: Measure the business output of the workload and the costs associated with delivering it Stop spending mone on undifferentiated heavy lifting Questions to assess Comments How do you govern usage? How do you monitor usage and cost? How do you decommission resources? How do you evaluate cost when you select services? Trade off between low level service like EC2, S3, EBS versus higher level like DynamoDB How do you meet cost targets when you select resource type, size and number? How do you plan for data transfer charges? As your requirements change, be aggressive in decommissioning resources, entire services, and systems that you no longer require. Sustainability \u00b6 Focuses on environmental impacts, especially energy consumption and efficiency. Scale infrastructure to continually match user load and ensure that only the minimum resources required to support users are deployed identify redundancy, underutilization, and potential decommission targets Implement patterns for performing load smoothing and maintaining consistent high utilization of deployed resources to minimize the resources consumed. Monitor workload activity to identify application components that consume the most resources. Understand how data is used within your workload, consumed by your users, transferred, and stored. Lifecycle data to more efficient, less performant storage when requirements decrease, and delete data that\u2019s no longer required. Adopt shared storage and single sources of truth to avoid data duplication and reduce the total storage requirements of your workload Back up data only when difficult to recreate Minimize the amount of hardware needed to provision and deploy Use automation and infrastructure as code to bring pre-production environments up when needed and take them down when not used. More readings \u00b6 system design exercices use AWS services","title":"Well Architectured"},{"location":"well-architectured/#aws-well-architectured","text":"The AWS Well-Architected Framework helps you understand the pros and cons of decisions you make while building systems on AWS. Those are the questions to ask for designing a cloud native solution by understanding the potential impact . All hardware are becoming software. Workload represents interrelated applications, infrastructure, policies, governance and operations.","title":"AWS Well Architectured"},{"location":"well-architectured/#six-pilards","text":"When architecting technology solutions, never neglect the six pillars of: Operational Excellence Security Reliability Performance Efficiency Cost Optimization Sustainability","title":"Six pilards"},{"location":"well-architectured/#design-principles","text":"Stop guessing your capacity needs : use as much or as little capacity as you need, and scale up and down automatically. Test systems at production scale , then decommission the resources. Automate to make architectural experimentation easier . Allow for evolutionary architectures : the capability to automate and test on demand lowers the risk of impact from design changes. This allows systems to evolve over time so that businesses can take advantage of innovations as a standard practice. Drive architectures using data : In the cloud, you can collect data on how your architectural choices affect the behavior of your workload. This lets you make fact-based decisions on how to improve your workload. Improve through game days to simulate events in production. This will help you understand where improvements can be made and can help develop organizational experience in dealing with events.","title":"Design Principles"},{"location":"well-architectured/#operational-excellence","text":"Support development and run workloads effectively, gain insight into their operations, and to continuously improve supporting processes and procedures. Four area of focus: Organization: define clear responsabilities, roles, and success interdependencies Prepare: design telemetry (logs, metrics...), improve flow, mitigate deployment risks, understand operational readiness Operate: understand workload health, operation health, achievement of business outcome. Runbooks and playbooks should define escalation process, and define owneship for each action Evolve: learn from experience, make improvements, share with teams Design principles: Perform operations as code Make frequent, small, reversible changes Refine operations procedures frequently. Set up regular game days to review and validate that all procedures are effective. Anticipate failure: Perform \u201cpre-mortem\u201d exercises to identify potential sources of failure so that they can be removed or mitigated. Learn from all operational failures Use established runbooks for well-understood events, and use playbooks to aid in investigation and resolution of issues. CloudWatch is used to aggregate and present business, workload, and operations level views of operations activities. Questions to assess How do you determine what your priorities are? How do you structure your organization to support your business outcomes? How does your organizational culture support your business outcomes? How do you design your workload so that you can understand its state? How do you reduce defects, ease remediation, and improve flow into production? How do you mitigate deployment risks? How do you know that you are ready to support a workload? How do you understand the health of your workload? How do you understand the health of your operations? How do you manage workload and operations events? How do you evolve operations?","title":"Operational Excellence"},{"location":"well-architectured/#security","text":"Design principles Apply security at all layers Automate security best practices Protect data in transit and at rest Questions to assess How do you manage identities for people and machines? How do you manage permissions for people and machines? How do you detect and investigate security events? How do you protect your network resources? How do you protect your compute resources? How do you classify your data? How do you protect your data at rest? How do you protect your data in transit? How do you anticipate, respond to, and recover from incidents? CloudTrail logs, AWS API calls, and CloudWatch provide monitoring of metrics with alarming, and AWS Config provides configuration history. Ensure that you have a way to quickly grant access for your security team, and automate the isolation of instances as well as the capturing of data and state for forensics.","title":"Security"},{"location":"well-architectured/#reliability","text":"The ability of a workload to perform its intended function correctly and consistently . Reliability requires that your workload be aware of failures as they occur and take action to avoid impact on availability. Workloads must be able to both withstand failures and automatically repair issues. Design principles: Automatically recover from failure Test recovery procedures Scale horizontally to increase aggregate workload availability Stop guessing capacity: monitor demand and workload utilization, and automate the addition or removal of resources to maintain the optimal level to satisfy demand without over- or under-provisioning Manage change in automation Before architecting any system, foundational requirements that influence reliability should be in place. Questions to assess Comments How do you manage service quotas and constraints? How do you plan your network topology? How do you design your workload service architecture? How do you design interactions in a distributed system to prevent failures? Search to improve mean time between failures (MTBF) How do you design interactions in a distributed system to mitigate or withstand failures? Look to improve mean time to recovery (MTTR) How do you monitor workload resources? Monitor Logs and metrics How do you design your workload to adapt to changes in demand? Add or remove resources automatically to adapt to the demand How do you implement change? Controlled changes to deploy new feature, patched or replaced in a predictable manner How do you back up data? Helps to addres RTO and RPO How do you use fault isolation to protect your workload? Components outside of the boundary should not be affected by the failure. How do you design your workload to withstand component failures? How do you test reliability? testing is the only way to ensure that it will operate as designed How do you plan for disaster recovery (DR)? Regularly back up your data and test your backup files to ensure that you can recover from both logical and physical errors Use AZ, regions and bulkhead (elements of an application are isolated into pools so that if one fails, the others will continue to function)","title":"Reliability"},{"location":"well-architectured/#performance-efficiency","text":"Use computing resources efficiently to meet system requirements, and to maintain that efficiency as demand changes and technologies evolve. Design principles: Democratize advanced technologies: delegate to your cloud vendor. Go global in minutes Use serverless architectures Experiment more often Consider mechanical sympathy: always use the technology approach that aligns best with your workload goal In AWS, compute is available in three forms: instances, containers, and functions. Storage is available in three forms: object, block, and file. Databases include relational, key-value, document, in-memory, graph, time series, and ledger databases. Questions to assess Comments How do you select the best performing architecture? Use a data-driven approach to select the patterns and implementation for your architecture and achieve a cost effective solution. How do you select your compute solution? Varies based on application design, usage patterns, and configuration settings How do you select your storage solution? The optimal storage solution for a system varies based on the kind of access method (block, file, or object), patterns of access (random or sequential), required throughput, frequency of access (online, offline, archival), frequency of update (WORM, dynamic), and availability and durability constraints How do you select your database solution? Consider requirements for availability, consistency, partition tolerance, latency, durability, scalability, and query capability How do you configure your networking solution? varies based on latency, throughput requirements, jitter, and bandwidth How do you evolve your workload to take advantage of new releases? How do you monitor your resources to ensure they are performing? How do you use tradeoffs to improve performance? improve performance by trading consistency, durability, and space for time and latency. Amazon CloudWatch is a monitoring and observability service that provides you with data and actionable insights to monitor your workload, respond to system-wide performance changes, optimize resource utilization, and get a unified view of operational health AWS cloudformation to define infrastructure as code.","title":"Performance efficiency"},{"location":"well-architectured/#cost-optimization","text":"run systems to deliver business value at the lowest price point Design principles: Implement Cloud Financial Management practices / team Adopt a concumption model Measure overall efficiency: Measure the business output of the workload and the costs associated with delivering it Stop spending mone on undifferentiated heavy lifting Questions to assess Comments How do you govern usage? How do you monitor usage and cost? How do you decommission resources? How do you evaluate cost when you select services? Trade off between low level service like EC2, S3, EBS versus higher level like DynamoDB How do you meet cost targets when you select resource type, size and number? How do you plan for data transfer charges? As your requirements change, be aggressive in decommissioning resources, entire services, and systems that you no longer require.","title":"Cost optimization"},{"location":"well-architectured/#sustainability","text":"Focuses on environmental impacts, especially energy consumption and efficiency. Scale infrastructure to continually match user load and ensure that only the minimum resources required to support users are deployed identify redundancy, underutilization, and potential decommission targets Implement patterns for performing load smoothing and maintaining consistent high utilization of deployed resources to minimize the resources consumed. Monitor workload activity to identify application components that consume the most resources. Understand how data is used within your workload, consumed by your users, transferred, and stored. Lifecycle data to more efficient, less performant storage when requirements decrease, and delete data that\u2019s no longer required. Adopt shared storage and single sources of truth to avoid data duplication and reduce the total storage requirements of your workload Back up data only when difficult to recreate Minimize the amount of hardware needed to provision and deploy Use automation and infrastructure as code to bring pre-production environments up when needed and take them down when not used.","title":"Sustainability"},{"location":"well-architectured/#more-readings","text":"system design exercices use AWS services","title":"More readings"},{"location":"infra/","text":"Major infrastructure services \u00b6 EC2 components \u00b6 EC2 is a renting machine Storing data on virtual drives: EBS Distribute load across machines using ELB Auto scale the service via group: ASG EC2 can have MacOS, Linux ad Windows OS. Amazon Machine Image: AMI, image for OS and preinstalled softwares. Amazon Linux 2 for linux base image. When creating an instance, we can select the OS, CPU, RAM, the VPC, the AZ subnet, and the storage (EBS) for root folder to get the OS, the network card, and the firewall rules as security group. The security group helps to isolate the instance, for example, authorizing ssh on port 22 and HTTP port 80. Get the public ssh key, and when the instance is started, use: ssh -i EC2key.pem ec2-user@ec2-52-8-75-8.us-west-1.compute.amazonaws.com to connect to the EC2. The .pem file need to be restricted with chmod 0400 Can also use EC2 Instance Connect to open a terminal in the web browser. Still needs to get SSH port open. EC2 has a section to add User data , which could be used to define a bash script to install dependent software and to start some services at boot time. EC2 instance types (see ec2instances.info or the reference aws ec2/instance-types ) includes: R: (memory) applications that needs a lot of RAM \u2013 in-memory caches C: (Compute Optimized) applications that needs good CPU \u2013 compute / databases, ETLm media transcoding, High Perf web servers, scientific modeling M: applications that are balanced (think \u201cmedium\u201d) \u2013 general / web app I: (storage) applications that need good local I/O (instance storage) \u2013 databases, NoSQL, cache like Redis, data warehousing, distributed file systems G: applications that need a GPU T2/T3 for burstable instance: When the machine needs to process something unexpected (a spike in load for example), it can burst. Use burst credits to control CPU usage. EC2 Nitro \u00b6 Next generation of EC2. It uses new virtualization schema. Supports IPv6, better I/O on EBS and better security. Name starts with C5, D5,... vCPU represents thread running on core CPU. You can optimize vCPU allocation on the EC2 instance, once created, by updating the launch configuration. Launch types \u00b6 On demand : short workload, predictable pricing, pay per second after first minute. Reserved for one or 3 years, used for long workloads like database. Get discounted rate from on-demand. Convertible reserved instance for changing resource capacity over time. Scheduled reserved instance for job based workload. Spot instance for very short - 90% discount on on-demand - used for work resilient to failure like batch job, data analysis, image processing,... Define a max spot price and get the instance while the current spot price < max. The hourly spot price varies based on offer and capacity. if the current spot price > max, then instance will be stopped with spot block we can define a time frame without interruptions. The expected state is defined in a 'spot request' which can be cancelled. One time or persistent request types are supported. Cancel a spot request does not terminate instances, but need to be the first thing to do and then terminate the instances. Spot fleets allow to automatically request spot instance with the lowest price. Dedicated hosts to book entire physical server and control instance placement. # years. BYOL. Use EC2 launch templates to automate instance launches, simplify permission policies, and enforce best practices across the organization. (Look very similar to docker image) AMI \u00b6 Bring our own image. Shareable on amazon marketplace. Can be saved on S3 storage. By default, our AMIs are private, and locked for our account / region. AMI are built for a specific AWS region. But they can be copied and shared See AWS doc - copying an AMI . EC2 Hibernate \u00b6 The in memory state is preserved, persisted to a file in the root EBS volume. It helps to make the instance startup time quicker. The root EBS volume is encrypted. Constrained by 150GB RAM. No more than 60 days. Basic Fault Tolerance \u00b6 The following diagram illustrates some fault tolerance principles offered by the basic AWS services: AMI defines image for the EC2 with static or dynamic configuration. From one AMI, we can scale by adding new EC2 based on same image. Instance failure can be replaced by starting a new instance from the same AMI Auto scaling group defines a set of EC2 instances, and can start new EC2 instance automatically To minimize down time, we can have one EC2 instance in standby, and use elastic IP addresses. Data is saved on EBS and replicated to other EBS inside the same availabiltiy zone. Snapshot backup can be done to replicate data between AZs, and persisted for long retention in S3. Need to flush data from memory to disk before any snapshot Auto scaling adjusts the capacity of EC2 and EC2 instance within the group Applications can be deployed between AZs. Elastic Load Balancer balances traffic among servers in multiple AZs and DNS will route traffic to the good server. Data can be replicated between regions Elastic IP addresses are static and defined at the AWS account level. New EC2 instance can be reallocated to Elastic IP @, but they are mapped by internet gateway to the private address of the EC2. The service may be down until new EC2 instace is restarted. ELB ensures higher fault tolerance for EC2s, containers, lambdas, IP addresses and physical servers Application LB load balances at the HTTP, HTTPS level, and within a VPC based on the content of the request. NLB is for TCP, UDP, TLS routing and load balancing. VPC \u00b6 A virtual private cloud (VPC) is a virtual network dedicated to your AWS account. It is logically isolated from other virtual networks in the AWS Cloud. You can launch your AWS resources, such as Amazon EC2 instances, into your VPC. You can specify an IP address range for the VPC, add subnets, associate security groups, and configure route tables. VPC Helps to: Assign static IP addresses, potentially multiple addresses for the same instance Change security group membership for your instances while they're running Control the outbound traffic from your instances (egress filtering) in addition to controlling the inbound traffic to them (ingress filtering) Default VPC includes an internet gateway The following diagram illustrates classical VPC, as defined years ago, with one vpc, 2 availability zones, 2 subnets with EC2 instances within those subnet and AZs. non-default subnet has a private IPv4 address, but no public IPv4 You can enable internet access for an EC2 instance launched into a non-default subnet by attaching an internet gateway to its VPC. Route tables defines 172.3 as local with /20 CIDR address range, internal to the VPC. Default route to internet goes to the IGW, which has an elastic IP address assigned to it. Because the VPC is cross AZs, we need a router to route between subnets. (See TCP/IP summary ) Alternatively, to allow an instance in your VPC to initiate outbound connections to the internet but prevent unsolicited inbound connections from the internet, you can use a network address translation (NAT) service for IPv4 traffic. NAT maps multiple private IPv4 addresses to a single public IPv4 address. A NAT device has an Elastic IP address and is connected to the internet through an internet gateway. figure: Full VPC diagram You can have VPC end point service to access a lot of AWS services , like S3, privately as those services will be in your VPC. TCP traffic is isolated. It is part of a larger offering called AWS PrivateLink establishes private connectivity between VPCs and services hosted on AWS or on-premises, without exposing data to the internet (No internet gateway, no NAT, no public IP @). You can optionally connect your VPC to your own corporate data center using an IPsec AWS managed VPN connection, making the AWS Cloud an extension of your data center. A VPN connection consists of a virtual private gateway (VGW) attached to your VPC and a customer gateway located in your data center. A virtual private gateway is the VPN concentrator on the Amazon side of the VPN connection. A customer gateway is a physical device or software appliance on your side of the VPN connection. As seen in \"Full VPC diagram\", the VPC peering helps to connect between VPCs in different region, or within the same region. And Transit GTW is used to interconnect your virtual private clouds (VPCs) and on-premises networks Security group \u00b6 Define inbound and outbound security rules. They regulate access to ports, authorized IP ranges IPv4 and IPv6, control inbound and outbound network. By default all inbound traffic is denied and outbound authorized. They contain allow rules only. Can be attached to multiple EC2 instances and to load balancers Locked down to a region / VPC combination Live outside of the EC2 Define one separate security group for SSH access where you can authorize only one IP@ Connect refused is an application error or the app is not launched Instances with the same security group can access each other Security group can reference other security groups, IP address, CIDR but no DNS server Important Ports: 22 for SSH and SFTP 21 for FTP 80 for HTTP 443 for https 3389: Remote desktop protocol Networking \u00b6 IPv4 allows 3.7 billions of different addresses. Private IP @ is for private network connections. Internet gateway has public and private connections. Public IP can be geo-located. When connected to an EC2 the prompt lists the private IP ( ec2-user@ip-172-31-18-48 ). Private IP stays stable on instance restart, while public may change. With Elastic IP address, we can mask an EC2 instance failure by rapidly remapping the address to another instance. But better to use DNS. Elastic IP is a public IPv4 that you own as long as you want and you can attach it to one EC2 instance at a time. Playing with Apache HTTP \u00b6 # Swap to root sudo su # update OS yum update -y # Get Apache HTTPd yum install -y httpd.x86_64 # Start the service systemctl start httpd.service # Enable it cross restart systemctl enable httpd.service > Created symlink from /etc/systemd/system/multi-user.target.wants/httpd.service to /usr/lib/systemd/system/httpd.service # Get the availability zone EC2-AZ = $( curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone ) # Change the home page by changing /var/www/html/index.html echo \"<h3>Hello World from $( hostname -f ) in AZ= $EC2_AZ </h3>\" > /var/www/html/index.html This script can be added as User Data (Under Advanced Details while configuring new instance) so when the instance starts it executes this code. Elastic Network Insterfaces \u00b6 ENI is a logical component in a VPC that represents a virtual network card. It has the following attributes: One primary private IPv4, one or more secondary IPv4 One Elastic IP (IPv4) per private IPv4 One Public IPv4 One or more security groups A MAC address We can create ENI independently and attach them on the fly (move them) on EC2 instances for failover Bound to a specific availability zone (AZ) New ENI doc. Placement groups \u00b6 Define strategy to place EC2 instances: Cluster : groups instances into a low-latency group in a single Availability Zone highest performance while talking to each other as when performing big data analysis Spread : groups across underlying hardware (max 7 instances per group per AZ) Reduced risk is simultaneous failure EC2 Instances are on different physical hardware Application that needs to maximize high availability Critical Applications where each instance must be isolated from failure from each other Partition : spreads instances across many different partitions (which rely on different sets of racks) within an AZ. Partition is a set of racks Up to 100s of EC2 instances The instances in a partition do not share racks with the instances in the other partitions A partition failure can affect many EC2s but won\u2019t affect other partitions EC2 instances get access to the partition information as metadata HDFS, HBase, Cassandra, Kafka Access from network and policies menu, define the group with expected strategy, and then it is used when creating the EC2 instance by adding the instance to a placement group. Elastic Load balancer \u00b6 Route traffic into the different EC2 instances. Elastic Load Balancing scales your load balancer capacity automatically in response to changes in incoming traffic. It also exposes a single point of access (DNS) to the deployed applications. In case of EC2 failure, it can route to a new instance, transparently and across multiple AZs. It uses health check (/health on the app called the ping path ) to assess instance availability. It also provides SSL termination. It supports to separate private (internal) to public (external) traffic. Need to enable availability zone to be able to route traffic between target groups in different AZs. When you create a load balancer, you must choose whether to make it an internal load balancer or an internet-facing load balancer. Internet-facing load balancer have public IP addresses. The DNS name of an internet-facing load balancer is publicly resolvable to the public IP addresses of the nodes. Internal load balancer have only private IP addresses. Internal load balancers can only route requests from clients with access to the VPC for the load balancer. Four types of ELB supported: Classic load balancer: older generation. TCP and HTTP layer. For each instance created, update the load balancer configuration so it can route the traffic. Application load balancer : HTTP, HTTPS (layer 7), Web Socket. It specifies availability zones: it routes traffic to the targets in these Availability Zones. Each AZ has one subnet. To increase availability, we need at least two AZs. It uses target groups, to group applications route on URL, hostname and query string Get a fixed hostname the application do not see the IP address of the client directly (ELB does a connection termination), but ELB put it in the header X-Forwarded-For , X-Forwarded-Port and X-Forwarded-Proto . Great for microservices or for container based apps. Free to get started Gateway LB : also use target group. Network load balancer : TCP, UDP (layer 4), TLS handle millions request/s reach less than 100ms latency while ALB is at 400ms use to get a public static IP address per availability zone Routes each individual TCP connection to a single target for the life of the connection not free To route traffic, first the DNS name of the load balancer is resolved. (They are part of the amazaonaws.com domain). 1 to many IP Addresses are sent back to the client. With NLBs, Elastic Load Balancing creates a network interface for each Availability Zone that you enable. Each load balancer node in the Availability Zone uses this network interface to get a static IP address. ELB scales your load balancer and updates the DNS entry. The time to live is set to 60s. To control that only the load balancer is sending traffic to the application, we need to set up an application security group on HTTP, and HTTPS with the source being the security group id of the ELB. LBs can scale but need to engage AWS operational team. HTTP 503 means LB is at capacity or target app is not registered. Verify security group in case of no communication between LB and app. Target group defines protocol to use, health check checking and what applications to reach (instance, IP or lambda). Example of listener rule for an ALB: ALB and Classic can use HTTP connection multiplexing to keep one connection with the backend application. Connection multiplexing improves latency and reduces the load on your applications. Load balancer stickiness \u00b6 Used when the same client needs to interact with the same backend instance. A cookie, with expiration date, is used to identify the client. The classical gateway or ALB manages the routing. This could lead to unbalance traffic so overloading one instance. With ALB, stickness is configured in the target group properties. Cross Zone Load Balancing \u00b6 Each load balancer instance distributes traffic evenly across all registered instances in all availability zones. If one AZ has 2 targets and another one has 8 targets, then with cross-zone, the LBs in each availability zone will route to any instance, so each will receive 10% of the traffic. Without that, the 2 targets zone will receive 25% traffic each, and the instance on the othe AZ only 6.25% of the traffic. This is the default setting for ALB and free of charge. It is disabled by default for NLB. TLS - Transport Layer Security, \u00b6 An SSL/TLS Certificate allows traffic between clients and load balancer to be encrypted in transit (in-flight encryption). Load balancer uses an X.509 certificate (SSL/TLS server certificate). Manage certificates using ACM (AWS Certificate Manager) When defining a HTTPS listener in a LB, we must specify a default certificate for the HTTPS protocol, while defining the routing rule to a given target group. Need multiple certs to support multiple domains. Clients can use SNI (Server Name Indication) to specify the hostname they reach. The ALB or NLB will get the certificate for this host to support the TLS handshake. Connection draining \u00b6 This is a setting to control connection timeout and reconnect when an instance is not responding. It is to set up the time to complete \u201cin-flight requests\u201d. When an instance is \"draining\", ELB stops sending new requests to the instance. The time out can be adjusted, depending of the application, from 1 to 3600 seconds, default is 300 seconds, or disabled (set value to 0). Auto Scaling Group (ASG) \u00b6 The goal of an ASG is to scale out (add EC2 instances) to match an increased load, or scale in (remove EC2 instances) to match a decreased load. It helps to provision and balance capacity across Availability Zones to optimize availability. It can also ensure we have a minimum and a maximum number of machines running. It detects when an instance is unhealthy. Automatically Register new instances to a load balancer. ASG has the following attributes: AMI + Instance Type with EC2 User Data (Can use template to define instances) EBS Volumes Security Groups SSH Key Pair Min Size / Max Size / Initial Capacity to control number of instances Network + Subnets Information to specify where to run the EC2 instances. Load Balancer Information, with target groups to be used as a grouping of the newly created instances Scaling Policies help to define rules to manage instance life cycle, based for example on CPU usage or network bandwidth used. when creating scaling policies, CloudWatch alarms are created. Ex: \"Create an alarm if: CPUUtilization < 36 for 15 data points within 15 minutes\". ASG tries the balance the number of instances across AZ by default, and then delete based on the age of the launch configuration The capacity of our ASG cannot go over the maximum capacity we have allocated during scale out events when an ALB validates an health check issue it terminates the EC2 instance.","title":"VPC- EC2"},{"location":"infra/#major-infrastructure-services","text":"","title":"Major infrastructure services"},{"location":"infra/#ec2-components","text":"EC2 is a renting machine Storing data on virtual drives: EBS Distribute load across machines using ELB Auto scale the service via group: ASG EC2 can have MacOS, Linux ad Windows OS. Amazon Machine Image: AMI, image for OS and preinstalled softwares. Amazon Linux 2 for linux base image. When creating an instance, we can select the OS, CPU, RAM, the VPC, the AZ subnet, and the storage (EBS) for root folder to get the OS, the network card, and the firewall rules as security group. The security group helps to isolate the instance, for example, authorizing ssh on port 22 and HTTP port 80. Get the public ssh key, and when the instance is started, use: ssh -i EC2key.pem ec2-user@ec2-52-8-75-8.us-west-1.compute.amazonaws.com to connect to the EC2. The .pem file need to be restricted with chmod 0400 Can also use EC2 Instance Connect to open a terminal in the web browser. Still needs to get SSH port open. EC2 has a section to add User data , which could be used to define a bash script to install dependent software and to start some services at boot time. EC2 instance types (see ec2instances.info or the reference aws ec2/instance-types ) includes: R: (memory) applications that needs a lot of RAM \u2013 in-memory caches C: (Compute Optimized) applications that needs good CPU \u2013 compute / databases, ETLm media transcoding, High Perf web servers, scientific modeling M: applications that are balanced (think \u201cmedium\u201d) \u2013 general / web app I: (storage) applications that need good local I/O (instance storage) \u2013 databases, NoSQL, cache like Redis, data warehousing, distributed file systems G: applications that need a GPU T2/T3 for burstable instance: When the machine needs to process something unexpected (a spike in load for example), it can burst. Use burst credits to control CPU usage.","title":"EC2 components"},{"location":"infra/#ec2-nitro","text":"Next generation of EC2. It uses new virtualization schema. Supports IPv6, better I/O on EBS and better security. Name starts with C5, D5,... vCPU represents thread running on core CPU. You can optimize vCPU allocation on the EC2 instance, once created, by updating the launch configuration.","title":"EC2 Nitro"},{"location":"infra/#launch-types","text":"On demand : short workload, predictable pricing, pay per second after first minute. Reserved for one or 3 years, used for long workloads like database. Get discounted rate from on-demand. Convertible reserved instance for changing resource capacity over time. Scheduled reserved instance for job based workload. Spot instance for very short - 90% discount on on-demand - used for work resilient to failure like batch job, data analysis, image processing,... Define a max spot price and get the instance while the current spot price < max. The hourly spot price varies based on offer and capacity. if the current spot price > max, then instance will be stopped with spot block we can define a time frame without interruptions. The expected state is defined in a 'spot request' which can be cancelled. One time or persistent request types are supported. Cancel a spot request does not terminate instances, but need to be the first thing to do and then terminate the instances. Spot fleets allow to automatically request spot instance with the lowest price. Dedicated hosts to book entire physical server and control instance placement. # years. BYOL. Use EC2 launch templates to automate instance launches, simplify permission policies, and enforce best practices across the organization. (Look very similar to docker image)","title":"Launch types"},{"location":"infra/#ami","text":"Bring our own image. Shareable on amazon marketplace. Can be saved on S3 storage. By default, our AMIs are private, and locked for our account / region. AMI are built for a specific AWS region. But they can be copied and shared See AWS doc - copying an AMI .","title":"AMI"},{"location":"infra/#ec2-hibernate","text":"The in memory state is preserved, persisted to a file in the root EBS volume. It helps to make the instance startup time quicker. The root EBS volume is encrypted. Constrained by 150GB RAM. No more than 60 days.","title":"EC2 Hibernate"},{"location":"infra/#basic-fault-tolerance","text":"The following diagram illustrates some fault tolerance principles offered by the basic AWS services: AMI defines image for the EC2 with static or dynamic configuration. From one AMI, we can scale by adding new EC2 based on same image. Instance failure can be replaced by starting a new instance from the same AMI Auto scaling group defines a set of EC2 instances, and can start new EC2 instance automatically To minimize down time, we can have one EC2 instance in standby, and use elastic IP addresses. Data is saved on EBS and replicated to other EBS inside the same availabiltiy zone. Snapshot backup can be done to replicate data between AZs, and persisted for long retention in S3. Need to flush data from memory to disk before any snapshot Auto scaling adjusts the capacity of EC2 and EC2 instance within the group Applications can be deployed between AZs. Elastic Load Balancer balances traffic among servers in multiple AZs and DNS will route traffic to the good server. Data can be replicated between regions Elastic IP addresses are static and defined at the AWS account level. New EC2 instance can be reallocated to Elastic IP @, but they are mapped by internet gateway to the private address of the EC2. The service may be down until new EC2 instace is restarted. ELB ensures higher fault tolerance for EC2s, containers, lambdas, IP addresses and physical servers Application LB load balances at the HTTP, HTTPS level, and within a VPC based on the content of the request. NLB is for TCP, UDP, TLS routing and load balancing.","title":"Basic Fault Tolerance"},{"location":"infra/#vpc","text":"A virtual private cloud (VPC) is a virtual network dedicated to your AWS account. It is logically isolated from other virtual networks in the AWS Cloud. You can launch your AWS resources, such as Amazon EC2 instances, into your VPC. You can specify an IP address range for the VPC, add subnets, associate security groups, and configure route tables. VPC Helps to: Assign static IP addresses, potentially multiple addresses for the same instance Change security group membership for your instances while they're running Control the outbound traffic from your instances (egress filtering) in addition to controlling the inbound traffic to them (ingress filtering) Default VPC includes an internet gateway The following diagram illustrates classical VPC, as defined years ago, with one vpc, 2 availability zones, 2 subnets with EC2 instances within those subnet and AZs. non-default subnet has a private IPv4 address, but no public IPv4 You can enable internet access for an EC2 instance launched into a non-default subnet by attaching an internet gateway to its VPC. Route tables defines 172.3 as local with /20 CIDR address range, internal to the VPC. Default route to internet goes to the IGW, which has an elastic IP address assigned to it. Because the VPC is cross AZs, we need a router to route between subnets. (See TCP/IP summary ) Alternatively, to allow an instance in your VPC to initiate outbound connections to the internet but prevent unsolicited inbound connections from the internet, you can use a network address translation (NAT) service for IPv4 traffic. NAT maps multiple private IPv4 addresses to a single public IPv4 address. A NAT device has an Elastic IP address and is connected to the internet through an internet gateway. figure: Full VPC diagram You can have VPC end point service to access a lot of AWS services , like S3, privately as those services will be in your VPC. TCP traffic is isolated. It is part of a larger offering called AWS PrivateLink establishes private connectivity between VPCs and services hosted on AWS or on-premises, without exposing data to the internet (No internet gateway, no NAT, no public IP @). You can optionally connect your VPC to your own corporate data center using an IPsec AWS managed VPN connection, making the AWS Cloud an extension of your data center. A VPN connection consists of a virtual private gateway (VGW) attached to your VPC and a customer gateway located in your data center. A virtual private gateway is the VPN concentrator on the Amazon side of the VPN connection. A customer gateway is a physical device or software appliance on your side of the VPN connection. As seen in \"Full VPC diagram\", the VPC peering helps to connect between VPCs in different region, or within the same region. And Transit GTW is used to interconnect your virtual private clouds (VPCs) and on-premises networks","title":"VPC"},{"location":"infra/#security-group","text":"Define inbound and outbound security rules. They regulate access to ports, authorized IP ranges IPv4 and IPv6, control inbound and outbound network. By default all inbound traffic is denied and outbound authorized. They contain allow rules only. Can be attached to multiple EC2 instances and to load balancers Locked down to a region / VPC combination Live outside of the EC2 Define one separate security group for SSH access where you can authorize only one IP@ Connect refused is an application error or the app is not launched Instances with the same security group can access each other Security group can reference other security groups, IP address, CIDR but no DNS server Important Ports: 22 for SSH and SFTP 21 for FTP 80 for HTTP 443 for https 3389: Remote desktop protocol","title":"Security group"},{"location":"infra/#networking","text":"IPv4 allows 3.7 billions of different addresses. Private IP @ is for private network connections. Internet gateway has public and private connections. Public IP can be geo-located. When connected to an EC2 the prompt lists the private IP ( ec2-user@ip-172-31-18-48 ). Private IP stays stable on instance restart, while public may change. With Elastic IP address, we can mask an EC2 instance failure by rapidly remapping the address to another instance. But better to use DNS. Elastic IP is a public IPv4 that you own as long as you want and you can attach it to one EC2 instance at a time.","title":"Networking"},{"location":"infra/#playing-with-apache-http","text":"# Swap to root sudo su # update OS yum update -y # Get Apache HTTPd yum install -y httpd.x86_64 # Start the service systemctl start httpd.service # Enable it cross restart systemctl enable httpd.service > Created symlink from /etc/systemd/system/multi-user.target.wants/httpd.service to /usr/lib/systemd/system/httpd.service # Get the availability zone EC2-AZ = $( curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone ) # Change the home page by changing /var/www/html/index.html echo \"<h3>Hello World from $( hostname -f ) in AZ= $EC2_AZ </h3>\" > /var/www/html/index.html This script can be added as User Data (Under Advanced Details while configuring new instance) so when the instance starts it executes this code.","title":"Playing with Apache HTTP"},{"location":"infra/#elastic-network-insterfaces","text":"ENI is a logical component in a VPC that represents a virtual network card. It has the following attributes: One primary private IPv4, one or more secondary IPv4 One Elastic IP (IPv4) per private IPv4 One Public IPv4 One or more security groups A MAC address We can create ENI independently and attach them on the fly (move them) on EC2 instances for failover Bound to a specific availability zone (AZ) New ENI doc.","title":"Elastic Network Insterfaces"},{"location":"infra/#placement-groups","text":"Define strategy to place EC2 instances: Cluster : groups instances into a low-latency group in a single Availability Zone highest performance while talking to each other as when performing big data analysis Spread : groups across underlying hardware (max 7 instances per group per AZ) Reduced risk is simultaneous failure EC2 Instances are on different physical hardware Application that needs to maximize high availability Critical Applications where each instance must be isolated from failure from each other Partition : spreads instances across many different partitions (which rely on different sets of racks) within an AZ. Partition is a set of racks Up to 100s of EC2 instances The instances in a partition do not share racks with the instances in the other partitions A partition failure can affect many EC2s but won\u2019t affect other partitions EC2 instances get access to the partition information as metadata HDFS, HBase, Cassandra, Kafka Access from network and policies menu, define the group with expected strategy, and then it is used when creating the EC2 instance by adding the instance to a placement group.","title":"Placement groups"},{"location":"infra/#elastic-load-balancer","text":"Route traffic into the different EC2 instances. Elastic Load Balancing scales your load balancer capacity automatically in response to changes in incoming traffic. It also exposes a single point of access (DNS) to the deployed applications. In case of EC2 failure, it can route to a new instance, transparently and across multiple AZs. It uses health check (/health on the app called the ping path ) to assess instance availability. It also provides SSL termination. It supports to separate private (internal) to public (external) traffic. Need to enable availability zone to be able to route traffic between target groups in different AZs. When you create a load balancer, you must choose whether to make it an internal load balancer or an internet-facing load balancer. Internet-facing load balancer have public IP addresses. The DNS name of an internet-facing load balancer is publicly resolvable to the public IP addresses of the nodes. Internal load balancer have only private IP addresses. Internal load balancers can only route requests from clients with access to the VPC for the load balancer. Four types of ELB supported: Classic load balancer: older generation. TCP and HTTP layer. For each instance created, update the load balancer configuration so it can route the traffic. Application load balancer : HTTP, HTTPS (layer 7), Web Socket. It specifies availability zones: it routes traffic to the targets in these Availability Zones. Each AZ has one subnet. To increase availability, we need at least two AZs. It uses target groups, to group applications route on URL, hostname and query string Get a fixed hostname the application do not see the IP address of the client directly (ELB does a connection termination), but ELB put it in the header X-Forwarded-For , X-Forwarded-Port and X-Forwarded-Proto . Great for microservices or for container based apps. Free to get started Gateway LB : also use target group. Network load balancer : TCP, UDP (layer 4), TLS handle millions request/s reach less than 100ms latency while ALB is at 400ms use to get a public static IP address per availability zone Routes each individual TCP connection to a single target for the life of the connection not free To route traffic, first the DNS name of the load balancer is resolved. (They are part of the amazaonaws.com domain). 1 to many IP Addresses are sent back to the client. With NLBs, Elastic Load Balancing creates a network interface for each Availability Zone that you enable. Each load balancer node in the Availability Zone uses this network interface to get a static IP address. ELB scales your load balancer and updates the DNS entry. The time to live is set to 60s. To control that only the load balancer is sending traffic to the application, we need to set up an application security group on HTTP, and HTTPS with the source being the security group id of the ELB. LBs can scale but need to engage AWS operational team. HTTP 503 means LB is at capacity or target app is not registered. Verify security group in case of no communication between LB and app. Target group defines protocol to use, health check checking and what applications to reach (instance, IP or lambda). Example of listener rule for an ALB: ALB and Classic can use HTTP connection multiplexing to keep one connection with the backend application. Connection multiplexing improves latency and reduces the load on your applications.","title":"Elastic Load balancer"},{"location":"infra/#load-balancer-stickiness","text":"Used when the same client needs to interact with the same backend instance. A cookie, with expiration date, is used to identify the client. The classical gateway or ALB manages the routing. This could lead to unbalance traffic so overloading one instance. With ALB, stickness is configured in the target group properties.","title":"Load balancer stickiness"},{"location":"infra/#cross-zone-load-balancing","text":"Each load balancer instance distributes traffic evenly across all registered instances in all availability zones. If one AZ has 2 targets and another one has 8 targets, then with cross-zone, the LBs in each availability zone will route to any instance, so each will receive 10% of the traffic. Without that, the 2 targets zone will receive 25% traffic each, and the instance on the othe AZ only 6.25% of the traffic. This is the default setting for ALB and free of charge. It is disabled by default for NLB.","title":"Cross Zone Load Balancing"},{"location":"infra/#tls-transport-layer-security","text":"An SSL/TLS Certificate allows traffic between clients and load balancer to be encrypted in transit (in-flight encryption). Load balancer uses an X.509 certificate (SSL/TLS server certificate). Manage certificates using ACM (AWS Certificate Manager) When defining a HTTPS listener in a LB, we must specify a default certificate for the HTTPS protocol, while defining the routing rule to a given target group. Need multiple certs to support multiple domains. Clients can use SNI (Server Name Indication) to specify the hostname they reach. The ALB or NLB will get the certificate for this host to support the TLS handshake.","title":"TLS - Transport Layer Security,"},{"location":"infra/#connection-draining","text":"This is a setting to control connection timeout and reconnect when an instance is not responding. It is to set up the time to complete \u201cin-flight requests\u201d. When an instance is \"draining\", ELB stops sending new requests to the instance. The time out can be adjusted, depending of the application, from 1 to 3600 seconds, default is 300 seconds, or disabled (set value to 0).","title":"Connection draining"},{"location":"infra/#auto-scaling-group-asg","text":"The goal of an ASG is to scale out (add EC2 instances) to match an increased load, or scale in (remove EC2 instances) to match a decreased load. It helps to provision and balance capacity across Availability Zones to optimize availability. It can also ensure we have a minimum and a maximum number of machines running. It detects when an instance is unhealthy. Automatically Register new instances to a load balancer. ASG has the following attributes: AMI + Instance Type with EC2 User Data (Can use template to define instances) EBS Volumes Security Groups SSH Key Pair Min Size / Max Size / Initial Capacity to control number of instances Network + Subnets Information to specify where to run the EC2 instances. Load Balancer Information, with target groups to be used as a grouping of the newly created instances Scaling Policies help to define rules to manage instance life cycle, based for example on CPU usage or network bandwidth used. when creating scaling policies, CloudWatch alarms are created. Ex: \"Create an alarm if: CPUUtilization < 36 for 15 data points within 15 minutes\". ASG tries the balance the number of instances across AZ by default, and then delete based on the age of the launch configuration The capacity of our ASG cannot go over the maximum capacity we have allocated during scale out events when an ALB validates an health check issue it terminates the EC2 instance.","title":"Auto Scaling Group (ASG)"}]}